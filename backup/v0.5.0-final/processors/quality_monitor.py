# _*_ coding: utf-8 _*_
# File Path: E:/MyFile/stock_database_v1/src/processors\quality_monitor.py
# File Name: quality_monitor
# @ Author: mango-gh22
# @ Dateï¼š2025/12/14 15:37
"""
desc 
"""

# src/processors/quality_monitor.py
"""
è´¨é‡ç›‘æ§å™¨ - è‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥ä¸æŠ¥å‘Š
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import schedule
import time
import logging
from typing import Dict, List, Optional
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import json

from src.processors.validator import DataValidator
from src.processors.adjustor import StockAdjustor
from src.query.query_engine import QueryEngine

logger = logging.getLogger(__name__)


class QualityMonitor:
    """è´¨é‡ç›‘æ§å™¨"""

    def __init__(self, config_path: str = 'config/database.yaml'):
        """
        åˆå§‹åŒ–è´¨é‡ç›‘æ§å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.validator = DataValidator()
        self.adjustor = StockAdjustor()
        self.query_engine = QueryEngine()

        # ç›‘æ§é…ç½®
        self.monitoring_interval = 3600  # 1å°æ—¶
        self.alert_thresholds = {
            'error_count': 10,
            'warning_count': 50,
            'completeness_rate': 0.95,
            'consistency_rate': 0.98
        }

        # æŠ¥è­¦é…ç½®
        self.alert_emails = []
        self.slack_webhook = None

        logger.info("è´¨é‡ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")

    def run_daily_check(self):
        """è¿è¡Œæ¯æ—¥è´¨é‡æ£€æŸ¥"""
        logger.info("å¼€å§‹æ¯æ—¥è´¨é‡æ£€æŸ¥")

        check_time = datetime.now()
        report = {
            'timestamp': check_time.isoformat(),
            'checks': [],
            'summary': {},
            'alerts': []
        }

        try:
            # 1. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
            logger.info("æ‰§è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥")
            completeness_report = self._check_completeness()
            report['checks'].append({
                'type': 'completeness',
                'result': completeness_report
            })

            # 2. ä¸šåŠ¡è§„åˆ™éªŒè¯
            logger.info("æ‰§è¡Œä¸šåŠ¡è§„åˆ™éªŒè¯")
            business_rules_report = self._check_business_rules()
            report['checks'].append({
                'type': 'business_rules',
                'result': business_rules_report
            })

            # 3. ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
            logger.info("æ‰§è¡Œç»Ÿè®¡å¼‚å¸¸æ£€æµ‹")
            anomaly_report = self._detect_anomalies()
            report['checks'].append({
                'type': 'anomaly_detection',
                'result': anomaly_report
            })

            # 4. å¤æƒå› å­éªŒè¯
            logger.info("éªŒè¯å¤æƒå› å­")
            adjustment_report = self._validate_adjustments()
            report['checks'].append({
                'type': 'adjustment_validation',
                'result': adjustment_report
            })

            # ç”Ÿæˆæ‘˜è¦
            report['summary'] = self._generate_daily_summary(report['checks'])

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æŠ¥è­¦
            alerts = self._check_alerts(report['summary'])
            report['alerts'] = alerts

            # ä¿å­˜æŠ¥å‘Š
            self._save_daily_report(report)

            # å‘é€æŠ¥è­¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if alerts:
                self._send_alerts(alerts, report)

            logger.info(f"æ¯æ—¥è´¨é‡æ£€æŸ¥å®Œæˆ: {report['summary']}")

            return report

        except Exception as e:
            logger.error(f"æ¯æ—¥è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            error_report = {
                'timestamp': datetime.now().isoformat(),
                'error': str(e),
                'checks': []
            }
            self._save_daily_report(error_report)
            return error_report

    def _check_completeness(self) -> Dict:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        try:
            # è·å–æ‰€æœ‰è‚¡ç¥¨
            stock_df = self.query_engine.get_stock_list()
            symbols = stock_df['symbol'].tolist() if not stock_df.empty else []

            total_symbols = len(symbols)
            missing_data_symbols = []

            # æŠ½æ ·æ£€æŸ¥
            sample_size = min(50, total_symbols)
            sample_symbols = np.random.choice(symbols, sample_size, replace=False) if symbols else []

            for symbol in sample_symbols:
                # æ£€æŸ¥æœ€è¿‘30å¤©æ•°æ®
                end_date = datetime.now().strftime('%Y-%m-%d')
                start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')

                df = self.query_engine.query_daily_data(
                    symbol=symbol,
                    start_date=start_date,
                    end_date=end_date
                )

                if df.empty or len(df) < 15:  # å‡è®¾è‡³å°‘åº”è¯¥æœ‰15ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
                    missing_data_symbols.append(symbol)

            completeness_rate = 1 - (len(missing_data_symbols) / sample_size) if sample_size > 0 else 0

            return {
                'total_symbols': total_symbols,
                'sample_size': sample_size,
                'missing_data_count': len(missing_data_symbols),
                'completeness_rate': completeness_rate,
                'missing_symbols': missing_data_symbols[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
            }

        except Exception as e:
            logger.error(f"å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return {'error': str(e)}

    def _check_business_rules(self) -> Dict:
        """æ£€æŸ¥ä¸šåŠ¡è§„åˆ™"""
        try:
            # ä½¿ç”¨éªŒè¯å™¨æ£€æŸ¥
            # æŠ½æ ·æ£€æŸ¥å‡ åªè‚¡ç¥¨
            stock_df = self.query_engine.get_stock_list()
            symbols = stock_df['symbol'].tolist() if not stock_df.empty else []

            sample_size = min(10, len(symbols))
            sample_symbols = np.random.choice(symbols, sample_size, replace=False) if symbols else []

            total_violations = 0
            detailed_violations = []

            for symbol in sample_symbols:
                results = self.validator.validate_business_logic(symbol)

                for result in results:
                    if result.result != result.result.PASS:
                        total_violations += result.affected_rows
                        detailed_violations.append({
                            'symbol': symbol,
                            'rule': result.rule_name,
                            'violations': result.affected_rows,
                            'description': result.rule_description
                        })

            return {
                'sample_size': sample_size,
                'total_violations': total_violations,
                'violation_rate': total_violations / (sample_size * 100) if sample_size > 0 else 0,
                'detailed_violations': detailed_violations[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
            }

        except Exception as e:
            logger.error(f"ä¸šåŠ¡è§„åˆ™æ£€æŸ¥å¤±è´¥: {e}")
            return {'error': str(e)}

    def _detect_anomalies(self) -> Dict:
        """æ£€æµ‹å¼‚å¸¸"""
        try:
            # ä½¿ç”¨éªŒè¯å™¨çš„ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
            stock_df = self.query_engine.get_stock_list()
            symbols = stock_df['symbol'].tolist() if not stock_df.empty else []

            sample_size = min(20, len(symbols))
            sample_symbols = np.random.choice(symbols, sample_size, replace=False) if symbols else []

            total_anomalies = 0
            anomaly_details = []

            for symbol in sample_symbols:
                results = self.validator.detect_statistical_anomalies(symbol)

                for result in results:
                    if result.result != result.result.PASS:
                        total_anomalies += result.affected_rows
                        anomaly_details.append({
                            'symbol': symbol,
                            'anomaly_type': result.rule_name,
                            'count': result.affected_rows,
                            'description': result.rule_description
                        })

            return {
                'sample_size': sample_size,
                'total_anomalies': total_anomalies,
                'anomaly_rate': total_anomalies / sample_size if sample_size > 0 else 0,
                'anomaly_details': anomaly_details[:5]
            }

        except Exception as e:
            logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            return {'error': str(e)}

    def _validate_adjustments(self) -> Dict:
        """éªŒè¯å¤æƒè®¡ç®—"""
        try:
            # æŠ½æ ·éªŒè¯å‡ åªè‚¡ç¥¨çš„å¤æƒè®¡ç®—
            stock_df = self.query_engine.get_stock_list()
            symbols = stock_df['symbol'].tolist() if not stock_df.empty else []

            sample_size = min(5, len(symbols))
            sample_symbols = np.random.choice(symbols, sample_size, replace=False) if symbols else []

            validation_results = []

            for symbol in sample_symbols:
                result = self.adjustor.validate_adjustment(symbol)
                validation_results.append(result)

            valid_count = sum(1 for r in validation_results if r['has_factors'])

            return {
                'sample_size': sample_size,
                'valid_adjustments': valid_count,
                'validation_rate': valid_count / sample_size if sample_size > 0 else 0,
                'results': validation_results
            }

        except Exception as e:
            logger.error(f"å¤æƒéªŒè¯å¤±è´¥: {e}")
            return {'error': str(e)}

    def _generate_daily_summary(self, checks: List) -> Dict:
        """ç”Ÿæˆæ¯æ—¥æ‘˜è¦"""
        summary = {
            'total_checks': len(checks),
            'check_time': datetime.now().isoformat(),
            'metrics': {}
        }

        for check in checks:
            check_type = check['type']
            result = check['result']

            if 'error' not in result:
                if check_type == 'completeness':
                    summary['metrics']['completeness_rate'] = result.get('completeness_rate', 0)
                elif check_type == 'business_rules':
                    summary['metrics']['violation_rate'] = result.get('violation_rate', 0)
                elif check_type == 'anomaly_detection':
                    summary['metrics']['anomaly_rate'] = result.get('anomaly_rate', 0)
                elif check_type == 'adjustment_validation':
                    summary['metrics']['adjustment_validation_rate'] = result.get('validation_rate', 0)

        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        quality_score = np.mean(list(summary['metrics'].values())) if summary['metrics'] else 0
        summary['quality_score'] = quality_score

        return summary

    def _check_alerts(self, summary: Dict) -> List[Dict]:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æŠ¥è­¦"""
        alerts = []

        metrics = summary.get('metrics', {})

        # æ£€æŸ¥å®Œæ•´æ€§ç‡
        completeness_rate = metrics.get('completeness_rate', 1.0)
        if completeness_rate < self.alert_thresholds['completeness_rate']:
            alerts.append({
                'type': 'completeness',
                'severity': 'WARNING' if completeness_rate > 0.9 else 'ERROR',
                'message': f'æ•°æ®å®Œæ•´æ€§ç‡ä½: {completeness_rate:.2%}',
                'threshold': self.alert_thresholds['completeness_rate']
            })

        # æ£€æŸ¥è¿è§„ç‡
        violation_rate = metrics.get('violation_rate', 0)
        if violation_rate > 0.1:  # 10%è¿è§„ç‡
            alerts.append({
                'type': 'business_rules',
                'severity': 'WARNING',
                'message': f'ä¸šåŠ¡è§„åˆ™è¿è§„ç‡é«˜: {violation_rate:.2%}',
                'threshold': 0.1
            })

        return alerts

    def _save_daily_report(self, report: Dict):
        """ä¿å­˜æ¯æ—¥æŠ¥å‘Š"""
        try:
            # ä¿å­˜ä¸ºJSONæ–‡ä»¶
            report_dir = 'reports/quality'
            import os
            os.makedirs(report_dir, exist_ok=True)

            date_str = datetime.now().strftime('%Y%m%d')
            filename = f"{report_dir}/quality_report_{date_str}.json"

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)

            logger.info(f"è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {filename}")

            # ä¹Ÿå¯ä»¥ä¿å­˜åˆ°æ•°æ®åº“
            self._save_report_to_db(report)

        except Exception as e:
            logger.error(f"ä¿å­˜è´¨é‡æŠ¥å‘Šå¤±è´¥: {e}")

    def _save_report_to_db(self, report: Dict):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ•°æ®åº“"""
        try:
            query = """
                INSERT INTO data_quality_log 
                (check_type, check_date, check_result, error_message, 
                 affected_rows, severity_level, suggestion)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
            """

            params = (
                'daily_summary',
                datetime.now().date(),
                'INFO',
                json.dumps(report.get('summary', {})),
                0,
                'INFO',
                'Daily quality check completed'
            )

            self.validator.db_connector.execute_query(query, params)

        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šåˆ°æ•°æ®åº“å¤±è´¥: {e}")

    def _send_alerts(self, alerts: List[Dict], report: Dict):
        """å‘é€æŠ¥è­¦"""
        if not self.alert_emails:
            return

        try:
            # æ„å»ºæŠ¥è­¦é‚®ä»¶
            subject = f"æ•°æ®è´¨é‡æŠ¥è­¦ - {datetime.now().strftime('%Y-%m-%d %H:%M')}"

            body = "æ•°æ®è´¨é‡æ£€æŸ¥å‘ç°ä»¥ä¸‹é—®é¢˜:\n\n"
            for alert in alerts:
                body += f"â€¢ [{alert['severity']}] {alert['message']}\n"

            body += f"\nè´¨é‡åˆ†æ•°: {report.get('summary', {}).get('quality_score', 0):.2%}\n"
            body += f"æ£€æŸ¥æ—¶é—´: {report.get('timestamp', 'N/A')}\n"

            # å‘é€é‚®ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
            # å®é™…åº”ç”¨ä¸­éœ€è¦é…ç½®SMTPæœåŠ¡å™¨
            logger.info(f"éœ€è¦å‘é€æŠ¥è­¦: {subject}")
            logger.info(f"æŠ¥è­¦å†…å®¹:\n{body}")

            # TODO: å®é™…é‚®ä»¶å‘é€é€»è¾‘
            # self._send_email(subject, body)

        except Exception as e:
            logger.error(f"å‘é€æŠ¥è­¦å¤±è´¥: {e}")

    def start_monitoring(self, interval_minutes: int = 60):
        """
        å¯åŠ¨å®šæ—¶ç›‘æ§

        Args:
            interval_minutes: ç›‘æ§é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
        """
        logger.info(f"å¯åŠ¨è´¨é‡ç›‘æ§ï¼Œé—´éš”{interval_minutes}åˆ†é’Ÿ")

        # ç«‹å³è¿è¡Œä¸€æ¬¡
        self.run_daily_check()

        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(interval_minutes).minutes.do(self.run_daily_check)

        # ä¿æŒè¿è¡Œ
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                logger.info("ç›‘æ§å·²åœæ­¢")
                break
            except Exception as e:
                logger.error(f"ç›‘æ§è¿è¡Œé”™è¯¯: {e}")
                time.sleep(300)  # å‡ºé”™åç­‰å¾…5åˆ†é’Ÿ

    def generate_weekly_report(self) -> Dict:
        """ç”Ÿæˆæ¯å‘¨è´¨é‡æŠ¥å‘Š"""
        try:
            # è·å–æœ€è¿‘7å¤©çš„æŠ¥å‘Š
            end_date = datetime.now()
            start_date = end_date - timedelta(days=7)

            report_files = []
            report_dir = 'reports/quality'
            import os
            import glob

            if os.path.exists(report_dir):
                pattern = os.path.join(report_dir, 'quality_report_*.json')
                report_files = glob.glob(pattern)

            weekly_reports = []
            for file in report_files:
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        report = json.load(f)
                        weekly_reports.append(report)
                except:
                    continue

            # åˆ†æå‘¨åº¦è¶‹åŠ¿
            trend_analysis = self._analyze_weekly_trend(weekly_reports)

            weekly_report = {
                'period': {
                    'start': start_date.strftime('%Y-%m-%d'),
                    'end': end_date.strftime('%Y-%m-%d')
                },
                'report_count': len(weekly_reports),
                'trend_analysis': trend_analysis,
                'recommendations': self._generate_recommendations(trend_analysis)
            }

            # ä¿å­˜å‘¨æŠ¥
            week_str = end_date.strftime('%Y%W')
            filename = f"{report_dir}/weekly_report_{week_str}.json"

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(weekly_report, f, indent=2, ensure_ascii=False)

            logger.info(f"å‘¨åº¦è´¨é‡æŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")

            return weekly_report

        except Exception as e:
            logger.error(f"ç”Ÿæˆå‘¨åº¦æŠ¥å‘Šå¤±è´¥: {e}")
            return {'error': str(e)}

    def _analyze_weekly_trend(self, reports: List[Dict]) -> Dict:
        """åˆ†æå‘¨åº¦è¶‹åŠ¿"""
        if not reports:
            return {'message': 'æ²¡æœ‰è¶³å¤Ÿçš„æŠ¥å‘Šæ•°æ®'}

        trends = {
            'quality_scores': [],
            'completeness_rates': [],
            'dates': []
        }

        for report in reports:
            summary = report.get('summary', {})
            metrics = summary.get('metrics', {})

            trends['quality_scores'].append(summary.get('quality_score', 0))
            trends['completeness_rates'].append(metrics.get('completeness_rate', 0))
            trends['dates'].append(report.get('timestamp', ''))

        # è®¡ç®—è¶‹åŠ¿
        if len(trends['quality_scores']) > 1:
            quality_trend = 'ä¸Šå‡' if trends['quality_scores'][-1] > trends['quality_scores'][0] else 'ä¸‹é™'
            completeness_trend = 'ä¸Šå‡' if trends['completeness_rates'][-1] > trends['completeness_rates'][0] else 'ä¸‹é™'
        else:
            quality_trend = 'ç¨³å®š'
            completeness_trend = 'ç¨³å®š'

        return {
            'avg_quality_score': np.mean(trends['quality_scores']) if trends['quality_scores'] else 0,
            'avg_completeness_rate': np.mean(trends['completeness_rates']) if trends['completeness_rates'] else 0,
            'quality_trend': quality_trend,
            'completeness_trend': completeness_trend,
            'best_score': max(trends['quality_scores']) if trends['quality_scores'] else 0,
            'worst_score': min(trends['quality_scores']) if trends['quality_scores'] else 0
        }

    def _generate_recommendations(self, trend_analysis: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        avg_quality = trend_analysis.get('avg_quality_score', 0)
        avg_completeness = trend_analysis.get('avg_completeness_rate', 0)

        if avg_quality < 0.9:
            recommendations.append("æ•´ä½“æ•°æ®è´¨é‡æœ‰å¾…æå‡ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æºå’ŒETLæµç¨‹")

        if avg_completeness < 0.95:
            recommendations.append("æ•°æ®å®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®è¡¥å……ç¼ºå¤±æ•°æ®æˆ–æ£€æŸ¥æ•°æ®é‡‡é›†è¿‡ç¨‹")

        if trend_analysis.get('quality_trend') == 'ä¸‹é™':
            recommendations.append("æ•°æ®è´¨é‡å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œéœ€è¦ç«‹å³è°ƒæŸ¥åŸå› ")

        if not recommendations:
            recommendations.append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰ç»´æŠ¤æµç¨‹")

        return recommendations

    def close(self):
        """å…³é—­è¿æ¥"""
        self.validator.close()
        self.adjustor.close()
        self.query_engine.close()
        logger.info("è´¨é‡ç›‘æ§å™¨è¿æ¥å·²å…³é—­")


def test_quality_monitor():
    """æµ‹è¯•è´¨é‡ç›‘æ§å™¨"""
    import logging

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("ğŸ§ª æµ‹è¯•è´¨é‡ç›‘æ§å™¨")
    print("=" * 50)

    monitor = QualityMonitor()

    try:
        # è¿è¡Œæ¯æ—¥æ£€æŸ¥
        print("\nğŸ“Š è¿è¡Œæ¯æ—¥è´¨é‡æ£€æŸ¥")
        daily_report = monitor.run_daily_check()

        if 'error' in daily_report:
            print(f"âŒ æ£€æŸ¥å¤±è´¥: {daily_report['error']}")
        else:
            print(f"âœ… æ£€æŸ¥å®Œæˆ: {len(daily_report['checks'])}é¡¹æ£€æŸ¥")

            summary = daily_report.get('summary', {})
            print(f"   è´¨é‡åˆ†æ•°: {summary.get('quality_score', 0):.2%}")
            print(f"   æ£€æŸ¥æ—¶é—´: {summary.get('check_time', 'N/A')}")

            if daily_report['alerts']:
                print(f"   å‘ç° {len(daily_report['alerts'])} ä¸ªæŠ¥è­¦:")
                for alert in daily_report['alerts']:
                    print(f"   [{alert['severity']}] {alert['message']}")
            else:
                print("   æ— æŠ¥è­¦")

        # ç”Ÿæˆå‘¨æŠ¥
        print("\nğŸ“ˆ ç”Ÿæˆå‘¨åº¦è´¨é‡æŠ¥å‘Š")
        weekly_report = monitor.generate_weekly_report()

        if 'error' in weekly_report:
            print(f"âŒ å‘¨æŠ¥ç”Ÿæˆå¤±è´¥: {weekly_report['error']}")
        else:
            print(f"âœ… å‘¨æŠ¥ç”Ÿæˆå®Œæˆ")
            period = weekly_report.get('period', {})
            print(f"   æŠ¥å‘ŠæœŸé—´: {period.get('start', 'N/A')} åˆ° {period.get('end', 'N/A')}")
            print(f"   æŠ¥å‘Šæ•°é‡: {weekly_report.get('report_count', 0)}")

            trend = weekly_report.get('trend_analysis', {})
            print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {trend.get('avg_quality_score', 0):.2%}")
            print(f"   è´¨é‡è¶‹åŠ¿: {trend.get('quality_trend', 'N/A')}")

            recommendations = weekly_report.get('recommendations', [])
            if recommendations:
                print(f"   æ”¹è¿›å»ºè®®:")
                for rec in recommendations:
                    print(f"   â€¢ {rec}")

        print("\nğŸ‰ è´¨é‡ç›‘æ§å™¨æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    finally:
        monitor.close()


if __name__ == "__main__":
    test_quality_monitor()
