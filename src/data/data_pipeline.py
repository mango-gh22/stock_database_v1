# _*_ coding: utf-8 _*_
# File Path: E:/MyFile/stock_database_v1/src/data\data_pipeline.py
# File Name: data_pipeline
# @ Author: mango-gh22
# @ Dateï¼š2025/12/7 19:48
"""
desc æ„å»ºä¸€ä¸ªå®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“ï¼Œæ•´åˆæ•°æ®é‡‡é›†ã€æ¸…æ´—ã€å¤„ç†å’Œå­˜å‚¨åŠŸèƒ½
stock_database_v1/src/data/data_pipeline.py
æ•°æ®ç®¡é“ - æ•´åˆé‡‡é›†ã€æ¸…æ´—ã€å¤„ç†å’Œå­˜å‚¨
"""

import logging
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from pathlib import Path
import json
import time

# å¯¼å…¥ç°æœ‰æ¨¡å—
from src.data.data_collector import BaseDataCollector

# from src.data.data_storage import DataStorage
# å…³é”®ä¿®å¤ï¼šä½¿ç”¨ AdaptiveDataStorage è€Œä¸æ˜¯ DataStorage
from src.data.adaptive_storage import AdaptiveDataStorage

from src.utils.code_converter import normalize_stock_code
from src.config.logging_config import setup_logging

# ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œå¯ä»¥ç»™ AdaptiveDataStorage èµ·ä¸ªåˆ«å
# DataStorage = AdaptiveDataStorage  # è¿™æ ·ä»£ç ä¸­å·²æœ‰çš„ DataStorage å¼•ç”¨ä»ç„¶æœ‰æ•ˆ

logger = setup_logging()


class DataPipeline:
    """æ•°æ®ç®¡é“ - æ•´åˆå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹"""

    def __init__(self,
                 collector: BaseDataCollector,
                 storage: AdaptiveDataStorage,  # è¿™é‡Œåº”è¯¥æ˜¯ AdaptiveDataStorage
                 config_path: str = 'config/database.yaml'):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡é“

        Args:
            collector: æ•°æ®é‡‡é›†å™¨å®ä¾‹
            storage: æ•°æ®å­˜å‚¨å™¨å®ä¾‹
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.collector = collector
        self.storage = storage  # è¿™åº”è¯¥æ˜¯ AdaptiveDataStorage çš„å®ä¾‹,ç±»å‹æ˜ç¡®ä¸º AdaptiveDataStorage
        self.config_path = config_path

        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        self.data_processor = DataProcessor()

        # ç¼“å­˜ç›®å½•
        self.cache_dir = Path('data/cache')
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # æŠ¥å‘Šç›®å½•
        self.report_dir = Path('data/reports')
        self.report_dir.mkdir(parents=True, exist_ok=True)


    def fetch_and_store_daily_data(self,
                                   symbol: str,
                                   start_date: str,
                                   end_date: str,
                                   auto_adjust: bool = True) -> Dict[str, Any]:
        """
        è·å–å¹¶å­˜å‚¨æ—¥çº¿æ•°æ®ï¼ˆå®Œæ•´æµç¨‹ï¼‰

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ 'YYYYMMDD'
            end_date: ç»“æŸæ—¥æœŸ 'YYYYMMDD'
            auto_adjust: æ˜¯å¦è‡ªåŠ¨è°ƒæ•´æ—¥æœŸèŒƒå›´

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        result = {
            'symbol': symbol,
            'start_date': start_date,
            'end_date': end_date,
            'status': 'pending',
            'records_fetched': 0,
            'records_stored': 0,
            'processing_time': 0,
            'errors': []
        }

        start_time = time.time()

        try:
            # 1. æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
            normalized_symbol = normalize_stock_code(symbol)
            logger.info(f"å¼€å§‹å¤„ç†è‚¡ç¥¨: {symbol} -> {normalized_symbol}")

            # 2. è‡ªåŠ¨è°ƒæ•´æ—¥æœŸèŒƒå›´ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if auto_adjust:
                last_update = self.storage.get_last_update_date(normalized_symbol, self.storage.supported_tables.get('daily', 'stock_daily_data'))
                if last_update:
                    # ä»æœ€åæ›´æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹
                    last_date = datetime.strptime(last_update, '%Y%m%d')
                    next_date = (last_date + timedelta(days=1)).strftime('%Y%m%d')
                    if next_date <= end_date:
                        start_date = next_date
                        logger.info(f"è°ƒæ•´å¼€å§‹æ—¥æœŸä¸º: {start_date} (åŸºäºæœ€åæ›´æ–°æ—¥æœŸ)")

            # 3. è·å–æ•°æ®
            logger.info(f"è·å–æ•°æ®: {normalized_symbol} [{start_date} - {end_date}]")
            df_raw = self.collector.fetch_daily_data(normalized_symbol, start_date, end_date)

            if df_raw.empty:
                result['status'] = 'no_data'
                result['processing_time'] = time.time() - start_time
                logger.warning(f"æœªè·å–åˆ°æ•°æ®: {normalized_symbol}")
                return result

            result['records_fetched'] = len(df_raw)

            # 4. æ•°æ®æ¸…æ´—
            logger.info(f"æ¸…æ´—æ•°æ®: {normalized_symbol} ({len(df_raw)} æ¡è®°å½•)")
            df_clean = self.data_processor.clean_daily_data(df_raw, normalized_symbol)

            # 5. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            logger.info(f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡: {normalized_symbol}")
            df_with_indicators = self.data_processor.calculate_technical_indicators(df_clean)

            # 6. æ•°æ®éªŒè¯
            logger.info(f"éªŒè¯æ•°æ®è´¨é‡: {normalized_symbol}")
            quality_report = self.data_processor.validate_data_quality(df_with_indicators)

            if quality_report['status'] == 'poor':
                logger.warning(f"æ•°æ®è´¨é‡è¾ƒå·®: {normalized_symbol}, è´¨é‡è¯„åˆ†: {quality_report['quality_score']}")
                result['warnings'].append(f"æ•°æ®è´¨é‡è¯„åˆ†è¾ƒä½: {quality_report['quality_score']}")

            # 7. å­˜å‚¨æ•°æ® - å…³é”®ä¿®å¤ï¼šæå–å…ƒç»„ä¸­çš„æ•´æ•°
            logger.info(f"å­˜å‚¨æ•°æ®: {normalized_symbol}")
            storage_result = self.storage.store_daily_data(df_with_indicators)  # è¿”å›å…ƒç»„

            # ========== å…³é”®ä¿®å¤ï¼šæå–è¡Œæ•° ==========
            if isinstance(storage_result, tuple) and len(storage_result) >= 2:
                rows_affected = storage_result[0]  # å…ƒç»„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯è¡Œæ•°
                storage_status = storage_result[1]  # ç¬¬äºŒä¸ªå…ƒç´ æ˜¯çŠ¶æ€ä¿¡æ¯
            else:
                rows_affected = storage_result if isinstance(storage_result, int) else 0
                storage_status = {}

            # ç¡®ä¿ rows_affected æ˜¯æ•´æ•°
            rows_affected = int(rows_affected) if rows_affected else 0
            result['records_stored'] = rows_affected  # å­˜å‚¨æ•´æ•°ï¼Œä¸æ˜¯å…ƒç»„ï¼

            # 8. è®°å½•æ›´æ–°æ—¥å¿— - ä¼ é€’æ•´æ•°
            execution_time = time.time() - start_time

            # æ ¹æ®å­˜å‚¨çŠ¶æ€ç¡®å®šæ—¥å¿—çŠ¶æ€
            log_status = 'success'
            log_error = None

            if isinstance(storage_status, dict):
                status_info = storage_status.get('status', '')
                if status_info == 'skipped':
                    log_status = 'skipped'
                    log_error = storage_status.get('reason', '')
                elif status_info == 'error':
                    log_status = 'error'
                    log_error = storage_status.get('error', '')

            self.storage.log_data_update(
                data_type=self.storage.supported_tables.get('daily', 'stock_daily_data'),
                symbol=normalized_symbol,
                start_date=start_date,
                end_date=end_date,
                rows_affected=rows_affected,  # ä¼ é€’æ•´æ•°
                status=log_status,
                error_message=log_error,
                execution_time=execution_time
            )

            result['status'] = 'success'
            result['processing_time'] = execution_time
            result['quality_score'] = quality_report['quality_score']

            logger.info(f"å¤„ç†å®Œæˆ: {normalized_symbol}, å­˜å‚¨ {rows_affected} æ¡è®°å½•")

        except Exception as e:
            result['status'] = 'error'
            result['errors'].append(str(e))
            result['processing_time'] = time.time() - start_time

            # è®°å½•é”™è¯¯æ—¥å¿—
            self.storage.log_data_update(
                data_type=self.storage.supported_tables.get('daily', 'stock_daily_data'),
                symbol=normalized_symbol if 'normalized_symbol' in locals() else symbol,
                start_date=start_date,
                end_date=end_date,
                rows_affected=0,
                status='error',
                error_message=str(e),
                execution_time=time.time() - start_time
            )

            logger.error(f"å¤„ç†å¤±è´¥: {symbol}, é”™è¯¯: {e}")

        return result

    # åœ¨ DataPipeline ç±»ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•
    def run_incremental_update(self, market: str = "ä¸Šè¯", days_back: int = 7,
                               limit: int = 20, max_concurrent: int = 3) -> Dict[str, Any]:
        """
        å¢é‡æ›´æ–° - é«˜å±‚ä¸šåŠ¡æ¥å£ï¼ˆé—¨é¢æ¨¡å¼ï¼‰

        Args:
            market: å¸‚åœºç±»å‹ (ä¸Šè¯/æ·±è¯/ç§‘åˆ›æ¿ç­‰)
            days_back: å›æº¯å¤©æ•°
            limit: è‚¡ç¥¨æ•°é‡é™åˆ¶
            max_concurrent: æœ€å¤§å¹¶å‘æ•°

        Returns:
            æ›´æ–°ç»“æœå­—å…¸
        """
        from datetime import datetime, timedelta
        import logging
        logger = logging.getLogger(__name__)

        logger.info(f"ğŸ”„ å¼€å§‹å¢é‡æ›´æ–°: {market}ï¼Œå›æº¯ {days_back} å¤©ï¼Œé™åˆ¶ {limit} åªè‚¡ç¥¨")

        try:
            # 1. è·å–æŒ‡å®šå¸‚åœºçš„è‚¡ç¥¨åˆ—è¡¨
            logger.info(f"ğŸ“‹ è·å–{market}è‚¡ç¥¨åˆ—è¡¨...")
            stock_list = self.collector.fetch_stock_list(market)

            if stock_list.empty:
                logger.warning(f"âš ï¸ æœªè·å–åˆ°{market}è‚¡ç¥¨åˆ—è¡¨")
                return {
                    "status": "error",
                    "success": False,
                    "message": f"æœªè·å–åˆ°{market}è‚¡ç¥¨åˆ—è¡¨"
                }

            # 2. æå–è‚¡ç¥¨ä»£ç ï¼ˆé™åˆ¶æ•°é‡ï¼‰
            symbols = stock_list['symbol'].head(limit).tolist()
            logger.info(f"ğŸ“Š å¤„ç† {len(symbols)} åªè‚¡ç¥¨: {symbols[:3]}...")

            # 3. è®¡ç®—å¢é‡æ—¥æœŸèŒƒå›´
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y%m%d')
            logger.info(f"ğŸ“… æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")

            # 4. è°ƒç”¨åº•å±‚çš„æ‰¹é‡å¤„ç†æ–¹æ³•
            logger.info(f"âš¡ å¼€å§‹æ‰¹é‡å¤„ç†ï¼Œå¹¶å‘æ•°: {max_concurrent}")
            batch_result = self.batch_process_stocks(
                symbols=symbols,
                start_date=start_date,
                end_date=end_date,
                max_concurrent=max_concurrent
            )

            # 5. æ•´ç†ç»“æœï¼ŒåŒ¹é… run.py ä¸­çš„æœŸæœ›æ ¼å¼
            result = {
                "status": "success",
                "success": batch_result['success'] > 0,  # åªè¦æœ‰æˆåŠŸå°±ä¸ºTrue
                "market": market,
                "days_back": days_back,
                "total_symbols": batch_result['total_symbols'],
                "success_count": batch_result['success'],
                "failed": batch_result['failed'],
                "no_data": batch_result['no_data'],
                "total_records": batch_result['total_records'],
                "new_records": batch_result['total_records'],  # å‡è®¾æ‰€æœ‰è®°å½•éƒ½æ˜¯æ–°çš„
                "updated_records": batch_result['success'],  # æˆåŠŸå¤„ç†çš„è‚¡ç¥¨æ•°
                "duration": batch_result['processing_time'],
                "batch_id": batch_result.get('batch_id', '')
            }

            logger.info(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: æˆåŠŸ {result['success_count']} åªï¼Œå¤±è´¥ {result['failed']} åª")

            return result

        except Exception as e:
            logger.error(f"âŒ å¢é‡æ›´æ–°å¤±è´¥: {e}")
            return {
                "status": "error",
                "success": False,
                "error": str(e)
            }



    def batch_process_stocks(self,
                             symbols: List[str],
                             start_date: str,
                             end_date: str,
                             max_concurrent: int = 3) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å¤šåªè‚¡ç¥¨

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            max_concurrent: æœ€å¤§å¹¶å‘æ•°

        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        batch_result = {
            'batch_id': f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'start_time': datetime.now().isoformat(),
            'total_symbols': len(symbols),
            'processed': 0,
            'success': 0,
            'failed': 0,
            'no_data': 0,
            'total_records': 0,
            'symbol_results': [],
            'processing_time': 0
        }

        batch_start_time = time.time()

        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(symbols)} åªè‚¡ç¥¨")

        # é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…APIé™åˆ¶
        import concurrent.futures

        def process_single(symbol: str) -> Dict[str, Any]:
            """å¤„ç†å•åªè‚¡ç¥¨"""
            try:
                result = self.fetch_and_store_daily_data(symbol, start_date, end_date)
                return result
            except Exception as e:
                return {
                    'symbol': symbol,
                    'status': 'error',
                    'errors': [str(e)],
                    'processing_time': 0
                }

        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            future_to_symbol = {
                executor.submit(process_single, symbol): symbol
                for symbol in symbols
            }

            for future in concurrent.futures.as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                try:
                    result = future.result()
                    batch_result['symbol_results'].append(result)

                    if result['status'] == 'success':
                        batch_result['success'] += 1
                        # ========== å…³é”®ä¿®å¤ï¼šç¡®ä¿æ˜¯æ•´æ•° ==========
                        records = result.get('records_stored', 0)
                        if isinstance(records, tuple):
                            records = records[0] if records else 0
                        batch_result['total_records'] += int(records)  # è½¬æ¢ä¸ºæ•´æ•°
                    elif result['status'] == 'no_data':
                        batch_result['no_data'] += 1
                    else:
                        batch_result['failed'] += 1

                    batch_result['processed'] += 1

                    # è¿›åº¦æ—¥å¿—
                    if batch_result['processed'] % 10 == 0:
                        logger.info(f"å¤„ç†è¿›åº¦: {batch_result['processed']}/{batch_result['total_symbols']}")

                except Exception as e:
                    logger.error(f"å¤„ç†è‚¡ç¥¨æ—¶å‘ç”Ÿå¼‚å¸¸ {symbol}: {e}")
                    batch_result['failed'] += 1
                    batch_result['processed'] += 1

        # å®Œæˆæ‰¹é‡å¤„ç†
        batch_result['end_time'] = datetime.now().isoformat()
        batch_result['processing_time'] = time.time() - batch_start_time

        # ç”ŸæˆæŠ¥å‘Š
        self._generate_batch_report(batch_result)

        logger.info(
            f"æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {batch_result['success']}, å¤±è´¥ {batch_result['failed']}, æ— æ•°æ® {batch_result['no_data']}")

        return batch_result

    def _generate_batch_report(self, batch_result: Dict[str, Any]):
        """ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Š"""
        report = {
            'batch_id': batch_result['batch_id'],
            'start_time': batch_result['start_time'],
            'end_time': batch_result['end_time'],
            'total_processing_time': round(batch_result['processing_time'], 2),

            'summary': {
                'total_symbols': batch_result['total_symbols'],
                'success': batch_result['success'],
                'failed': batch_result['failed'],
                'no_data': batch_result['no_data'],
                'total_records': batch_result['total_records']
            },

            'performance': {
                'symbols_per_second': round(batch_result['total_symbols'] / batch_result['processing_time'], 2) if
                batch_result['processing_time'] > 0 else 0,
                'records_per_second': round(batch_result['total_records'] / batch_result['processing_time'], 2) if
                batch_result['processing_time'] > 0 else 0
            },

            'detailed_results': []
        }

        # æ·»åŠ è¯¦ç»†ç»“æœ
        for result in batch_result['symbol_results']:
            detailed = {
                'symbol': result.get('symbol'),
                'status': result.get('status'),
                'records_fetched': result.get('records_fetched', 0),
                'records_stored': result.get('records_stored', 0),
                'processing_time': result.get('processing_time', 0),
                'quality_score': result.get('quality_score', 0),
                'errors': result.get('errors', [])
            }
            report['detailed_results'].append(detailed)

        # ä¿å­˜æŠ¥å‘Š
        report_file = self.report_dir / f"{batch_result['batch_id']}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        logger.info(f"æ‰¹é‡å¤„ç†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        # ç”Ÿæˆç®€è¦æ–‡æœ¬æŠ¥å‘Š
        text_report = f"""
æ‰¹é‡å¤„ç†æŠ¥å‘Š
============
æ‰¹æ¬¡ID: {batch_result['batch_id']}
å¤„ç†æ—¶é—´: {batch_result['start_time']} - {batch_result['end_time']}
æ€»è€—æ—¶: {round(batch_result['processing_time'], 2)} ç§’

æ±‡æ€»ç»Ÿè®¡:
--------
æ€»è‚¡ç¥¨æ•°: {batch_result['total_symbols']}
æˆåŠŸ: {batch_result['success']}
å¤±è´¥: {batch_result['failed']}
æ— æ•°æ®: {batch_result['no_data']}
æ€»è®°å½•æ•°: {batch_result['total_records']}

æ€§èƒ½æŒ‡æ ‡:
--------
è‚¡ç¥¨/ç§’: {report['performance']['symbols_per_second']}
è®°å½•/ç§’: {report['performance']['records_per_second']}

è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹: {report_file}
        """

        print(text_report)



class DataProcessor:
    """æ•°æ®å¤„ç†ç±» - è´Ÿè´£æ•°æ®æ¸…æ´—å’Œè®¡ç®—"""

    def __init__(self):
        self.required_columns = [
            'trade_date', 'open', 'close', 'high', 'low',
            'volume', 'amount', 'pre_close'
        ]

    def clean_daily_data(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        æ¸…æ´—æ—¥çº¿æ•°æ®

        Args:
            df: åŸå§‹æ•°æ®
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            æ¸…æ´—åçš„æ•°æ®
        """
        if df.empty:
            return df

        df_clean = df.copy()

        # 1. æ·»åŠ è‚¡ç¥¨ä»£ç 
        if 'symbol' not in df_clean.columns:
            df_clean['symbol'] = symbol

        # 2. ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
        if 'trade_date' in df_clean.columns:
            df_clean['trade_date'] = pd.to_datetime(df_clean['trade_date']).dt.strftime('%Y%m%d')

        # 3. å¤„ç†ç¼ºå¤±å€¼
        for col in ['open', 'high', 'low', 'close']:
            if col in df_clean.columns:
                # ä»·æ ¼æ•°æ®ä½¿ç”¨å‰å‘å¡«å……
                df_clean[col] = df_clean[col].fillna(method='ffill').fillna(method='bfill')

        if 'volume' in df_clean.columns:
            df_clean['volume'] = df_clean['volume'].fillna(0)

        if 'amount' in df_clean.columns:
            df_clean['amount'] = df_clean['amount'].fillna(0)

        # 4. éªŒè¯ä»·æ ¼æ•°æ®
        df_clean = self._validate_price_data(df_clean)

        # 5. å»é™¤é‡å¤æ•°æ®
        df_clean = df_clean.drop_duplicates(subset=['symbol', 'trade_date'], keep='last')

        # 6. æ’åº
        df_clean = df_clean.sort_values('trade_date')
        df_clean = df_clean.reset_index(drop=True)

        # 7. æ·»åŠ å¤„ç†æ ‡è®°
        df_clean['processed_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        df_clean['data_source'] = 'processed'

        return df_clean

    def _validate_price_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """éªŒè¯ä»·æ ¼æ•°æ®åˆç†æ€§"""
        if df.empty:
            return df

        # åŸºæœ¬éªŒè¯è§„åˆ™
        valid_mask = pd.Series(True, index=df.index)

        # 1. ä»·æ ¼å¿…é¡»ä¸ºæ­£
        for col in ['open', 'high', 'low', 'close']:
            if col in df.columns:
                valid_mask &= (df[col] > 0)

        # 2. high >= low
        if all(col in df.columns for col in ['high', 'low']):
            valid_mask &= (df['high'] >= df['low'])

        # 3. ä»·æ ¼åœ¨é«˜ä½èŒƒå›´å†…
        if all(col in df.columns for col in ['open', 'high', 'low']):
            valid_mask &= (df['open'] >= df['low']) & (df['open'] <= df['high'])

        if all(col in df.columns for col in ['close', 'high', 'low']):
            valid_mask &= (df['close'] >= df['low']) & (df['close'] <= df['high'])

        # ç§»é™¤æ— æ•ˆæ•°æ®
        df_valid = df[valid_mask].copy()

        if len(df) != len(df_valid):
            logger.warning(f"ç§»é™¤äº† {len(df) - len(df_valid)} æ¡æ— æ•ˆä»·æ ¼æ•°æ®")

        return df_valid

    def calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—æŠ€æœ¯æŒ‡æ ‡

        Args:
            df: æ¸…æ´—åçš„æ•°æ®

        Returns:
            åŒ…å«æŠ€æœ¯æŒ‡æ ‡çš„æ•°æ®
        """
        if df.empty or 'close' not in df.columns:
            return df

        df_indicators = df.copy()

        # ç¡®ä¿æ•°æ®å·²æ’åº
        df_indicators = df_indicators.sort_values('trade_date')

        # è®¡ç®—æ¶¨è·Œå¹…
        if 'pre_close' in df_indicators.columns:
            df_indicators['change'] = df_indicators['close'] - df_indicators['pre_close']
            df_indicators['pct_change'] = (df_indicators['change'] / df_indicators['pre_close'] * 100).round(4)

        # è®¡ç®—æŒ¯å¹…
        if all(col in df_indicators.columns for col in ['high', 'low', 'pre_close']):
            df_indicators['amplitude'] = (
                        (df_indicators['high'] - df_indicators['low']) / df_indicators['pre_close'] * 100).round(4)

        # è®¡ç®—ç§»åŠ¨å¹³å‡çº¿
        close_prices = df_indicators['close'].astype(float)

        ma_periods = [5, 10, 20, 30, 60, 120, 250]
        for period in ma_periods:
            df_indicators[f'ma{period}'] = close_prices.rolling(window=period).mean().round(4)

        # è®¡ç®—æˆäº¤é‡å‡çº¿
        if 'volume' in df_indicators.columns:
            volume_series = df_indicators['volume'].astype(float)
            for period in [5, 10, 20]:
                df_indicators[f'volume_ma{period}'] = volume_series.rolling(window=period).mean().round(2)

        # è®¡ç®—æˆäº¤é‡æ¯”
        if 'volume_ma5' in df_indicators.columns and 'volume' in df_indicators.columns:
            df_indicators['volume_ratio'] = (df_indicators['volume'] / df_indicators['volume_ma5']).round(2)

        # è®¡ç®—æ¢æ‰‹ç‡ï¼ˆéœ€è¦æµé€šè‚¡æœ¬æ•°æ®ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–è®¡ç®—ï¼‰
        if 'amount' in df_indicators.columns and 'volume' in df_indicators.columns:
            # å‡è®¾å¹³å‡ä»·æ ¼
            avg_price = (df_indicators['high'] + df_indicators['low']) / 2
            turnover = df_indicators['volume'] * avg_price
            df_indicators['turnover_rate'] = (turnover / df_indicators['amount']).round(4)

        return df_indicators

    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        éªŒè¯æ•°æ®è´¨é‡

        Args:
            df: æ•°æ®

        Returns:
            è´¨é‡æŠ¥å‘Š
        """
        report = {
            'total_records': len(df),
            'missing_values': {},
            'duplicates': 0,
            'price_issues': 0,
            'volume_issues': 0,
            'quality_score': 100,
            'status': 'excellent'
        }

        if df.empty:
            report['status'] = 'empty'
            report['quality_score'] = 0
            return report

        # 1. æ£€æŸ¥ç¼ºå¤±å€¼
        for col in self.required_columns:
            if col in df.columns:
                missing_count = df[col].isnull().sum()
                if missing_count > 0:
                    report['missing_values'][col] = int(missing_count)

        # 2. æ£€æŸ¥é‡å¤æ•°æ®
        if 'symbol' in df.columns and 'trade_date' in df.columns:
            duplicates = df.duplicated(subset=['symbol', 'trade_date']).sum()
            report['duplicates'] = int(duplicates)

        # 3. æ£€æŸ¥ä»·æ ¼é—®é¢˜
        price_cols = ['open', 'high', 'low', 'close']
        price_issues = 0

        for col in price_cols:
            if col in df.columns:
                # æ£€æŸ¥è´Ÿå€¼
                negative = (df[col] <= 0).sum()
                price_issues += negative

        # æ£€æŸ¥ä»·æ ¼å…³ç³»
        if all(col in df.columns for col in ['high', 'low']):
            invalid_high_low = (df['high'] < df['low']).sum()
            price_issues += invalid_high_low

        report['price_issues'] = int(price_issues)

        # 4. æ£€æŸ¥æˆäº¤é‡é—®é¢˜
        if 'volume' in df.columns:
            negative_volume = (df['volume'] < 0).sum()
            report['volume_issues'] = int(negative_volume)

        # 5. è®¡ç®—è´¨é‡è¯„åˆ†
        penalty = 0

        # ç¼ºå¤±å€¼æƒ©ç½š
        for col, count in report['missing_values'].items():
            penalty += (count / len(df)) * 20

        # é‡å¤æ•°æ®æƒ©ç½š
        if report['duplicates'] > 0:
            penalty += (report['duplicates'] / len(df)) * 30

        # ä»·æ ¼é—®é¢˜æƒ©ç½š
        if report['price_issues'] > 0:
            penalty += (report['price_issues'] / len(df)) * 50

        # æˆäº¤é‡é—®é¢˜æƒ©ç½š
        if report['volume_issues'] > 0:
            penalty += min(report['volume_issues'] * 10, 100)

        quality_score = max(0, 100 - penalty)
        report['quality_score'] = round(quality_score, 1)

        # 6. ç¡®å®šçŠ¶æ€
        if quality_score >= 90:
            report['status'] = 'excellent'
        elif quality_score >= 70:
            report['status'] = 'good'
        elif quality_score >= 50:
            report['status'] = 'fair'
        elif quality_score >= 30:
            report['status'] = 'poor'
        else:
            report['status'] = 'very_poor'

        return report

    def calculate_advanced_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—é«˜çº§æŠ€æœ¯æŒ‡æ ‡

        Args:
            df: åŒ…å«åŸºç¡€æŒ‡æ ‡çš„æ•°æ®

        Returns:
            åŒ…å«é«˜çº§æŒ‡æ ‡çš„æ•°æ®
        """
        if df.empty or 'close' not in df.columns:
            return df

        df_advanced = df.copy()

        # ç¡®ä¿æ•°æ®æ’åº
        df_advanced = df_advanced.sort_values('trade_date')
        close_prices = df_advanced['close'].astype(float)

        # 1. RSI (ç›¸å¯¹å¼ºå¼±æŒ‡æ•°)
        delta = close_prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df_advanced['rsi'] = (100 - (100 / (1 + rs))).round(2)

        # 2. MACD
        ema12 = close_prices.ewm(span=12, adjust=False).mean()
        ema26 = close_prices.ewm(span=26, adjust=False).mean()
        df_advanced['macd'] = (ema12 - ema26).round(4)
        df_advanced['macd_signal'] = df_advanced['macd'].ewm(span=9, adjust=False).mean().round(4)
        df_advanced['macd_hist'] = (df_advanced['macd'] - df_advanced['macd_signal']).round(4)

        # 3. å¸ƒæ—å¸¦
        window = 20
        df_advanced['bb_middle'] = close_prices.rolling(window=window).mean()
        bb_std = close_prices.rolling(window=window).std()
        df_advanced['bb_upper'] = df_advanced['bb_middle'] + 2 * bb_std
        df_advanced['bb_lower'] = df_advanced['bb_middle'] - 2 * bb_std
        df_advanced['bb_width'] = (
                    (df_advanced['bb_upper'] - df_advanced['bb_lower']) / df_advanced['bb_middle'] * 100).round(2)

        # 4. ATR (å¹³å‡çœŸå®æ³¢å¹…)
        high_low = df_advanced['high'] - df_advanced['low']
        high_close = abs(df_advanced['high'] - df_advanced['close'].shift())
        low_close = abs(df_advanced['low'] - df_advanced['close'].shift())
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        df_advanced['atr'] = true_range.rolling(window=14).mean().round(4)

        # 5. æ³¢åŠ¨ç‡
        if 'pct_change' in df_advanced.columns:
            df_advanced['volatility_20d'] = df_advanced['pct_change'].rolling(window=20).std() * np.sqrt(252)

        return df_advanced


class TushareDataCollector(BaseDataCollector):
    """Tushareæ•°æ®é‡‡é›†å™¨å®ç°"""

    def __init__(self, config_path: str = 'config/database.yaml'):
        super().__init__(config_path)
        self._init_tushare()

    def _init_tushare(self):
        """åˆå§‹åŒ–Tushare"""
        try:
            import tushare as ts
            from src.config.config_loader import load_tushare_config

            config = load_tushare_config()
            token = config.get('token')

            if token:
                ts.set_token(token)
                self.pro = ts.pro_api()
                logger.info("Tushare APIåˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("æœªé…ç½®Tushare token")
                self.pro = None
        except ImportError:
            logger.error("æœªå®‰è£…tushareåº“")
            self.pro = None
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–Tushareå¤±è´¥: {e}")
            self.pro = None

    def fetch_daily_data(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """è·å–æ—¥çº¿æ•°æ®"""
        if not self.pro:
            logger.error("Tushareæœªåˆå§‹åŒ–")
            return pd.DataFrame()

        try:
            # æ‰§è¡Œé€Ÿç‡é™åˆ¶
            self.enforce_rate_limit()

            # è½¬æ¢ä»£ç æ ¼å¼
            ts_code = self._convert_to_ts_code(symbol)

            # è·å–æ•°æ®
            df = self.pro.daily(
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date
            )

            if df is not None and not df.empty:
                # é‡å‘½ååˆ—ä»¥åŒ¹é…æˆ‘ä»¬çš„æ ¼å¼
                column_mapping = {
                    'ts_code': 'symbol',
                    'trade_date': 'trade_date',
                    'open': 'open',
                    'high': 'high',
                    'low': 'low',
                    'close': 'close',
                    'pre_close': 'pre_close',
                    'change': 'change',
                    'pct_chg': 'pct_change',
                    'vol': 'volume',
                    'amount': 'amount'
                }

                df = df.rename(columns=column_mapping)
                df['symbol'] = symbol  # ä½¿ç”¨æ ‡å‡†åŒ–ä»£ç 

                return df
            else:
                logger.warning(f"æœªè·å–åˆ°æ•°æ®: {symbol}")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"è·å–æ—¥çº¿æ•°æ®å¤±è´¥ {symbol}: {e}")
            return pd.DataFrame()

    def fetch_minute_data(self, symbol: str, trade_date: str, freq: str = '1min') -> pd.DataFrame:
        """è·å–åˆ†é’Ÿçº¿æ•°æ®"""
        if not self.pro:
            logger.error("Tushareæœªåˆå§‹åŒ–")
            return pd.DataFrame()

        try:
            self.enforce_rate_limit()

            ts_code = self._convert_to_ts_code(symbol)

            df = self.pro.ft_mins(
                ts_code=ts_code,
                freq=freq,
                start_date=trade_date,
                end_date=trade_date
            )

            if df is not None and not df.empty:
                df = df.rename(columns={
                    'ts_code': 'symbol',
                    'trade_time': 'trade_time',
                    'open': 'open',
                    'high': 'high',
                    'low': 'low',
                    'close': 'close',
                    'vol': 'volume',
                    'amount': 'amount'
                })

                df['symbol'] = symbol
                df['trade_date'] = trade_date

                return df
            else:
                logger.warning(f"æœªè·å–åˆ°åˆ†é’Ÿæ•°æ®: {symbol} {trade_date}")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"è·å–åˆ†é’Ÿæ•°æ®å¤±è´¥ {symbol}: {e}")
            return pd.DataFrame()

    def fetch_basic_info(self, symbol: str) -> Dict[str, Any]:
        """è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
        if not self.pro:
            return {}

        try:
            self.enforce_rate_limit()

            ts_code = self._convert_to_ts_code(symbol)

            df = self.pro.stock_basic(
                ts_code=ts_code,
                fields='ts_code,symbol,name,area,industry,market,list_date,is_hs'
            )

            if df is not None and not df.empty:
                return df.iloc[0].to_dict()
            else:
                return {}

        except Exception as e:
            logger.error(f"è·å–åŸºæœ¬ä¿¡æ¯å¤±è´¥ {symbol}: {e}")
            return {}

    def _convert_to_ts_code(self, normalized_code: str) -> str:
        """å°†æ ‡å‡†åŒ–ä»£ç è½¬æ¢ä¸ºTushareæ ¼å¼"""
        if normalized_code.startswith('sh'):
            return f"{normalized_code[2:]}.SH"
        elif normalized_code.startswith('sz'):
            return f"{normalized_code[2:]}.SZ"
        else:
            return normalized_code


# ä½¿ç”¨ç¤ºä¾‹
def main():
    """æ•°æ®ç®¡é“ä½¿ç”¨ç¤ºä¾‹"""
    print("è‚¡ç¥¨æ•°æ®ç®¡é“ç¤ºä¾‹")
    print("=" * 50)

    # 1. åˆ›å»ºé‡‡é›†å™¨
    print("åˆå§‹åŒ–æ•°æ®é‡‡é›†å™¨...")
    collector = TushareDataCollector()

    # 2. åˆ›å»ºå­˜å‚¨å™¨
    print("åˆå§‹åŒ–æ•°æ®å­˜å‚¨å™¨...")
    storage = DataStorage()

    # 3. åˆ›å»ºæ•°æ®ç®¡é“
    print("åˆ›å»ºæ•°æ®ç®¡é“...")
    pipeline = DataPipeline(collector, storage)

    # 4. ç¤ºä¾‹è‚¡ç¥¨ä»£ç 
    sample_symbols = [
        '600519',  # è´µå·èŒ…å°
        '000001',  # å¹³å®‰é“¶è¡Œ
        '000858'  # äº”ç²®æ¶²
    ]

    # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
    normalized_symbols = []
    for symbol in sample_symbols:
        try:
            normalized = normalize_stock_code(symbol)
            normalized_symbols.append(normalized)
            print(f"åŸå§‹ä»£ç : {symbol} -> æ ‡å‡†åŒ–: {normalized}")
        except Exception as e:
            print(f"ä»£ç è½¬æ¢å¤±è´¥ {symbol}: {e}")

    if not normalized_symbols:
        print("æ²¡æœ‰æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç ï¼Œä½¿ç”¨ç¤ºä¾‹ä»£ç ")
        normalized_symbols = ['sh600519', 'sz000001']

    # 5. æ‰¹é‡å¤„ç†æ•°æ®
    print(f"\næ‰¹é‡å¤„ç† {len(normalized_symbols)} åªè‚¡ç¥¨æ•°æ®...")
    batch_result = pipeline.batch_process_stocks(
        symbols=normalized_symbols,
        start_date='20240101',
        end_date='20241231',
        max_concurrent=2
    )

    # 6. æ˜¾ç¤ºç»“æœ
    print(f"\næ‰¹é‡å¤„ç†å®Œæˆ:")
    print(f"æ€»è‚¡ç¥¨æ•°: {batch_result['total_symbols']}")
    print(f"æˆåŠŸ: {batch_result['success']}")
    print(f"å¤±è´¥: {batch_result['failed']}")
    print(f"æ— æ•°æ®: {batch_result['no_data']}")
    print(f"æ€»è®°å½•æ•°: {batch_result['total_records']}")
    print(f"æ€»è€—æ—¶: {round(batch_result['processing_time'], 2)} ç§’")

    # 7. æ•°æ®å¤„ç†ç¤ºä¾‹
    print(f"\næ•°æ®å¤„ç†ç¤ºä¾‹:")
    processor = DataProcessor()

    # è·å–å¹¶å¤„ç†å•åªè‚¡ç¥¨æ•°æ®
    for symbol in normalized_symbols[:1]:  # åªå¤„ç†ç¬¬ä¸€åªè‚¡ç¥¨ä½œä¸ºç¤ºä¾‹
        try:
            print(f"\nå¤„ç†è‚¡ç¥¨: {symbol}")

            # ä»æ•°æ®åº“è·å–æ•°æ®ï¼ˆå‡è®¾å·²ç»å­˜å‚¨ï¼‰
            last_update = storage.get_last_update_date(symbol, self.storage.supported_tables.get('daily', 'stock_daily_data'))
            if last_update:
                # è·å–æœ€è¿‘30å¤©æ•°æ®
                end_date = last_update
                start_date = (datetime.strptime(end_date, '%Y%m%d') - timedelta(days=30)).strftime('%Y%m%d')

                # ä½¿ç”¨é‡‡é›†å™¨è·å–æ•°æ®
                df_raw = collector.fetch_daily_data(symbol, start_date, end_date)

                if not df_raw.empty:
                    # æ¸…æ´—æ•°æ®
                    df_clean = processor.clean_daily_data(df_raw, symbol)

                    # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                    df_indicators = processor.calculate_technical_indicators(df_clean)

                    # è®¡ç®—é«˜çº§æŒ‡æ ‡
                    df_advanced = processor.calculate_advanced_indicators(df_indicators)

                    # éªŒè¯æ•°æ®è´¨é‡
                    quality_report = processor.validate_data_quality(df_advanced)

                    print(f"åŸå§‹æ•°æ®: {len(df_raw)} æ¡")
                    print(f"æ¸…æ´—åæ•°æ®: {len(df_clean)} æ¡")
                    print(f"æ•°æ®è´¨é‡è¯„åˆ†: {quality_report['quality_score']} - {quality_report['status']}")
                    print(f"æ•°æ®åˆ—: {list(df_advanced.columns)}")

                    # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
                    print(f"\næ•°æ®ç¤ºä¾‹:")
                    print(df_advanced[['trade_date', 'open', 'close', 'volume', 'ma5', 'ma10']].head())
                else:
                    print(f"æœªè·å–åˆ°æ•°æ®: {symbol}")
            else:
                print(f"æœªæ‰¾åˆ°æ•°æ®: {symbol}")

        except Exception as e:
            print(f"å¤„ç†è‚¡ç¥¨æ•°æ®å¤±è´¥ {symbol}: {e}")

    print("\næ•°æ®ç®¡é“ç¤ºä¾‹å®Œæˆ!")


if __name__ == "__main__":
    main()