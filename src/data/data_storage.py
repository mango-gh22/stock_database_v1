# -*- coding: utf-8 -*-
# File Path: E:/MyFile/stock_database_v1/src/data\data_storage.py
# @ Author: m_mango
# @ Dateï¼š2025/12/5 18:46
"""
æ•°æ®å­˜å‚¨ç®¡ç†å™¨ - å¢å¼ºä¼˜åŒ–ç‰ˆ
åœ¨åŸç‰ˆåŸºç¡€ä¸Šä¼˜åŒ–ï¼šåŠ¨æ€åˆ—æ˜ å°„ã€æ‰¹é‡æ€§èƒ½ã€é”™è¯¯å¤„ç†ã€å…¼å®¹æ€§
"""

import logging
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from datetime import datetime, timedelta
from tqdm import tqdm
import time
import logging

import os
import sys
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.dirname(project_root))

from src.config.logging_config import setup_logging
from src.database.db_connector import DatabaseConnector
from src.utils.code_converter import normalize_stock_code  # âœ… å¼ºåˆ¶æ·»åŠ æ­¤è¡Œ

logger = setup_logging()


class DataStorage:
    """æ•°æ®å­˜å‚¨ç®¡ç†å™¨ - å¢å¼ºä¼˜åŒ–ç‰ˆ"""

    def __init__(self, config_path: str = 'config/database.yaml'):
        """
        åˆå§‹åŒ–æ•°æ®å­˜å‚¨å™¨

        Args:
            config_path: æ•°æ®åº“é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # å…³é”®ä¿®å¤ï¼šæ·»åŠ  logger
        import logging
        self.logger = logging.getLogger(__name__)

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥å™¨
        self.db_connector = DatabaseConnector(config_path)

        # ç¼“å­˜åˆå§‹åŒ–
        self._table_columns_cache = {}  # ç¼“å­˜è¡¨ç»“æ„ï¼Œé¿å…é‡å¤æŸ¥
        self._column_order_cache = {}

        # è®¾ç½®æ˜ å°„å…³ç³»
        self._setup_column_mappings()

        self.logger.info("æ•°æ®å­˜å‚¨å™¨åˆå§‹åŒ–å®Œæˆ")

        # æ”¯æŒçš„è¡¨æ˜ å°„
        self.supported_tables = {
            'daily': 'stock_daily_data',
            'basic': 'stock_basic_info',
            'index_constituent': 'stock_index_constituent',
            'financial': 'stock_financial_indicators',
            'minute': 'stock_minute_data'
        }
        self.logger.info(f"æ•°æ®åº“è¿æ¥å™¨: {self.db_connector.config.get('host')}:{self.db_connector.config.get('port')}")

    def _setup_column_mappings(self):
        """è®¾ç½®åˆ—åæ˜ å°„å…³ç³» - ä¿®å¤ç‰ˆ-ä¿®æ­£ç‰ˆï¼šé‡æ¯”"""
        # æ­£ç¡®çš„æ˜ å°„å…³ç³»ï¼ˆæ ¹æ®è¡¨ç»“æ„ï¼‰
        self.column_mapping = {
            # ä»·æ ¼å­—æ®µ
            'open': 'open_price',
            'high': 'high_price', 
            'low': 'low_price',
            'close': 'close_price',
            
            # æˆäº¤å­—æ®µ - å…³é”®ä¿®å¤ï¼
            'volume': 'volume',
            'amount': 'amount',      # Baostockçš„amount â†’ æ•°æ®åº“çš„amount

            # æ¶¨è·Œå¹…
            'pctChg': 'change_percent',
            'pct_change': 'change_percent',
            'change': 'change_amount',
            'pcfNcfTTM': 'pcf_ttm',


            # æŠ€æœ¯æŒ‡æ ‡
            'turnoverrate': 'turnover_rate',  # âœ… ä¿®æ­£ï¼šæ¢æ‰‹ç‡--æ€»è‚¡æœ¬
            'turn': 'turnover_rate_f',  # âœ… ä¿®æ­£:Baostockçš„turn â†’ æ•°æ®åº“çš„turnover_rate_fæµé€šæ¢æ‰‹ç‡

            # âœ… æ–°å¢ï¼šé‡æ¯”ï¼ˆä»å¤–éƒ¨è®¡ç®—ï¼‰
            'volume_ratio': 'volume_ratio',

            # å…¶ä»–å­—æ®µ
            'preclose': 'pre_close_price',
            'pre_close': 'pre_close_price',
            'pctChg': 'change_percent',
            'amplitude': 'amplitude',
            'turnover': 'turnover_rate',
            'adjustflag': 'adjust_flag',
            'tradestatus': 'trade_status',
            
            # è´¢åŠ¡æŒ‡æ ‡
            'pe': 'pe',
            'pe_ttm': 'pe_ttm',
            'pb': 'pb',
            'ps': 'ps',
            'ps_ttm': 'ps_ttm',
            'dv_ratio': 'dv_ratio',
            'dv_ttm': 'dv_ttm',
            
            # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            'totalShare': 'total_share',
            'floatShare': 'float_share',
            'freeShare': 'free_share',
            'totalMv': 'total_mv',
            'circMv': 'circ_mv'
        }
        
        # å¯é€‰ï¼šæ·»åŠ éªŒè¯æ—¥å¿—
        import logging
        logger = logging.getLogger(__name__)
        logger.info(f"å­—æ®µæ˜ å°„å·²è®¾ç½®: {len(self.column_mapping)} ä¸ªæ˜ å°„å…³ç³»")
    def _get_table_columns(self, table_name: str) -> set:
        """è·å–æ•°æ®åº“è¡¨çš„å®é™…å­—æ®µåé›†åˆ"""
        if table_name in self._table_columns_cache:
            return self._table_columns_cache[table_name]

        try:
            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(f"SHOW COLUMNS FROM `{table_name}`")
                    cols = {row[0] for row in cursor.fetchall()}
                    self._table_columns_cache[table_name] = cols
                    logger.debug(f"ç¼“å­˜è¡¨ `{table_name}` å­—æ®µ: {sorted(cols)}")
                    return cols
        except Exception as e:
            logger.error(f"æ— æ³•è·å–è¡¨ `{table_name}` ç»“æ„: {e}")
            # æ ¹æ®ä½ æä¾›çš„è¡¨ç»“æ„ï¼Œè¿”å›é»˜è®¤å­—æ®µ
            default_cols = {
                'symbol', 'trade_date', 'open_price', 'high_price', 'low_price',
                'close_price', 'pre_close_price', 'volume', 'amount', 'turnover_rate',
                'turnover_rate_f',
                'change_percent', 'ma5', 'ma10', 'ma20', 'ma30', 'ma60', 'ma120', 'ma250',
                'amplitude', 'data_source', 'processed_time', 'quality_grade',
                'created_time', 'updated_time', 'volume_ma5', 'volume_ma10', 'volume_ma20',
                'rsi', 'bb_middle', 'bb_upper', 'bb_lower', 'volatility_20d',
                'pe', 'pe_ttm', 'pb', 'ps', 'ps_ttm', 'dv_ratio', 'dv_ttm',
                'total_share', 'float_share', 'free_share', 'total_mv', 'circ_mv'
            }
            self._table_columns_cache[table_name] = default_cols
            logger.warning(f"ä½¿ç”¨é»˜è®¤å­—æ®µé›†åˆ: {len(default_cols)} ä¸ªå­—æ®µ")
            return default_cols

    def _preprocess_data(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:
        """æ ¸å¿ƒä¼˜åŒ–ï¼šå› å­å­—æ®µé˜²è¯¯åˆ  + ä»£ç ç»“æ„ç®€åŒ–
        é¢„å¤„ç†æ•°æ® - v0.8.0 å› å­å¼ºåˆ¶ä¿ç•™ç‰ˆ
        """
        if df.empty:
            logger.warning("è¾“å…¥æ•°æ®ä¸ºç©º")
            return df

        original_columns = list(df.columns)
        logger.debug(f"åŸå§‹å­—æ®µ: {original_columns}")

        df_processed = df.copy()

        # === 1. âœ… è‚¡ç¥¨ä»£ç å­—æ®µå¼ºåˆ¶æ ‡å‡†åŒ–ï¼ˆå…³é”®ä¿®å¤ï¼‰ ===
        symbol_created = False

        if 'code' in df_processed.columns:
            df_processed['symbol'] = df_processed['code'].apply(
                lambda x: str(x).replace('.', '') if pd.notna(x) else None
            )
            symbol_created = True
            logger.debug(f"ä» 'code' å­—æ®µç”Ÿæˆæ ‡å‡†åŒ– symbol")

        elif 'bs_code' in df_processed.columns:
            df_processed['symbol'] = df_processed['bs_code'].apply(
                lambda x: str(x).replace('.', '') if pd.notna(x) else None
            )
            symbol_created = True
            logger.debug(f"ä» 'bs_code' å­—æ®µç”Ÿæˆæ ‡å‡†åŒ– symbol")

        elif 'symbol' in df_processed.columns:
            df_processed['symbol'] = df_processed['symbol'].apply(
                lambda x: normalize_stock_code(str(x)) if pd.notna(x) else None
            )
            symbol_created = True
            logger.debug(f"æ ‡å‡†åŒ–ç°æœ‰ symbol å­—æ®µ")

        if not symbol_created:
            logger.error(f"âŒ é¢„å¤„ç†å¤±è´¥ï¼šæ‰¾ä¸åˆ°è‚¡ç¥¨ä»£ç å­—æ®µ")
            return pd.DataFrame()

        # === 2. æ—¥æœŸå­—æ®µå¤„ç† ===
        date_created = False

        if 'trade_date' in df_processed.columns:
            df_processed['trade_date'] = pd.to_datetime(df_processed['trade_date'], errors='coerce')
            date_created = True
            logger.debug(f"ä½¿ç”¨ç°æœ‰ 'trade_date' å­—æ®µ")

        elif 'date' in df_processed.columns:
            df_processed['trade_date'] = pd.to_datetime(df_processed['date'], errors='coerce')
            date_created = True
            logger.debug(f"ä» 'date' å­—æ®µç”Ÿæˆ trade_date")

        if not date_created:
            logger.error(f"âŒ é¢„å¤„ç†å¤±è´¥ï¼šæ‰¾ä¸åˆ°æ—¥æœŸå­—æ®µ")
            return pd.DataFrame()

        # === 3. å­—æ®µæ˜ å°„ï¼ˆBaostock â†’ æ•°æ®åº“ï¼‰===
        field_mapping = {
            # ä»·æ ¼å­—æ®µ
            'open': 'open_price',
            'high': 'high_price',
            'low': 'low_price',
            'close': 'close_price',
            'preclose': 'pre_close_price',
            'pre_close': 'pre_close_price',

            # æˆäº¤é‡é‡‘é¢
            'volume': 'volume',
            'amount': 'amount',

            # æ¶¨è·Œå¹…
            'pctChg': 'change_percent',
            'pct_change': 'change_percent',
            'change': 'change_amount',
            'pcfNcfTTM': 'pcf_ttm',

            # æŠ€æœ¯æŒ‡æ ‡
            'turn': 'turnover_rate_f',
            'turnoverrate': 'turnover_rate',
            'amplitude': 'amplitude',

            # å…¶ä»–å­—æ®µ
            'adjustflag': 'adjust_flag',
            'tradestatus': 'trade_status'
        }

        # åº”ç”¨å­—æ®µæ˜ å°„
        rename_map = {}
        for src_field, target_field in field_mapping.items():
            if src_field in df_processed.columns and target_field not in df_processed.columns:
                rename_map[src_field] = target_field

        if rename_map:
            df_processed = df_processed.rename(columns=rename_map)
            logger.debug(f"å­—æ®µæ˜ å°„: {rename_map}")

        # === 4. âŒ ç§»é™¤ Baostock åŸå§‹å› å­å­—æ®µï¼ˆé¿å…å‘½åå†²çªï¼‰===
        # Baostock è¿”å›çš„åŸå§‹å­—æ®µåœ¨æ˜ å°„ååº”åˆ é™¤
        baostock_raw_fields = ['peTTM', 'pbMRQ', 'psTTM', 'pcfNcfTTM']
        for field in baostock_raw_fields:
            if field in df_processed.columns:
                df_processed = df_processed.drop(columns=[field])
                logger.debug(f"åˆ é™¤ Baostock åŸå§‹å­—æ®µ: {field}")


        # === 5. æ•°æ®ç±»å‹è½¬æ¢ ===
        # âœ… ç»Ÿä¸€å®šä¹‰æ‰€æœ‰æ•°å€¼å­—æ®µï¼ˆåŒ…å«å› å­ï¼‰
        numeric_fields = [
            'open_price', 'high_price', 'low_price', 'close_price',
            'pre_close_price', 'volume', 'amount', 'change_percent',
            'turnover_rate', 'turnover_rate_f', 'amplitude',
            'change_amount', 'volume_ratio',
            # âœ… æ ¸å¿ƒå› å­å­—æ®µ
            'pe_ttm', 'pb', 'ps_ttm', 'pcf_ttm'
        ]

        # âœ… åŠ¨æ€æ·»åŠ å…¶ä»–å¯èƒ½å­˜åœ¨çš„å› å­å­—æ®µ
        additional_factors = ['pe', 'ps', 'dv_ratio', 'dv_ttm',
                              'total_share', 'float_share', 'free_share', 'total_mv', 'circ_mv']
        for field in additional_factors:
            if field in df_processed.columns:
                numeric_fields.append(field)

        # æ‰§è¡Œæ•°å€¼è½¬æ¢
        for field in numeric_fields:
            if field in df_processed.columns:
                df_processed[field] = pd.to_numeric(df_processed[field], errors='coerce')
                logger.debug(f"æ•°å€¼è½¬æ¢ {field}: {df_processed[field].notna().sum()} æ¡æœ‰æ•ˆ")


        # === 6. âœ… æ™ºèƒ½æ¸…ç†åŸå§‹å­—æ®µï¼ˆæ˜ç¡®ä¿ç•™å› å­ï¼‰===
        columns_to_remove = [
            'code', 'bs_code', 'date', 'open', 'high', 'low', 'close',
            'preclose', 'pctChg', 'turn', 'adjustflag', 'tradestatus'
        ]

        # âœ… å› å­ä¿æŠ¤åˆ—è¡¨ï¼ˆå¿…é¡»ä¿ç•™ï¼‰
        protected_fields = ['pe_ttm', 'pb', 'ps_ttm', 'pcf_ttm'] + additional_factors

        # æ‰§è¡Œæ¸…ç†
        for col in columns_to_remove:
            if col in df_processed.columns and col not in protected_fields:
                df_processed = df_processed.drop(columns=[col])
                logger.debug(f"åˆ é™¤åŸå§‹å­—æ®µ: {col}")


        # === 6. æ•°æ®è¿‡æ»¤ï¼ˆåªåˆ é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œï¼‰===
        before_filter = len(df_processed)
        df_processed = df_processed.dropna(subset=['symbol', 'trade_date'], how='any')

        # ç¡®ä¿ symbol ä¸ä¸ºç©ºå­—ç¬¦ä¸²
        if 'symbol' in df_processed.columns:
            df_processed = df_processed[df_processed['symbol'].notna() & (df_processed['symbol'] != '')]

        after_filter = len(df_processed)
        if before_filter > after_filter:
            logger.info(f"è¿‡æ»¤æ‰ {before_filter - after_filter} æ¡æ— æ•ˆè¡Œ")

        # === 7. æ—¥æœŸæ ¼å¼æ ‡å‡†åŒ– ===
        if 'trade_date' in df_processed.columns:
            # è½¬æ¢ä¸º MySQL æ ‡å‡†æ ¼å¼
            df_processed['trade_date'] = pd.to_datetime(
                df_processed['trade_date'],
                errors='coerce'
            ).dt.strftime('%Y-%m-%d')
            logger.debug("trade_date å·²æ ¼å¼åŒ–ä¸º YYYY-MM-DD")

        # === 8. æ·»åŠ å…ƒæ•°æ®å­—æ®µ ===
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        if 'created_time' in self._get_table_columns(table_name):
            df_processed['created_time'] = current_time

        if 'updated_time' in self._get_table_columns(table_name):
            df_processed['updated_time'] = current_time

        # === 9. è®¡ç®—è¡ç”Ÿå­—æ®µï¼ˆé‡æ¯”ã€æŒ¯å¹…ï¼‰===
        # è®¡ç®—é‡æ¯”ï¼ˆvolume_ratioï¼‰
        if 'volume' in df_processed.columns and len(df_processed) > 5:
            df_processed['volume_ratio'] = (
                    df_processed['volume'] /
                    df_processed['volume'].shift(1).rolling(5, min_periods=1).mean()
            ).fillna(1.0).clip(0, 50)
            logger.debug(f"è®¡ç®—é‡æ¯”: å‡å€¼={df_processed['volume_ratio'].mean():.2f}")

        # è®¡ç®—æŒ¯å¹…ï¼ˆamplitudeï¼‰
        if all(col in df_processed.columns for col in ['high_price', 'low_price', 'pre_close_price']):
            df_processed['amplitude'] = (
                    (df_processed['high_price'] - df_processed['low_price']) /
                    df_processed['pre_close_price'] * 100
            ).round(4)
            logger.debug(f"è®¡ç®—æŒ¯å¹…: å‡å€¼={df_processed['amplitude'].mean():.2f}%")

        # === 10. æœ€ç»ˆéªŒè¯ ===
        factor_cols = [col for col in ['pe_ttm', 'pb', 'ps_ttm', 'pcf_ttm'] if col in df_processed.columns]
        logger.info(f"âœ… å› å­å­—æ®µä¿ç•™éªŒè¯: {len(factor_cols)}ä¸ª -> {factor_cols}")

        logger.info(f"âœ… é¢„å¤„ç†å®Œæˆ: {len(df_processed)} æ¡è®°å½•, {len(df_processed.columns)} ä¸ªå­—æ®µ")
        logger.debug(f"æœ€ç»ˆå­—æ®µ: {list(df_processed.columns)}")

        return df_processed

    def _get_table_column_order(self, table_name: str) -> List[str]:
        """
        è·å–æ•°æ®åº“è¡¨çš„å­—æ®µå®šä¹‰é¡ºåº
        """
        cache_key = f"{table_name}_order"

        if hasattr(self, '_column_order_cache') and cache_key in self._column_order_cache:
            return self._column_order_cache[cache_key]

        try:
            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(f"SHOW COLUMNS FROM `{table_name}`")
                    columns = [row[0] for row in cursor.fetchall()]

                    if not hasattr(self, '_column_order_cache'):
                        self._column_order_cache = {}
                    self._column_order_cache[cache_key] = columns

                    logger.debug(f"è·å–è¡¨ {table_name} å­—æ®µé¡ºåº: {len(columns)} ä¸ªå­—æ®µ")
                    return columns

        except Exception as e:
            logger.error(f"æ— æ³•è·å–è¡¨ `{table_name}` å­—æ®µé¡ºåº: {e}")

            default_order = [
                'symbol', 'trade_date', 'open_price', 'high_price', 'low_price',
                'close_price', 'pre_close_price', 'volume', 'amount', 'turnover_rate',
                'turnover_rate_f', 'volume_ratio', 'ma5', 'ma10', 'ma20', 'ma30',
                'ma60', 'ma120', 'ma250', 'amplitude', 'data_source', 'processed_time',
                'quality_grade', 'created_time', 'updated_time', 'change_percent',
                'volume_ma5', 'volume_ma10', 'volume_ma20', 'rsi', 'bb_middle',
                'bb_upper', 'bb_lower', 'volatility_20d', 'pe', 'pe_ttm', 'pb',
                'ps', 'ps_ttm', 'dv_ratio', 'dv_ttm', 'total_share', 'float_share',
                'free_share', 'total_mv', 'circ_mv'
            ]

            if not hasattr(self, '_column_order_cache'):
                self._column_order_cache = {}
            self._column_order_cache[cache_key] = default_order

            return default_order

    def _build_dynamic_sql(self, df: pd.DataFrame, table_name: str) -> Tuple[str, str, List[str]]:
        """
        æ„å»ºåŠ¨æ€SQLè¯­å¥ - ç¡®ä¿ä½¿ç”¨æ•°æ®åº“å­—æ®µé¡ºåº
        """
        # è·å–æ•°æ®åº“è¡¨çš„å­—æ®µé¡ºåº
        db_column_order = self._get_table_column_order(table_name)

        # åªé€‰æ‹©æ•°æ®åº“è¡¨ä¸­å­˜åœ¨ä¸”DataFrameä¸­æœ‰çš„å­—æ®µ
        valid_columns = [col for col in db_column_order if col in df.columns]

        if not valid_columns:
            raise ValueError(f"æ²¡æœ‰æœ‰æ•ˆçš„åˆ—å¯ä»¥æ’å…¥åˆ°è¡¨ {table_name}")

        # æ£€æŸ¥å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨
        required_columns = ['symbol', 'trade_date']
        for req_col in required_columns:
            if req_col not in valid_columns:
                raise ValueError(f"å¿…éœ€å­—æ®µ '{req_col}' ä¸å­˜åœ¨")

        columns_str = ', '.join([f'`{col}`' for col in valid_columns])
        placeholders = ', '.join(['%s'] * len(valid_columns))
        insert_sql = f"INSERT INTO `{table_name}` ({columns_str}) VALUES ({placeholders})"

        # å‡è®¾å”¯ä¸€é”®ä¸º (symbol, trade_date)
        unique_columns = ['symbol', 'trade_date']
        update_columns = [col for col in valid_columns if col not in unique_columns]

        if update_columns:
            update_set = ', '.join([f"`{col}` = VALUES(`{col}`)" for col in update_columns])
            update_sql = f"ON DUPLICATE KEY UPDATE {update_set}"
        else:
            update_sql = ""

        logger.debug(f"æ„å»ºSQL - è¡¨: {table_name}, æ’å…¥åˆ—: {len(valid_columns)}")
        logger.debug(f"å­—æ®µé¡ºåº: {valid_columns}")

        return insert_sql, update_sql, valid_columns

    def _prepare_records(self, df: pd.DataFrame, valid_columns: List[str] = None) -> List[Tuple]:
        """
        å‡†å¤‡æ’å…¥è®°å½• - ä¿®å¤ï¼šä½¿ç”¨æŒ‡å®šçš„å­—æ®µé¡ºåº
        """
        if valid_columns is None:
            valid_columns = list(df.columns)

        records = []

        for _, row in df.iterrows():
            record = []
            for col in valid_columns:
                value = row[col] if col in row.index else None

                if pd.isna(value):
                    record.append(None)
                elif isinstance(value, (np.integer, np.int64)):
                    record.append(int(value))
                elif isinstance(value, (np.floating, np.float64)):
                    record.append(float(value))
                elif isinstance(value, (datetime, pd.Timestamp)):
                    # ç¡®ä¿æ—¶é—´æˆ³æ ¼å¼æ­£ç¡®
                    if isinstance(value, pd.Timestamp):
                        record.append(value.strftime('%Y-%m-%d %H:%M:%S'))
                    else:
                        record.append(value.strftime('%Y-%m-%d %H:%M:%S'))
                elif isinstance(value, str) and col in ['trade_date']:
                    # å¦‚æœæ˜¯æ—¥æœŸå­—ç¬¦ä¸²ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
                    try:
                        pd_date = pd.to_datetime(value)
                        record.append(pd_date.strftime('%Y-%m-%d'))
                    except:
                        record.append(value)
                else:
                    record.append(value)

            records.append(tuple(record))

        return records

    def store_daily_data(self, data, table_name: str = None) -> Tuple[int, Dict]:
        """
        å­˜å‚¨æ—¥çº¿æ•°æ® - å¢å¼ºç‰ˆï¼šæ”¯æŒå¤šç§è¾“å…¥ç±»å‹å¹¶ç¡®ä¿æ•°æ®ä¸€è‡´æ€§

        Args:
            data: è¾“å…¥æ•°æ®ï¼Œå¯ä»¥æ˜¯ pd.DataFrameã€list of dicts æˆ– dict
            table_name: ç›®æ ‡è¡¨åï¼Œé»˜è®¤ä½¿ç”¨ stock_daily_data

        Returns:
            (å½±å“è¡Œæ•°, è¯¦ç»†ä¿¡æ¯å­—å…¸)
        """
        try:
            # 1. è½¬æ¢è¾“å…¥ä¸º DataFrame
            if isinstance(data, pd.DataFrame):
                df = data.copy()
            elif isinstance(data, list):
                if not data:
                    logger.warning("æ—¥çº¿æ•°æ®ä¸ºç©ºåˆ—è¡¨ï¼Œè·³è¿‡å­˜å‚¨")
                    return 0, {'status': 'skipped', 'reason': 'empty_list'}
                df = pd.DataFrame(data)
            elif isinstance(data, dict):
                df = pd.DataFrame([data])
            else:
                error_msg = f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {type(data)}"
                logger.error(error_msg)
                return 0, {'status': 'error', 'reason': error_msg}

            if df.empty:
                logger.warning("æ—¥çº¿æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å­˜å‚¨")
                return 0, {'status': 'skipped', 'reason': 'empty_data'}

            logger.info(f"æ¥æ”¶åˆ°æ•°æ®: {type(data).__name__} -> è½¬æ¢å {len(df)} è¡Œ {len(df.columns)} åˆ—")

            # 2. ç¡®å®šè¡¨å
            if table_name is None:
                table_name = self.supported_tables.get('daily', 'stock_daily_data')

            # 3. æ•°æ®éªŒè¯å’Œæ—¥å¿—
            logger.info(f"å¼€å§‹å¤„ç†æ•°æ®ï¼Œç›®æ ‡è¡¨: {table_name}")
            logger.debug(f"æ•°æ®åˆ—å: {list(df.columns)}")
            logger.debug(f"å‰2è¡Œç¤ºä¾‹:\n{df.head(2).to_string()}")

            # æ£€æŸ¥å¿…è¦å­—æ®µ
            required_columns = ['symbol', 'trade_date']
            missing_columns = [col for col in required_columns if col not in df.columns]

            if missing_columns:
                error_msg = f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_columns}"
                logger.error(error_msg)
                return 0, {'status': 'error', 'reason': error_msg, 'missing_columns': missing_columns}

            # 4. æ•°æ®é¢„å¤„ç†
            logger.info(f"å¼€å§‹é¢„å¤„ç†æ•°æ®ï¼ŒåŸå§‹æ•°æ® {len(df)} æ¡")
            df_processed = self._preprocess_data(df, table_name)

            if df_processed.empty:
                logger.error("é¢„å¤„ç†åæ•°æ®ä¸ºç©º")
                return 0, {'status': 'skipped', 'reason': 'preprocess_failed'}

            logger.info(f"âœ… é¢„å¤„ç†å®Œæˆ: {len(df_processed)} æ¡è®°å½•")
            logger.debug(f"é¢„å¤„ç†ååˆ—å: {list(df_processed.columns)}")

            # 5. æ£€æŸ¥å­—æ®µä¸€è‡´æ€§
            with self.db_connector.get_connection() as conn:
                with conn.cursor(dictionary=True) as cursor:
                    # è·å–è¡¨ç»“æ„
                    cursor.execute(f"DESCRIBE {table_name}")
                    table_columns = [row['Field'] for row in cursor.fetchall()]

                    # è·å–æ•°æ®åˆ—
                    data_columns = list(df_processed.columns)

                    # æ‰¾å‡ºåŒ¹é…çš„åˆ—
                    matching_columns = [col for col in data_columns if col in table_columns]
                    missing_in_table = [col for col in data_columns if col not in table_columns]
                    missing_in_data = [col for col in table_columns if
                                       col not in data_columns and col not in ['id', 'created_time', 'updated_time']]

                    logger.info(f"å­—æ®µåŒ¹é…: {len(matching_columns)}/{len(data_columns)} ä¸ªå­—æ®µå¯æ’å…¥")
                    if missing_in_table:
                        logger.warning(f"æ•°æ®ä¸­çš„å­—æ®µåœ¨è¡¨ä¸­ä¸å­˜åœ¨: {missing_in_table}")
                    if missing_in_data:
                        logger.debug(f"è¡¨ä¸­çš„å­—æ®µåœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_in_data}")

            # 6. æ„å»ºåŠ¨æ€SQL - åªä½¿ç”¨åŒ¹é…çš„å­—æ®µ
            insert_sql, update_sql, valid_columns = self._build_dynamic_sql(df_processed, table_name)
            full_sql = insert_sql + (" " + update_sql if update_sql else "")

            logger.debug(f"SQLè¯­å¥: {full_sql}")
            logger.info(f"å°†æ’å…¥ {len(valid_columns)} ä¸ªå­—æ®µ: {valid_columns}")

            # 7. å‡†å¤‡è®°å½•
            records = self._prepare_records(df_processed, valid_columns)

            if not records:
                logger.error("æ²¡æœ‰æœ‰æ•ˆçš„è®°å½•å¯ä»¥æ’å…¥")
                return 0, {'status': 'skipped', 'reason': 'no_valid_records'}

            # 8. æ‰¹é‡æ’å…¥ - ä½¿ç”¨äº‹åŠ¡ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            logger.info(f"å¼€å§‹æ’å…¥ {len(records)} æ¡è®°å½•åˆ°è¡¨ {table_name}")
            logger.debug(f"ç¬¬ä¸€æ¡è®°å½•ç¤ºä¾‹: {records[0] if records else 'None'}")

            affected_rows = 0
            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    try:
                        # æ‰¹é‡æ’å…¥
                        cursor.executemany(full_sql, records)
                        affected_rows = cursor.rowcount

                        # æ˜¾å¼æäº¤äº‹åŠ¡
                        conn.commit()

                        logger.info(f"âœ… æ•°æ®åº“æäº¤æˆåŠŸï¼Œå½±å“è¡Œæ•°: {affected_rows}")

                        # éªŒè¯å®é™…æ’å…¥çš„è®°å½•
                        symbol = df_processed['symbol'].iloc[0] if 'symbol' in df_processed.columns else 'unknown'
                        cursor.execute(f"SELECT COUNT(*) as count FROM {table_name} WHERE symbol = %s", (symbol,))
                        actual_count = cursor.fetchone()[0]
                        logger.info(f"âœ… éªŒè¯æˆåŠŸ: è¡¨ä¸­æœ‰ {actual_count} æ¡ {symbol} çš„è®°å½•")

                    except Exception as e:
                        # å‡ºé”™æ—¶å›æ»š
                        conn.rollback()
                        logger.error(f"âŒ æ•°æ®åº“æ’å…¥å¤±è´¥ï¼Œå·²å›æ»š: {e}")

                        # å°è¯•å•æ¡æ’å…¥ä»¥æ‰¾å‡ºé—®é¢˜è®°å½•
                        logger.info("å°è¯•å•æ¡æ’å…¥ä»¥è°ƒè¯•é—®é¢˜...")
                        for i, record in enumerate(records[:3]):  # åªæµ‹è¯•å‰3æ¡
                            try:
                                cursor.execute(full_sql, record)
                                logger.info(f"  è®°å½• {i + 1} å•ç‹¬æ’å…¥æˆåŠŸ")
                            except Exception as single_error:
                                logger.error(f"  è®°å½• {i + 1} å¤±è´¥: {single_error}")
                                logger.error(f"    è®°å½•å†…å®¹: {record}")

                        raise

            # 9. è¿”å›æˆåŠŸç»“æœ
            symbol = df_processed['symbol'].iloc[0] if 'symbol' in df_processed.columns else 'unknown'
            logger.info(
                f"å­˜å‚¨æ—¥çº¿æ•°æ®å®Œæˆ: {symbol}, "
                f"è¡¨: {table_name}, "
                f"è®°å½•: {len(records)}æ¡, "
                f"å½±å“: {affected_rows}è¡Œ"
            )

            # 10. è®°å½•æ•°æ®æ›´æ–°æ—¥å¿—
            self._log_data_update(
                data_type='daily',
                symbol=symbol,
                table_name=table_name,
                records_processed=len(records),
                records_affected=affected_rows,
                status='success'
            )

            return affected_rows, {
                'status': 'success',
                'table': table_name,
                'records_processed': len(records),
                'records_affected': affected_rows,
                'symbol': symbol,
                'input_type': type(data).__name__,
                'input_shape': f"{len(df)}x{len(df.columns)}",
                'processed_shape': f"{len(df_processed)}x{len(df_processed.columns)}",
                'matching_columns': len(matching_columns),
                'total_columns': len(data_columns),
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"å­˜å‚¨æ—¥çº¿æ•°æ®å¤±è´¥: {e}", exc_info=True)

            # è®°å½•å¤±è´¥æ—¥å¿—
            symbol = 'unknown'
            if 'df' in locals() and 'symbol' in df.columns:
                symbol = df['symbol'].iloc[0] if not df.empty else 'unknown'

            self._log_data_update(
                data_type='daily',
                symbol=symbol,
                table_name=table_name or 'stock_daily_data',
                records_processed=0,
                records_affected=0,
                status='error',
                error_message=str(e)
            )

            return 0, {
                'status': 'error',
                'error': str(e),
                'error_type': type(e).__name__,
                'symbol': symbol,
                'table': table_name,
                'timestamp': datetime.now().isoformat()
            }


    def _build_dynamic_sql(self, df: pd.DataFrame, table_name: str) -> Tuple[str, str, list]:
        """
        æ„å»ºåŠ¨æ€SQLè¯­å¥

        Returns:
            (INSERTè¯­å¥, ON DUPLICATE KEY UPDATEè¯­å¥, æœ‰æ•ˆå­—æ®µåˆ—è¡¨)
        """
        try:
            # è·å–è¡¨çš„å®é™…å­—æ®µ
            with self.db_connector.get_connection() as conn:
                with conn.cursor(dictionary=True) as cursor:
                    cursor.execute(f"DESCRIBE {table_name}")
                    table_fields = [row['Field'] for row in cursor.fetchall()]

            # æ‰¾å‡ºæ•°æ®ä¸­å­˜åœ¨çš„è¡¨å­—æ®µ
            data_columns = list(df.columns)
            valid_columns = [col for col in data_columns if col in table_fields]

            if not valid_columns:
                raise ValueError(f"æ²¡æœ‰åŒ¹é…çš„å­—æ®µå¯ä»¥æ’å…¥åˆ°è¡¨ {table_name}")

            # æ„å»ºINSERTéƒ¨åˆ†
            columns_str = ', '.join(valid_columns)
            placeholders = ', '.join(['%s'] * len(valid_columns))
            insert_sql = f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})"

            # æ„å»ºON DUPLICATE KEY UPDATEéƒ¨åˆ†
            # ç¡®å®šå”¯ä¸€é”®ï¼šå‡è®¾symbolå’Œtrade_dateæ˜¯å”¯ä¸€é”®
            unique_keys = ['symbol', 'trade_date']
            update_columns = [col for col in valid_columns if
                              col not in unique_keys and col not in ['id', 'created_time']]

            if update_columns:
                update_set = ', '.join([f"{col} = VALUES({col})" for col in update_columns])
                update_sql = f"ON DUPLICATE KEY UPDATE {update_set}"
            else:
                update_sql = ""

            logger.debug(f"æ„å»ºSQL: {len(valid_columns)}ä¸ªå­—æ®µï¼Œ{len(update_columns)}ä¸ªæ›´æ–°å­—æ®µ")

            return insert_sql, update_sql, valid_columns

        except Exception as e:
            logger.error(f"æ„å»ºSQLè¯­å¥å¤±è´¥: {e}", exc_info=True)
            raise

    def _prepare_records(self, df: pd.DataFrame, valid_columns: list) -> list:
        """
        å‡†å¤‡è¦æ’å…¥çš„è®°å½•
        """
        try:
            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
            for col in valid_columns:
                if col not in df.columns:
                    df[col] = None

            # è½¬æ¢DataFrameä¸ºè®°å½•åˆ—è¡¨
            records = []
            for _, row in df.iterrows():
                record = tuple(row[col] if pd.notna(row[col]) else None for col in valid_columns)
                records.append(record)

            logger.debug(f"å‡†å¤‡äº† {len(records)} æ¡è®°å½•ï¼Œæ¯æ¡ {len(valid_columns)} ä¸ªå­—æ®µ")

            return records

        except Exception as e:
            logger.error(f"å‡†å¤‡è®°å½•å¤±è´¥: {e}", exc_info=True)
            return []

    def _log_data_update(self, data_type: str, symbol: str, table_name: str,
                         records_processed: int, records_affected: int,
                         status: str, error_message: str = None):
        """
        è®°å½•æ•°æ®æ›´æ–°æ—¥å¿—
        """
        try:
            log_entry = {
                'data_type': data_type,
                'symbol': symbol,
                'table_name': table_name,
                'records_processed': records_processed,
                'records_affected': records_affected,
                'status': status,
                'error_message': error_message,
                'created_time': datetime.now()
            }

            # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
            logger.info(f"ğŸ“ æ•°æ®æ›´æ–°æ—¥å¿—: {data_type} {symbol} è¡Œæ•°: {records_processed} çŠ¶æ€: {status}")

            # å¯ä»¥åœ¨è¿™é‡Œå°†æ—¥å¿—å­˜å…¥æ•°æ®åº“
            # self._save_update_log_to_db(log_entry)

        except Exception as e:
            logger.error(f"è®°å½•æ•°æ®æ›´æ–°æ—¥å¿—å¤±è´¥: {e}")

    def get_last_update_date(self, symbol: str = None, table_name: str = None) -> Optional[str]:
        """
        è·å–æŒ‡å®šè‚¡ç¥¨æˆ–å…¨è¡¨çš„æœ€åæ›´æ–°æ—¥æœŸ

        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼ˆå¯å¸¦ç‚¹ï¼Œå¦‚ 'sh.600519'ï¼‰
            table_name: è¡¨å

        Returns:
            æœ€åæ›´æ–°æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚ '2025-12-31'ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        try:
            if table_name is None:
                table_name = self.supported_tables.get('daily', 'stock_daily_data')

            with self.db_connector.get_connection() as conn:
                with conn.cursor(dictionary=True) as cursor:
                    if symbol:
                        clean_symbol = str(symbol).replace('.', '')
                        query = f"SELECT MAX(trade_date) as last_date FROM `{table_name}` WHERE symbol = %s"
                        cursor.execute(query, (clean_symbol,))
                    else:
                        query = f"SELECT MAX(trade_date) as last_date FROM `{table_name}`"
                        cursor.execute(query)

                    result = cursor.fetchone()
                    if result and result['last_date']:
                        if isinstance(result['last_date'], str):
                            return result['last_date']
                        else:
                            return result['last_date'].strftime('%Y-%m-%d')
                    return None

        except Exception as e:
            logger.warning(f"è·å–æœ€åæ›´æ–°æ—¥æœŸå¤±è´¥: {e}")
            return None


    def log_data_update(self, data_type: str, symbol: str, *args, **kwargs):
        """
        è®°å½•æ•°æ®æ›´æ–°æ—¥å¿— - ç®€åŒ–ç¨³å®šç‰ˆ
        å…ˆä¿è¯ä¸æŠ¥é”™ï¼Œè®©ä¸»æµç¨‹èƒ½è¿è¡Œ
        """
        try:
            # ç®€å•çš„æ—¥å¿—è®°å½•
            log_msg = f"æ•°æ®æ›´æ–°æ—¥å¿—: {data_type} {symbol}"

            # å°è¯•è§£æåŸºæœ¬å‚æ•°
            if len(args) >= 2:
                # æ ¼å¼: (rows_affected, status)
                rows = args[0]
                status = args[1] if len(args) > 1 else 'unknown'
                log_msg += f" è¡Œæ•°: {rows} çŠ¶æ€: {status}"
            elif 'rows_affected' in kwargs:
                log_msg += f" è¡Œæ•°: {kwargs['rows_affected']} çŠ¶æ€: {kwargs.get('status', 'unknown')}"

            # è®°å½•æ—¥å¿—ï¼ˆä½¿ç”¨ print ç¡®ä¿æ€»æ˜¯èƒ½è¾“å‡ºï¼‰
            print(f"ğŸ“ {log_msg}")
            if hasattr(self, 'logger'):
                self.logger.info(log_msg)

            return {'success': True}

        except Exception as e:
            # å³ä½¿å‡ºé”™ä¹Ÿä¸å½±å“ä¸»æµç¨‹
            error_msg = f"æ—¥å¿—è®°å½•ç®€åŒ–å¤„ç†å¤±è´¥: {e}"
            print(f"âš ï¸ {error_msg}")
            if hasattr(self, 'logger'):
                self.logger.warning(error_msg)
            return {'success': False, 'error': str(e)}

def test_fixed_storage():
    """æµ‹è¯•ä¿®å¤åçš„æ•°æ®å­˜å‚¨å™¨"""
    import pandas as pd
    from datetime import datetime

    print("ğŸ§ª æµ‹è¯•ä¿®å¤ç‰ˆæ•°æ®å­˜å‚¨å™¨")
    print("=" * 50)

    try:
        # 1. åˆå§‹åŒ–
        print("åˆå§‹åŒ– DataStorage...")
        storage = DataStorage()
        print("âœ… DataStorage åˆå§‹åŒ–æˆåŠŸ")

        # 2. åˆ›å»ºæµ‹è¯•æ•°æ®
        print("åˆ›å»ºæµ‹è¯•æ•°æ®...")
        test_symbol = f"TEST_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        test_date = datetime.now().strftime('%Y-%m-%d')

        test_data = pd.DataFrame({
            'symbol': [test_symbol],
            'trade_date': [test_date],
            'open_price': [100.0],
            'high_price': [105.0],
            'low_price': [95.0],
            'close_price': [102.0],
            'volume': [1000000],
            'amount': [102000000],
            'data_source': ['test'],
            'processed_time': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')]
        })

        print(f"æµ‹è¯•æ•°æ®: {test_symbol} {test_date}")

        # 3. å­˜å‚¨æ•°æ®
        print("å­˜å‚¨æ•°æ®...")
        affected_rows, report = storage.store_daily_data(test_data)

        print(f"å­˜å‚¨ç»“æœ:")
        print(f"  å½±å“è¡Œæ•°: {affected_rows}")
        print(f"  çŠ¶æ€: {report['status']}")

        # 4. éªŒè¯æ•°æ®æ˜¯å¦æ’å…¥
        if affected_rows > 0:
            time.sleep(1)
            last_date = storage.get_last_update_date(symbol=test_symbol)
            if last_date == test_date:
                print("âœ… æ•°æ®éªŒè¯æˆåŠŸï¼šæœ€åæ›´æ–°æ—¥æœŸåŒ¹é…")
            else:
                print(f"âŒ éªŒè¯å¤±è´¥ï¼šæœŸæœ› {test_date}, å®é™… {last_date}")

            # æ¸…ç†æµ‹è¯•æ•°æ®
            print("æ¸…ç†æµ‹è¯•æ•°æ®...")
            try:
                with storage.db_connector.get_connection() as conn:
                    with conn.cursor() as cursor:
                        cursor.execute("DELETE FROM stock_daily_data WHERE symbol = %s", (test_symbol,))
                        conn.commit()
                        print(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤ {cursor.rowcount} æ¡è®°å½•")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†å¤±è´¥: {e}")

        return affected_rows > 0

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_fixed_storage()

    if success:
        print("\nâœ… ä¿®å¤ç‰ˆæ•°æ®å­˜å‚¨å™¨æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ ä¿®å¤ç‰ˆæ•°æ®å­˜å‚¨å™¨æµ‹è¯•å¤±è´¥")

    exit(0 if success else 1)