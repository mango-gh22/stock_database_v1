# _*_ coding: utf-8 _*_
# File Path: E:/MyFile/stock_database_v1/src/data\baostock_pb_factor_downloader.py
# File Name: baostock_pb_factor_downloader
# @ Author: mango-gh22
# @ Dateï¼š2026/1/3 11:20
"""
desc PBç­‰å› å­æ•°æ®ä¸‹è½½å™¨ - å•çº¿ç¨‹å®ç°ï¼ˆä¿®æ­£ç‰ˆï¼‰
ä¸‹è½½ï¼špeTTM(æ»šåŠ¨å¸‚ç›ˆç‡), pbMRQ(å¸‚å‡€ç‡), psTTM(æ»šåŠ¨å¸‚é”€ç‡)ç­‰ä¼°å€¼æŒ‡æ ‡
"""

import baostock as bs
import pandas as pd
import numpy as np
import time
import random
import logging
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime, timedelta
import threading
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.enhanced_trade_date_manager import get_enhanced_trade_date_manager
from src.utils.code_converter import normalize_stock_code
from src.data.baostock_factor_config import get_config_loader
from src.data.baostock_factor_base import BaseFactorDownloader

logger = logging.getLogger(__name__)


class BaostockPBFactorDownloader(BaseFactorDownloader):
    """PBç­‰å› å­æ•°æ®ä¸‹è½½å™¨ - ä¼°å€¼æŒ‡æ ‡ä¸“ç”¨ï¼ˆä¿®æ­£ç‰ˆï¼‰"""

    def __init__(self, config_path: str = 'config/factor_config.yaml'):
        super().__init__(config_path)

        # å› å­å­—æ®µæ˜ å°„ï¼ˆBaostock -> æ•°æ®åº“ï¼‰- ä¿®æ­£ç‰ˆ
        self.field_mapping = {
            # ä¼°å€¼æŒ‡æ ‡
            'peTTM': 'pe_ttm',  # æ»šåŠ¨å¸‚ç›ˆç‡
            'pbMRQ': 'pb',  # å¸‚å‡€ç‡ï¼ˆæœ€æ–°å­£æŠ¥ï¼‰
            'psTTM': 'ps_ttm',  # æ»šåŠ¨å¸‚é”€ç‡
            'pcfNcfTTM': 'pcf_ttm',  # æ»šåŠ¨å¸‚ç°ç‡ï¼ˆå¯é€‰ï¼‰

            # åŸºç¡€å­—æ®µ
            'date': 'trade_date',
            'code': 'bs_code',

            # å…¶ä»–å¯èƒ½éœ€è¦çš„å­—æ®µ - ä¿®æ­£å­—æ®µå
            'turn': 'turnover_rate_f',  # æµé€šæ¢æ‰‹ç‡ï¼ˆæ­£ç¡®å­—æ®µåï¼‰
            'tradestatus': 'trade_status',  # äº¤æ˜“çŠ¶æ€
        }

        # æ•°æ®åº“éœ€è¦çš„æ‰€æœ‰å› å­å­—æ®µï¼ˆæ ¹æ®æ‚¨æä¾›çš„è¡¨ç»“æ„ï¼‰
        self.target_factor_fields = [
            'pe', 'pe_ttm', 'pb', 'ps', 'ps_ttm',
            'dv_ratio', 'dv_ttm', 'total_share', 'float_share',
            'free_share', 'total_mv', 'circ_mv'
        ]

        # æœ¬æ¬¡ä¸‹è½½çš„å› å­å­—æ®µï¼ˆä¼°å€¼æŒ‡æ ‡ï¼‰
        self.download_factor_fields = ['pe_ttm', 'pb', 'ps_ttm']

        logger.info(f"åˆå§‹åŒ–PBå› å­ä¸‹è½½å™¨ï¼Œä¸‹è½½å­—æ®µ: {self.download_factor_fields}")

    def get_baostock_fields_string(self) -> str:
        """è·å–Baostockå­—æ®µå­—ç¬¦ä¸²ï¼ˆåŒ…å«æ‰€æœ‰éœ€è¦çš„å­—æ®µï¼‰"""
        # ä»é…ç½®è·å–å­—æ®µï¼Œå¦‚æœé…ç½®ä¸­æœ‰åˆ™ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤
        config_fields = self.config.get('baostock_fields.daily_fields')
        if config_fields:
            logger.debug(f"ä½¿ç”¨é…ç½®ä¸­çš„Baostockå­—æ®µ: {config_fields}")
            return config_fields

        # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œåˆ™æ„å»ºé»˜è®¤å­—æ®µ
        base_fields = ['date', 'code']

        # åè½¬æ˜ å°„ï¼ŒæŸ¥æ‰¾æ•°æ®åº“å­—æ®µå¯¹åº”çš„Baostockå­—æ®µ
        reverse_mapping = {v: k for k, v in self.field_mapping.items()}

        # æ·»åŠ æ‰€æœ‰éœ€è¦ä¸‹è½½çš„å› å­å­—æ®µ
        for db_field in self.download_factor_fields:
            if db_field in reverse_mapping:
                baostock_field = reverse_mapping[db_field]
                if baostock_field not in base_fields:
                    base_fields.append(baostock_field)

        # æ·»åŠ å…¶ä»–å¯èƒ½éœ€è¦çš„å­—æ®µ - ä¿®æ­£å­—æ®µå
        additional_fields = ['turn', 'tradestatus', 'adjustflag']
        for field in additional_fields:
            if field not in base_fields:
                base_fields.append(field)

        fields_str = ','.join(base_fields)
        logger.debug(f"æ„å»ºçš„Baostockå­—æ®µ: {fields_str}")
        return fields_str

    # åœ¨ baostock_pb_factor_downloader.py ä¸­æ·»åŠ æ—¥æœŸæ ¼å¼è½¬æ¢å‡½æ•°

    def _convert_date_format(self, date_str: str) -> str:
        """
        è½¬æ¢æ—¥æœŸæ ¼å¼ä¸ºBaostockéœ€è¦çš„æ ¼å¼ (YYYY-MM-DD -> YYYYMMDD)

        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼

        Returns:
            YYYYMMDDæ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²
        """
        if not date_str:
            return date_str

        try:
            # ç§»é™¤åˆ†éš”ç¬¦
            date_str = str(date_str).strip()

            # å¦‚æœå·²ç»æ˜¯8ä½æ•°å­—ï¼Œç›´æ¥è¿”å›
            if date_str.isdigit() and len(date_str) == 8:
                return date_str

            # å°è¯•è§£æå¸¸è§æ ¼å¼
            from datetime import datetime

            formats_to_try = [
                '%Y-%m-%d',  # 2025-12-01
                '%Y/%m/%d',  # 2025/12/01
                '%Y%m%d',  # 20251201
                '%Y-%m-%d %H:%M:%S',  # 2025-12-01 00:00:00
            ]

            for fmt in formats_to_try:
                try:
                    dt = datetime.strptime(date_str, fmt)
                    return dt.strftime('%Y%m%d')
                except:
                    continue

            # å¦‚æœéƒ½å¤±è´¥ï¼Œå°è¯•ç®€å•å¤„ç†
            date_str = date_str.replace('-', '').replace('/', '').replace(' ', '')
            if len(date_str) >= 8:
                return date_str[:8]

            raise ValueError(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")

        except Exception as e:
            logger.warning(f"æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥ {date_str}: {e}")
            # è¿”å›åŸå§‹å€¼
            return date_str

    # ä¿®æ”¹ fetch_factor_data æ–¹æ³•ï¼Œåœ¨å¼€å§‹å¤„æ·»åŠ æ—¥æœŸè½¬æ¢ï¼š
    def fetch_factor_data(self, symbol: str, start_date: str,
                          end_date: str) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨çš„å› å­æ•°æ®ï¼ˆä½¿ç”¨äº¤æ˜“æ—¥éªŒè¯ï¼‰
        """
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        start_date = self._convert_date_format(start_date)
        end_date = self._convert_date_format(end_date)

        # è·å–äº¤æ˜“æ—¥ç®¡ç†å™¨
        trade_manager = get_enhanced_trade_date_manager()

        # ç»§ç»­åŸæœ‰é€»è¾‘...

        # éªŒè¯æ—¥æœŸæ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        start_valid, start_reason = trade_manager.validate_trade_date(start_date)
        end_valid, end_reason = trade_manager.validate_trade_date(end_date)

        if not start_valid:
            logger.warning(f"å¼€å§‹æ—¥æœŸ {start_date} ä¸æ˜¯äº¤æ˜“æ—¥: {start_reason}")
            # è‡ªåŠ¨è°ƒæ•´ä¸ºæœ€è¿‘çš„äº¤æ˜“æ—¥
            adjusted_start = trade_manager.get_last_trade_date_str(start_date)
            logger.info(f"è‡ªåŠ¨è°ƒæ•´ä¸º: {adjusted_start}")
            start_date = adjusted_start

        if not end_valid:
            logger.warning(f"ç»“æŸæ—¥æœŸ {end_date} ä¸æ˜¯äº¤æ˜“æ—¥: {end_reason}")
            adjusted_end = trade_manager.get_last_trade_date_str(end_date)
            logger.info(f"è‡ªåŠ¨è°ƒæ•´ä¸º: {adjusted_end}")
            end_date = adjusted_end

        # ç¡®ä¿å¼€å§‹æ—¥æœŸä¸æ™šäºç»“æŸæ—¥æœŸ
        if start_date > end_date:
            logger.warning(f"å¼€å§‹æ—¥æœŸ {start_date} æ™šäºç»“æŸæ—¥æœŸ {end_date}")
            # äº¤æ¢æ—¥æœŸ
            start_date, end_date = end_date, start_date

        # ç»§ç»­åŸæœ‰çš„ä¸‹è½½é€»è¾‘
        with self._download_lock:
            return self._fetch_factor_data_sync(symbol, start_date, end_date)

    def _fetch_factor_data_sync(self, symbol: str, start_date: str,
                                end_date: str) -> Optional[pd.DataFrame]:
        """åŒæ­¥è·å–å› å­æ•°æ®ï¼ˆå†…éƒ¨å®ç°ï¼‰"""
        self._ensure_logged_in()

        bs_code = self._convert_to_bs_code(symbol)
        if not self._is_valid_stock(bs_code):
            logger.warning(f"âš ï¸ è·³è¿‡éè‚¡ç¥¨ä»£ç : {bs_code}")
            return pd.DataFrame()

        # æ ¼å¼åŒ–æ—¥æœŸ
        formatted_start = self._format_date_for_baostock(start_date)
        formatted_end = self._format_date_for_baostock(end_date)

        logger.info(f"ğŸ“¥ è·å–å› å­æ•°æ®: {bs_code} [{formatted_start} - {formatted_end}]")

        max_retries = self.config.get('execution.max_retries', 3)
        retry_delay_base = self.config.get('execution.retry_delay_base', 3)

        # æ„å»ºè¯·æ±‚å­—æ®µ
        fields_str = self.get_baostock_fields_string()

        for attempt in range(max_retries):
            try:
                self._enforce_rate_limit()

                if attempt > 0:
                    logger.info(f"é‡è¯•ç™»å½• (å°è¯• {attempt + 1}/{max_retries})")
                    self._login_baostock()

                # è·å–å› å­æ•°æ®
                logger.debug(f"è¯·æ±‚å­—æ®µ: {fields_str}")
                rs = bs.query_history_k_data_plus(
                    code=bs_code,
                    fields=fields_str,
                    start_date=formatted_start,
                    end_date=formatted_end,
                    frequency="d",
                    adjustflag="3"  # ä¸å¤æƒ
                )

                if rs.error_code != '0':
                    error_msg = rs.error_msg
                    # å¦‚æœæ˜¯æ•°æ®ä¸å­˜åœ¨é”™è¯¯ï¼Œè¿”å›ç©ºDataFrame
                    if "ä¸å­˜åœ¨" in error_msg or "æœªæ‰¾åˆ°" in error_msg:
                        logger.info(f"æ— å› å­æ•°æ®: {bs_code}")
                        return pd.DataFrame()
                    raise RuntimeError(f"Baostock error: {error_msg}")

                # å®‰å…¨è·å–æ•°æ®
                data_list = self._safe_fetch_data(rs)

                if not data_list:
                    logger.info(f"æ— å› å­æ•°æ®: {bs_code}")
                    return pd.DataFrame()

                # è½¬æ¢ä¸ºDataFrame
                df = pd.DataFrame(data_list, columns=rs.fields)

                # å¤„ç†æ•°æ®
                df_processed = self._process_factor_data(df, symbol)

                # æ›´æ–°ç»Ÿè®¡
                self._update_stats(success=True, records=len(df_processed))

                logger.info(f"âœ… è·å–å› å­æ•°æ®æˆåŠŸ: {bs_code}, {len(df_processed)} æ¡è®°å½•")
                return df_processed

            except Exception as e:
                err_str = str(e).lower()
                wait_sec = retry_delay_base + attempt * 2 + random.uniform(0, 1)

                # å¯¹ç‰¹å®šé”™è¯¯ç±»å‹å¢åŠ ç­‰å¾…æ—¶é—´
                if any(kw in err_str for kw in ['utf', 'codec', 'decompress', 'invalid', 'timeout']):
                    wait_sec *= 2

                logger.warning(f"âš ï¸ å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {str(e)[:100]} â†’ ç­‰å¾… {wait_sec:.1f}s")

                if attempt < max_retries - 1:
                    time.sleep(wait_sec)
                else:
                    logger.error(f"âŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥: {symbol}")
                    self._update_stats(success=False)
                    return pd.DataFrame()

        return pd.DataFrame()

    # åœ¨ _process_factor_data æ–¹æ³•ä¸­ï¼Œä¿®æ”¹ symbol å¤„ç†é€»è¾‘ï¼š

    def _process_factor_data(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """å¤„ç†å› å­æ•°æ®ï¼Œè½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼"""
        if df.empty:
            return df

        df_processed = df.copy()

        # 1. é‡å‘½ååˆ—ï¼ˆBaostock -> æ•°æ®åº“ï¼‰
        rename_dict = {}
        for baostock_field, db_field in self.field_mapping.items():
            if baostock_field in df_processed.columns and db_field not in df_processed.columns:
                rename_dict[baostock_field] = db_field

        if rename_dict:
            df_processed = df_processed.rename(columns=rename_dict)

        # 2. âœ… ç¡®ä¿ symbol å­—æ®µæ ‡å‡†åŒ–ï¼ˆå…³é”®ä¿®å¤ï¼‰
        if 'bs_code' in df_processed.columns:
            # ä» bs_code (sh.600519) è½¬æ¢ä¸º sh600519
            df_processed['symbol'] = df_processed['bs_code'].apply(
                lambda x: str(x).replace('.', '') if pd.notna(x) else None
            )
            logger.debug(f"ä» bs_code ç”Ÿæˆæ ‡å‡†åŒ– symbol")
        elif 'code' in df_processed.columns:
            # ä» code å­—æ®µè½¬æ¢
            df_processed['symbol'] = df_processed['code'].apply(
                lambda x: str(x).replace('.', '') if pd.notna(x) else None
            )
            logger.debug(f"ä» code ç”Ÿæˆæ ‡å‡†åŒ– symbol")
        else:
            # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ä¼ å…¥çš„ symbol å‚æ•°
            df_processed['symbol'] = normalize_stock_code(symbol)
            logger.debug(f"ä½¿ç”¨å‚æ•° symbol: {symbol}")

        # 3. å¤„ç†æ—¥æœŸå­—æ®µ
        if 'trade_date' in df_processed.columns:
            # è½¬æ¢ä¸ºdatetime
            df_processed['trade_date'] = pd.to_datetime(df_processed['trade_date'], errors='coerce')
            # ç§»é™¤æ— æ•ˆæ—¥æœŸ
            df_processed = df_processed[df_processed['trade_date'].notna()]
            # è½¬æ¢ä¸ºYYYYMMDDæ ¼å¼
            df_processed['trade_date'] = df_processed['trade_date'].dt.strftime('%Y%m%d')

        # 4. è½¬æ¢æ•°å€¼ç±»å‹
        numeric_columns = []
        for field in self.download_factor_fields:
            if field in df_processed.columns:
                numeric_columns.append(field)

        # æ·»åŠ å…¶ä»–å¯èƒ½çš„æ•°å€¼åˆ—
        other_numeric = ['turnover_rate']
        for col in other_numeric:
            if col in df_processed.columns:
                numeric_columns.append(col)

        for col in numeric_columns:
            if col in df_processed.columns:
                df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')
                # å¤„ç†å¼‚å¸¸å€¼
                if col in ['pe_ttm', 'pb', 'ps_ttm']:
                    # ä¼°å€¼æŒ‡æ ‡åº”è¯¥ä¸ºæ­£æ•°ï¼Œè´Ÿå€¼æˆ–æå¤§å€¼è®¾ä¸ºNaN
                    df_processed[col] = df_processed[col].apply(
                        lambda x: x if pd.notna(x) and 0 < x < 1e6 else np.nan
                    )

        # 5. æ’åº
        if 'trade_date' in df_processed.columns:
            df_processed = df_processed.sort_values('trade_date', ascending=False)

        # 6. é‡ç½®ç´¢å¼•
        df_processed = df_processed.reset_index(drop=True)

        return df_processed

    def download_batch_factors(self, symbols: List[str],
                               start_date: str = None,
                               end_date: str = None) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡ä¸‹è½½å› å­æ•°æ®ï¼ˆæ™ºèƒ½äº¤æ˜“æ—¥å¤„ç†ï¼‰- ä½¿ç”¨å¢å¼ºç‰ˆäº¤æ˜“æ—¥ç®¡ç†å™¨
        """
        # è·å–å¢å¼ºç‰ˆäº¤æ˜“æ—¥ç®¡ç†å™¨
        trade_manager = get_enhanced_trade_date_manager()

        # --- æ™ºèƒ½æ—¥æœŸèŒƒå›´å¤„ç† ---
        # 1. å¤„ç†ç»“æŸæ—¥æœŸ
        if not end_date:
            # ä½¿ç”¨æœ€åä¸€ä¸ªäº¤æ˜“æ—¥
            end_date = trade_manager.get_last_trade_date_str()
            logger.debug(f"æœªæŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨æœ€åäº¤æ˜“æ—¥: {end_date}")

        # 2. éªŒè¯å¹¶è°ƒæ•´ç»“æŸæ—¥æœŸ
        end_valid, end_reason = trade_manager.validate_trade_date(end_date)
        if not end_valid:
            logger.warning(f"ç»“æŸæ—¥æœŸ {end_date} ä¸æ˜¯äº¤æ˜“æ—¥: {end_reason}")
            adjusted_end = trade_manager.get_last_trade_date_str(end_date)
            logger.info(f"è‡ªåŠ¨è°ƒæ•´ä¸º: {adjusted_end}")
            end_date = adjusted_end

        # 3. å¤„ç†å¼€å§‹æ—¥æœŸ
        if not start_date:
            # é»˜è®¤å›æº¯30ä¸ªäº¤æ˜“æ—¥ï¼ˆè€Œä¸æ˜¯å›ºå®šå¤©æ•°ï¼‰
            default_trade_days = self.config.get('date_range.default_trade_days_back', 30)

            # ä½¿ç”¨äº¤æ˜“æ—¥ç®¡ç†å™¨è®¡ç®—å‡†ç¡®çš„äº¤æ˜“æ—¥èŒƒå›´
            start_date, _ = trade_manager.get_trade_date_range_str(
                days_back=default_trade_days,
                end_date_str=end_date
            )
            logger.debug(f"æœªæŒ‡å®šå¼€å§‹æ—¥æœŸï¼Œè‡ªåŠ¨å›æº¯{default_trade_days}ä¸ªäº¤æ˜“æ—¥: {start_date}")
        else:
            # éªŒè¯å¹¶è°ƒæ•´å¼€å§‹æ—¥æœŸ
            start_valid, start_reason = trade_manager.validate_trade_date(start_date)
            if not start_valid:
                logger.warning(f"å¼€å§‹æ—¥æœŸ {start_date} ä¸æ˜¯äº¤æ˜“æ—¥: {start_reason}")
                adjusted_start = trade_manager.get_last_trade_date_str(start_date)
                logger.info(f"è‡ªåŠ¨è°ƒæ•´ä¸º: {adjusted_start}")
                start_date = adjusted_start

        # 4. ç¡®ä¿æ—¥æœŸèŒƒå›´æœ‰æ•ˆæ€§
        if start_date > end_date:
            logger.warning(f"å¼€å§‹æ—¥æœŸ {start_date} æ™šäºç»“æŸæ—¥æœŸ {end_date}ï¼Œè‡ªåŠ¨äº¤æ¢æ—¥æœŸ")
            start_date, end_date = end_date, start_date

        # 5. è·å–å®Œæ•´äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆç”¨äºæ—¥å¿—å’ŒéªŒè¯ï¼‰
        trade_dates = trade_manager.get_trade_dates_in_range(start_date, end_date)
        if not trade_dates:
            logger.error(f"æ—¥æœŸèŒƒå›´ {start_date} - {end_date} å†…æ²¡æœ‰äº¤æ˜“æ—¥")
            return {}

        logger.info(f"ğŸ“… äº¤æ˜“æ—¥èŒƒå›´: {start_date} - {end_date}")
        logger.info(f"ğŸ“ˆ å®é™…äº¤æ˜“æ—¥æ•°: {len(trade_dates)}ä¸ª")

        # --- å¼€å§‹æ‰¹é‡ä¸‹è½½ ---
        self.download_stats['start_time'] = datetime.now()
        self.reset_stats()

        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½å› å­æ•°æ®")
        logger.info(f"ğŸ“Š è‚¡ç¥¨æ•°é‡: {len(symbols)}")
        logger.info(f"âš™ï¸  å•çº¿ç¨‹æ¨¡å¼")

        results = {}
        failed_symbols = []

        # å•çº¿ç¨‹é¡ºåºå¤„ç†
        for i, symbol in enumerate(symbols, 1):
            try:
                logger.info(f"[{i}/{len(symbols)}] å¤„ç† {symbol}")

                # ä½¿ç”¨å·²ç»éªŒè¯è°ƒæ•´è¿‡çš„æ—¥æœŸ
                df = self.fetch_factor_data(symbol, start_date, end_date)

                if df is not None and not df.empty:
                    results[symbol] = df

                    # æ£€æŸ¥å®é™…è·å–çš„äº¤æ˜“æ—¥æ•°
                    actual_dates = len(df)
                    expected_dates = len(trade_dates)
                    if actual_dates < expected_dates:
                        logger.warning(f"  âš ï¸  æ•°æ®ä¸å®Œæ•´: {actual_dates}/{expected_dates}ä¸ªäº¤æ˜“æ—¥")
                    else:
                        logger.info(f"  âœ… æˆåŠŸ: {actual_dates} æ¡è®°å½•")

                    # æ˜¾ç¤ºæ ·æœ¬æ•°æ®
                    if i <= 3:  # åªæ˜¾ç¤ºå‰3åªè‚¡ç¥¨çš„æ ·æœ¬
                        self._log_sample_data(symbol, df)
                else:
                    failed_symbols.append(symbol)
                    logger.warning(f"  âš ï¸  æ— æ•°æ®")

                # è¿›åº¦æŠ¥å‘Š
                if i % self.config.get('batch.progress_report_interval', 10) == 0:
                    self._report_progress(i, len(symbols))

                # è¯·æ±‚é—´éš”ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
                if i < len(symbols):
                    sleep_time = self.request_interval + random.uniform(0, 0.5)
                    time.sleep(sleep_time)

            except Exception as e:
                failed_symbols.append(symbol)
                logger.error(f"  âŒ å¤„ç†å¤±è´¥: {e}")
                logger.debug(f"å¤±è´¥è¯¦æƒ…:", exc_info=True)

        # ç»“æŸç»Ÿè®¡
        self.download_stats['end_time'] = datetime.now()

        # ç”ŸæˆæŠ¥å‘Š
        self._generate_batch_report(results, failed_symbols, start_date, end_date)

        # é¢å¤–ç»Ÿè®¡äº¤æ˜“æ—¥è¦†ç›–æƒ…å†µ
        if results:
            self._analyze_trade_date_coverage(results, trade_dates)

        return results

    def _analyze_trade_date_coverage(self, results: Dict[str, pd.DataFrame],
                                     expected_dates: List[str]) -> None:
        """
        åˆ†æäº¤æ˜“æ—¥è¦†ç›–æƒ…å†µ
        """
        if not results:
            return

        logger.info("ğŸ“Š äº¤æ˜“æ—¥è¦†ç›–åˆ†æ:")

        # ç»Ÿè®¡æ¯åªè‚¡ç¥¨çš„äº¤æ˜“æ—¥è¦†ç›–
        coverage_stats = {}
        for symbol, df in results.items():
            if 'date' in df.columns:
                actual_dates = set(df['date'].astype(str).tolist())
                expected_set = set(expected_dates)
                missing_dates = expected_set - actual_dates
                coverage = len(actual_dates) / len(expected_set) * 100
                coverage_stats[symbol] = {
                    'coverage_pct': coverage,
                    'missing_count': len(missing_dates)
                }

        if coverage_stats:
            avg_coverage = sum(stats['coverage_pct'] for stats in coverage_stats.values()) / len(coverage_stats)
            logger.info(f"  å¹³å‡äº¤æ˜“æ—¥è¦†ç›–ç‡: {avg_coverage:.1f}%")

            # æ‰¾å‡ºè¦†ç›–ç‡æœ€ä½çš„è‚¡ç¥¨
            worst_symbol = min(coverage_stats.items(),
                               key=lambda x: x[1]['coverage_pct'])
            if worst_symbol[1]['coverage_pct'] < 95:
                logger.warning(f"  æœ€ä½è¦†ç›–ç‡: {worst_symbol[0]} ({worst_symbol[1]['coverage_pct']:.1f}%)")

    def _log_sample_data(self, symbol: str, df: pd.DataFrame, num_rows: int = 3):
        """è®°å½•æ ·æœ¬æ•°æ®"""
        if df.empty:
            return

        sample = df.head(num_rows).copy()

        # é€‰æ‹©æ˜¾ç¤ºçš„åˆ—
        display_cols = ['trade_date']
        for field in self.download_factor_fields:
            if field in sample.columns:
                display_cols.append(field)

        # æ·»åŠ æ¢æ‰‹ç‡
        if 'turnover_rate' in sample.columns:
            display_cols.append('turnover_rate')

        # åªå–å­˜åœ¨çš„åˆ—
        display_cols = [col for col in display_cols if col in sample.columns]

        if display_cols:
            sample_display = sample[display_cols]
            logger.debug(f"  ğŸ“Š {symbol} æ ·æœ¬æ•°æ®:\n{sample_display.to_string()}")

    def _report_progress(self, current: int, total: int):
        """æŠ¥å‘Šè¿›åº¦"""
        progress_pct = (current / total) * 100

        # ç¡®ä¿start_timeå·²è®¾ç½®
        if self.download_stats.get('start_time'):
            elapsed = (datetime.now() - self.download_stats['start_time']).total_seconds()

            if elapsed > 0:
                speed = current / elapsed  # è‚¡ç¥¨/ç§’
                eta = (total - current) / speed if speed > 0 else 0
            else:
                speed = 0
                eta = 0

            logger.info(
                f"ğŸ“ˆ è¿›åº¦: {current}/{total} ({progress_pct:.1f}%) | "
                f"é€Ÿåº¦: {speed:.2f} è‚¡ç¥¨/ç§’ | "
                f"é¢„è®¡å‰©ä½™: {eta / 60:.1f}åˆ†é’Ÿ"
            )
        else:
            logger.info(f"ğŸ“ˆ è¿›åº¦: {current}/{total} ({progress_pct:.1f}%)")

    def _generate_batch_report(self, results: Dict[str, pd.DataFrame],
                               failed_symbols: List[str],
                               start_date: str, end_date: str):
        """ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Š"""
        total_symbols = len(results) + len(failed_symbols)
        success_count = len(results)
        fail_count = len(failed_symbols)

        total_records = sum(len(df) for df in results.values())

        # å®‰å…¨è®¡ç®—æŒç»­æ—¶é—´
        start_time = self.download_stats.get('start_time')
        end_time = self.download_stats.get('end_time')

        if start_time and end_time:
            duration = (end_time - start_time).total_seconds()
        else:
            duration = 0
            logger.warning("æ— æ³•è®¡ç®—æŒç»­æ—¶é—´ï¼Œå¼€å§‹æˆ–ç»“æŸæ—¶é—´ä¸ºç©º")

        report = {
            'batch_id': f"factor_batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'timestamp': datetime.now().isoformat(),
            'date_range': {'start': start_date, 'end': end_date},
            'statistics': {
                'total_symbols': total_symbols,
                'successful': success_count,
                'failed': fail_count,
                'success_rate': (success_count / total_symbols * 100) if total_symbols > 0 else 0,
                'total_records': total_records,
                'duration_seconds': duration,
                'speed_symbols_per_second': success_count / duration if duration > 0 else 0
            },
            'success_symbols': list(results.keys()),
            'failed_symbols': failed_symbols,
            'factors_downloaded': self.download_factor_fields
        }

        # è®°å½•æŠ¥å‘Š
        logger.info(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ:")
        logger.info(f"  æˆåŠŸ: {success_count}/{total_symbols} ({report['statistics']['success_rate']:.1f}%)")
        logger.info(f"  å¤±è´¥: {fail_count}")
        logger.info(f"  æ€»è®°å½•: {total_records}")
        logger.info(f"  è€—æ—¶: {duration:.1f}ç§’")

        if duration > 0:
            logger.info(f"  é€Ÿåº¦: {report['statistics']['speed_symbols_per_second']:.2f} è‚¡ç¥¨/ç§’")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        if self.config.get('monitoring.save_report', True):
            self._save_report_to_file(report)

        return report

    def _save_report_to_file(self, report: Dict[str, Any]):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            report_dir_str = self.config.get('monitoring.report_dir', 'data/reports/factors')
            report_dir = Path(report_dir_str)
            report_dir.mkdir(parents=True, exist_ok=True)

            report_file = report_dir / f"{report['batch_id']}.json"

            import json
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)

            logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

    def validate_factor_data(self, df: pd.DataFrame) -> Tuple[bool, List[str]]:
        """
        éªŒè¯å› å­æ•°æ®è´¨é‡

        Args:
            df: è¦éªŒè¯çš„DataFrame

        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é—®é¢˜åˆ—è¡¨)
        """
        if df.empty:
            return False, ["æ•°æ®ä¸ºç©º"]

        problems = []

        # 1. æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['symbol', 'trade_date']
        missing_fields = [field for field in required_fields if field not in df.columns]
        if missing_fields:
            problems.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")

        # 2. æ£€æŸ¥è‡³å°‘æœ‰ä¸€ä¸ªå› å­å­—æ®µ
        factor_fields_present = [field for field in self.download_factor_fields if field in df.columns]
        if not factor_fields_present:
            problems.append(f"æ— æœ‰æ•ˆå› å­å­—æ®µï¼ŒæœŸæœ›: {self.download_factor_fields}")

        # 3. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if 'trade_date' in df.columns:
            date_count = df['trade_date'].notna().sum()
            if date_count < len(df):
                problems.append(f"æ—¥æœŸå­—æ®µç¼ºå¤±: {len(df) - date_count}æ¡")

        # 4. æ£€æŸ¥æ•°å€¼èŒƒå›´
        for factor_field in factor_fields_present:
            if factor_field in df.columns:
                valid_count = df[factor_field].apply(
                    lambda x: pd.notna(x) and 0 < x < 1e6
                ).sum()

                invalid_count = len(df) - valid_count
                if invalid_count > 0:
                    problems.append(f"{factor_field}: {invalid_count}ä¸ªæ— æ•ˆå€¼")

        return len(problems) == 0, problems


def test_pb_factor_downloader():
    """æµ‹è¯•PBå› å­ä¸‹è½½å™¨"""
    import sys
    import logging as log

    # é…ç½®è¯¦ç»†æ—¥å¿—
    log.basicConfig(
        level=log.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("ğŸ§ª æµ‹è¯•PBå› å­ä¸‹è½½å™¨")
    print("=" * 50)

    try:
        # åˆ›å»ºä¸‹è½½å™¨
        downloader = BaostockPBFactorDownloader()

        # æµ‹è¯•ç™»å½•
        downloader._ensure_logged_in()
        if not hasattr(downloader, 'lg') or not downloader.lg:
            print("âŒ Baostockç™»å½•å¤±è´¥")
            return False

        print("âœ… Baostockç™»å½•æˆåŠŸ")

        # æµ‹è¯•å­—æ®µæ˜ å°„
        print("\nğŸ” æµ‹è¯•å­—æ®µæ˜ å°„:")
        print(f"  ä¸‹è½½çš„å› å­å­—æ®µ: {downloader.download_factor_fields}")
        fields_str = downloader.get_baostock_fields_string()
        print(f"  Baostockå­—æ®µå­—ç¬¦ä¸²: {fields_str}")

        # æµ‹è¯•å•åªè‚¡ç¥¨ä¸‹è½½
        print("\nğŸ“¥ æµ‹è¯•å•åªè‚¡ç¥¨ä¸‹è½½:")
        test_symbol = '600519'  # è´µå·èŒ…å°

        # è®¾ç½®æ—¥æœŸèŒƒå›´ï¼ˆæœ€è¿‘7å¤©ï¼Œé¿å…æ•°æ®è¿‡å¤šï¼‰
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=7)).strftime('%Y%m%d')

        print(f"  è‚¡ç¥¨: {test_symbol}")
        print(f"  æ—¥æœŸèŒƒå›´: {start_date} - {end_date}")

        df = downloader.fetch_factor_data(test_symbol, start_date, end_date)

        if df is not None and not df.empty:
            print(f"  âœ… ä¸‹è½½æˆåŠŸ: {len(df)} æ¡è®°å½•")

            # æ˜¾ç¤ºåˆ—ä¿¡æ¯
            print(f"  æ•°æ®åˆ—: {list(df.columns)}")

            # æ˜¾ç¤ºæ ·æœ¬æ•°æ®
            if not df.empty:
                sample = df.head(3)
                display_cols = ['trade_date']
                for field in downloader.download_factor_fields:
                    if field in sample.columns:
                        display_cols.append(field)

                if display_cols:
                    sample_display = sample[display_cols]
                    print(f"  æ ·æœ¬æ•°æ®:")
                    print(sample_display.to_string())

            # éªŒè¯æ•°æ®
            is_valid, problems = downloader.validate_factor_data(df)
            if is_valid:
                print("  âœ… æ•°æ®éªŒè¯é€šè¿‡")
            else:
                print(f"  âš ï¸  æ•°æ®éªŒè¯é—®é¢˜: {problems}")
        else:
            print(f"  âš ï¸  æ— æ•°æ®")

        # æµ‹è¯•æ‰¹é‡ä¸‹è½½ï¼ˆå°‘é‡è‚¡ç¥¨ï¼‰
        print("\nğŸš€ æµ‹è¯•æ‰¹é‡ä¸‹è½½ï¼ˆå°‘é‡è‚¡ç¥¨ï¼‰:")
        test_symbols = ['600519', '000001', '000858']  # èŒ…å°ã€å¹³å®‰ã€äº”ç²®æ¶²

        results = downloader.download_batch_factors(
            symbols=test_symbols,
            start_date=start_date,
            end_date=end_date
        )

        print(f"  æ‰¹é‡å¤„ç†ç»“æœ:")
        print(f"    æˆåŠŸ: {len(results)} åª")
        print(f"    æ€»è®°å½•: {sum(len(df) for df in results.values())}")

        # æ˜¾ç¤ºç»Ÿè®¡
        stats = downloader.get_download_stats()
        print(f"\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        print(f"    æ€»è¯·æ±‚: {stats['total_requests']}")
        print(f"    æˆåŠŸ: {stats['successful']}")
        print(f"    å¤±è´¥: {stats['failed']}")
        if 'success_rate' in stats:
            print(f"    æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        print(f"    æ€»è®°å½•: {stats['total_records']}")

        # é€€å‡ºç™»å½•
        downloader.logout()
        print("\nâœ… PBå› å­ä¸‹è½½å™¨æµ‹è¯•å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_pb_factor_downloader()
    sys.exit(0 if success else 1)