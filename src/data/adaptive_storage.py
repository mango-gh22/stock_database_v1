# _*_ coding: utf-8 _*_
# File Path: E:/MyFile/stock_database_v1/src/data\adaptive_storage.py
# File Name: adaptive_storage
# @ Author: mango-gh22
# @ Dateï¼š2025/12/10 22:15
"""
desc é€‚é…ç°æœ‰è¡¨ç»“æ„çš„å­˜å‚¨ç±»
è‡ªé€‚åº”æ•°æ®å­˜å‚¨å™¨ - æ ¹æ®å®é™…è¡¨ç»“æ„åŠ¨æ€é€‚é…
"""

import logging

from src.database.db_connector import DatabaseConnector
from src.utils.logger import get_logger
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Tuple, Dict, List, Optional, Any
from src.utils.code_converter import normalize_stock_code

logger = get_logger(__name__)

class AdaptiveDataStorage:
    """è‡ªé€‚åº”æ•°æ®å­˜å‚¨å™¨ - æ ¹æ®è¡¨ç»“æ„åŠ¨æ€é€‚é…"""

    def __init__(self, config_path: str = 'config/database.yaml', table_name: str = 'stock_daily_data'):
        """
        è‡ªé€‚åº”æ•°æ®å­˜å‚¨å™¨åˆå§‹åŒ–

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            table_name: ç›®æ ‡è¡¨å
        """
        self.logger = logging.getLogger(__name__)  # ğŸ‘ˆ æ·»åŠ è¿™è¡Œ

        if config_path is None:
            from pathlib import Path
            project_root = Path(__file__).parent.parent.parent
            config_path = str(project_root / "config" / "database.yaml")

        self.db_connector = DatabaseConnector(config_path)
        self.table_name = table_name
        self.table_columns = self._load_table_columns()
        self.column_mapping = self._create_column_mapping()
        logger.info(f"è‡ªé€‚åº”æ•°æ®å­˜å‚¨å™¨åˆå§‹åŒ–å®Œæˆ: {table_name}, {len(self.table_columns)}åˆ—")

    def _load_table_columns(self) -> List[str]:
        """åŠ è½½è¡¨ä¸­å®é™…å­˜åœ¨çš„åˆ—"""
        try:
            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(f"DESCRIBE {self.table_name}")
                    columns = cursor.fetchall()
                    return [col[0] for col in columns]
        except Exception as e:
            logger.error(f"åŠ è½½è¡¨åˆ—å¤±è´¥: {e}")
            # è¿”å›å¸¸è§åˆ—ä½œä¸ºåå¤‡
            return [
                'id', 'symbol', 'trade_date', 'open', 'close', 'high', 'low',
                'pre_close', 'volume', 'amount', 'pct_change', 'change_amount',
                'turnover_rate', 'turnover_rate_f', 'volume_ratio',
                'ma5', 'ma10', 'ma20', 'ma30', 'ma60', 'ma120', 'ma250',
                'amplitude', 'created_at', 'updated_at'
            ]

    def _create_column_mapping(self) -> Dict[str, str]:
        """åˆ›å»ºåˆ—åæ˜ å°„å…³ç³» - v0.6.2 ä¿®å¤ç‰ˆï¼ˆä»…ä¿®æ”¹5ä¸ªå­—æ®µï¼‰"""
        mapping = {
            # åŸºç¡€ä»·æ ¼æ•°æ®æ˜ å°„ - ä¿®å¤ï¼šç›®æ ‡åˆ—å¿…é¡»ä¸æ•°æ®åº“ä¸€è‡´
            'open_price': 'open_price',      # âœ… ä¿®å¤ï¼š'open' â†’ 'open_price'
            'high_price': 'high_price',      # âœ… ä¿®å¤ï¼š'high' â†’ 'high_price'
            'low_price': 'low_price',        # âœ… ä¿®å¤ï¼š'low' â†’ 'low_price'
            'close_price': 'close_price',    # âœ… ä¿®å¤ï¼š'close' â†’ 'close_price'
            'pre_close_price': 'pre_close_price',  # âœ… ä¿®å¤ï¼š'pre_close' â†’ 'pre_close_price'

            # å…¶ä½™å­—æ®µä¿æŒä¸å˜ï¼ˆå·²ç»æ­£ç¡®ï¼‰
            'change_percent': 'pct_change',
            'change_amount': 'change_amount',
            'volume': 'volume',
            'amount': 'amount',
            'amplitude': 'amplitude',

            # æŠ€æœ¯æŒ‡æ ‡æ˜ å°„ï¼ˆæ­£ç¡®ï¼‰
            'ma5': 'ma5',
            'ma10': 'ma10',
            'ma20': 'ma20',
            'ma30': 'ma30',
            'ma60': 'ma60',
            'ma120': 'ma120',
            'ma250': 'ma250',

            # æ—¥æœŸå’Œä»£ç 
            'trade_date': 'trade_date',
            'symbol': 'symbol',

            # å…ƒæ•°æ®
            'created_at': 'created_at',
            'updated_at': 'updated_at'
        }

        # éªŒè¯æ˜ å°„ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
        valid_mapping = {}
        for source_col, target_col in mapping.items():
            if target_col in self.table_columns:
                valid_mapping[source_col] = target_col
            else:
                logger.debug(f"æ˜ å°„è·³è¿‡: {source_col} â†’ {target_col} (ç›®æ ‡åˆ—ä¸å­˜åœ¨)")

        logger.info(f"åˆ—æ˜ å°„åˆ›å»ºå®Œæˆ: {len(valid_mapping)}ä¸ªæœ‰æ•ˆæ˜ å°„")
        return valid_mapping

    def store_daily_data(self, df: pd.DataFrame) -> Tuple[int, Dict]:
        """
        å­˜å‚¨æ—¥çº¿æ•°æ® - è‡ªé€‚åº”è¡¨ç»“æ„

        Args:
            df: è¦å­˜å‚¨çš„æ•°æ®

        Returns:
            (å½±å“è¡Œæ•°, è¯¦ç»†æŠ¥å‘Š)
        """
        if df.empty:
            logger.warning("æ—¥çº¿æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å­˜å‚¨")
            return 0, {'status': 'skipped', 'reason': 'empty_data'}

        try:
            # 1. å‡†å¤‡æ•°æ®ï¼ˆæ ¹æ®è¡¨ç»“æ„è°ƒæ•´ï¼‰
            df_processed = self._prepare_data(df)

            if df_processed.empty:
                return 0, {'status': 'skipped', 'reason': 'no_valid_columns'}

            # 2. æ„å»ºå¹¶æ‰§è¡ŒSQL
            affected_rows = self._execute_insert(df_processed)

            # 3. è¿”å›ç»“æœ
            symbol = df_processed['symbol'].iloc[0] if 'symbol' in df_processed.columns else 'unknown'
            logger.info(f"å­˜å‚¨å®Œæˆ: {symbol}, {affected_rows}è¡Œå½±å“")

            return affected_rows, {
                'status': 'success',
                'table': self.table_name,
                'records': len(df_processed),
                'affected': affected_rows,
                'symbol': symbol,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"å­˜å‚¨å¤±è´¥: {e}")
            return 0, {
                'status': 'error',
                'reason': str(e),
                'timestamp': datetime.now().isoformat()
            }

    def _prepare_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å‡†å¤‡æ•°æ® - v0.6.1 ä¿®å¤ç‰ˆ"""
        df_processed = df.copy()

        # v0.6.1 è¯Šæ–­æ—¥å¿—
        self.logger.info(f"ğŸ“Š å‡†å¤‡æ•°æ®: è¾“å…¥{len(df_processed)}è¡Œ, {len(df_processed.columns)}åˆ—")
        self.logger.debug(f"ğŸ“Š è¾“å…¥åˆ—å: {list(df_processed.columns)}")

        # 1. å¿«é€ŸåŒ¹é…è·¯å¾„ï¼ˆè¾“å…¥åˆ—åå·²åŒ¹é…æ•°æ®åº“ï¼‰
        direct_match_cols = [col for col in df_processed.columns if col in self.table_columns]

        if len(direct_match_cols) >= 7:  # æ ¸å¿ƒåˆ—è¶³å¤Ÿ
            self.logger.info(f"âœ… å¿«é€Ÿè·¯å¾„: {len(direct_match_cols)}åˆ—ç›´æ¥åŒ¹é…")
            df_processed = df_processed[direct_match_cols]
        else:
            # 2. å¤‡ç”¨è·¯å¾„ï¼šå°è¯•æ˜ å°„ï¼ˆä¸åº”è¯¥èµ°åˆ°è¿™é‡Œï¼‰
            self.logger.warning(f"âš ï¸ åªæœ‰{len(direct_match_cols)}åˆ—ç›´æ¥åŒ¹é…ï¼Œå°è¯•æ˜ å°„")

            rename_dict = {}
            for src, tgt in self.column_mapping.items():
                if src in df_processed.columns and tgt in self.table_columns:
                    rename_dict[src] = tgt

            if rename_dict:
                df_processed = df_processed.rename(columns=rename_dict)
                self.logger.info(f"é‡å‘½ååˆ—: {rename_dict}")

            # å†æ¬¡è¿‡æ»¤
            final_cols = [col for col in df_processed.columns if col in self.table_columns]
            df_processed = df_processed[final_cols]

        # 3. æ•°æ®ç±»å‹è½¬æ¢ï¼ˆä¿ç•™æˆåŠŸåˆ—ï¼‰
        numeric_cols = ['open_price', 'high_price', 'low_price', 'close_price', 'volume', 'amount']
        for col in numeric_cols:
            if col in df_processed.columns:
                try:
                    df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')
                except Exception as e:
                    self.logger.warning(f"åˆ—è½¬æ¢å¤±è´¥ {col}: {e}")

        # 4. å¤„ç†ç¼ºå¤±å€¼ï¼ˆåªè¿‡æ»¤ symbol/trade_date ä¸ºç©ºçš„è¡Œï¼‰
        before_filter = len(df_processed)

        # v0.6.1 å…³é”®ä¿®å¤ï¼šåªè¿‡æ»¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œï¼Œä¸åˆ é™¤ NaN æ•°å€¼
        if 'symbol' in df_processed.columns:
            df_processed = df_processed[df_processed['symbol'].notna() & (df_processed['symbol'] != '')]

        if 'trade_date' in df_processed.columns:
            df_processed = df_processed[df_processed['trade_date'].notna()]

        after_filter = len(df_processed)
        if before_filter > after_filter:
            self.logger.info(f"è¿‡æ»¤æ‰ {before_filter - after_filter} æ¡æ— æ•ˆè¡Œ")

        # 5. æ—¥æœŸæ ¼å¼åŒ–
        if 'trade_date' in df_processed.columns:
            df_processed['trade_date'] = self._standardize_date_format(df_processed['trade_date'])

        # 6. æ·»åŠ æ—¶é—´æˆ³
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        if 'created_time' in self.table_columns and 'created_time' not in df_processed.columns:
            df_processed['created_at'] = current_time
        if 'updated_time' in self.table_columns and 'updated_time' not in df_processed.columns:
            df_processed['updated_at'] = current_time

        self.logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(df_processed)}è¡Œ, {len(df_processed.columns)}åˆ—")
        self.logger.debug(f"æœ€ç»ˆåˆ—: {list(df_processed.columns)}")

        return df_processed

    def _standardize_date_format(self, date_series):
        """
        æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º 'YYYY-MM-DD'

        æ”¯æŒè¾“å…¥æ ¼å¼:
        - '20240128' -> '2024-01-28'
        - '2024-01-28' -> '2024-01-28' (ä¿æŒä¸å˜)
        - '2024/01/28' -> '2024-01-28'
        - datetimeå¯¹è±¡ -> 'YYYY-MM-DD'
        """

        def format_single_date(date_val):
            if pd.isna(date_val):
                return None

            # å¦‚æœæ˜¯å­—ç¬¦ä¸²
            if isinstance(date_val, str):
                # ç§»é™¤ç©ºæ ¼
                date_str = date_val.strip()

                # å¦‚æœå·²ç»æ˜¯ YYYY-MM-DD æ ¼å¼ï¼Œç›´æ¥è¿”å›
                if len(date_str) == 10 and date_str[4] == '-' and date_str[7] == '-':
                    return date_str

                # å¦‚æœæ˜¯ YYYY/MM/DD æ ¼å¼
                if len(date_str) == 10 and date_str[4] == '/' and date_str[7] == '/':
                    return date_str.replace('/', '-')

                # å¦‚æœæ˜¯ YYYYMMDD æ ¼å¼
                if len(date_str) == 8 and date_str.isdigit():
                    return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"

            # å¦‚æœæ˜¯datetimeæˆ–Timestamp
            elif isinstance(date_val, (datetime, pd.Timestamp)):
                return date_val.strftime('%Y-%m-%d')

            # å…¶ä»–æƒ…å†µå°è¯•è½¬æ¢
            try:
                return pd.to_datetime(date_val, errors='coerce').strftime('%Y-%m-%d')
            except:
                return None

        # åº”ç”¨æ ¼å¼åŒ–å‡½æ•°
        if isinstance(date_series, pd.Series):
            return date_series.apply(format_single_date)
        else:
            return format_single_date(date_series)

    # _*_ coding: utf-8 _*_
    # File Path: E:/MyFile/stock_database_v1/src/data\adaptive_storage.py
    # ... ä¿ç•™å¤´éƒ¨å¯¼å…¥å’Œæ–¹æ³• ...

    def _execute_insert(self, df: pd.DataFrame) -> int:
        """æ‰§è¡Œæ’å…¥æ“ä½œ - v0.6.8 æœ€ç»ˆç‰ˆï¼šç»•è¿‡æŸåçš„å”¯ä¸€é”®"""
        if df.empty:
            return 0

        # è·å–å¯æ’å…¥åˆ—ï¼ˆæ˜ç¡®æ’é™¤idï¼‰
        table_columns = self._get_insertable_columns()
        insert_columns = [col for col in table_columns if col.lower() != 'id' and col in df.columns]

        if not insert_columns:
            logger.warning("æ²¡æœ‰å¯æ’å…¥çš„åˆ—")
            return 0

        self.logger.info(f"æ’å…¥åˆ—: {len(insert_columns)}ä¸ªï¼Œè¡¨æ€»åˆ—: {len(table_columns)}ä¸ª")

        # æ„å»ºSQLï¼ˆä½¿ç”¨INSERT IGNOREé¿å¼€å†²çªï¼‰
        columns_str = ', '.join(insert_columns)
        placeholders = ', '.join(['%s'] * len(insert_columns))
        sql = f"INSERT IGNORE INTO {self.table_name} ({columns_str}) VALUES ({placeholders})"

        records = self._prepare_records(df, insert_columns)
        if not records:
            return 0

        symbol = df['symbol'].iloc[0] if 'symbol' in df else 'unknown'

        try:
            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    # å…ˆæŸ¥è¯¢å½“å‰è®°å½•æ•°
                    cursor.execute(
                        "SELECT COUNT(*) FROM {} WHERE symbol = %s".format(self.table_name),
                        (symbol,)
                    )
                    pre_count = cursor.fetchone()[0]

                    # æ‰§è¡Œæ’å…¥
                    cursor.executemany(sql, records)
                    conn.commit()

                    # æŸ¥è¯¢æ’å…¥åè®°å½•æ•°
                    cursor.execute(
                        "SELECT COUNT(*) FROM {} WHERE symbol = %s".format(self.table_name),
                        (symbol,)
                    )
                    post_count = cursor.fetchone()[0]

                    actual_new = post_count - pre_count

                    self.logger.info(f"ğŸ¯ æ’å…¥å®Œæˆ: {symbol}, æ–°å¢{actual_new}æ¡, æ€»è®¡{post_count}æ¡")
                    return actual_new
        except Exception as e:
            logger.error(f"æ’å…¥å¤±è´¥: {e}")
            return 0

    def _get_insertable_columns(self) -> List[str]:
        """è·å–å¯æ’å…¥çš„åˆ—ï¼ˆæ’é™¤è‡ªå¢ä¸»é”®ï¼‰- v0.6.2 æ–°å¢"""
        try:
            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(f"DESCRIBE {self.table_name}")
                    columns_info = cursor.fetchall()

                    insertable = []
                    for col_info in columns_info:
                        col_name = col_info[0]
                        # æ£€æŸ¥æ˜¯å¦ä¸ºè‡ªå¢ä¸»é”®
                        is_auto_increment = False
                        is_primary_key = False

                        if len(col_info) > 5:
                            extra_info = str(col_info[5]).lower()
                            is_auto_increment = 'auto_increment' in extra_info

                        if len(col_info) > 3:
                            key_info = str(col_info[3])
                            is_primary_key = key_info == 'PRI'

                        if not (is_auto_increment and is_primary_key):
                            insertable.append(col_name)
                        else:
                            self.logger.debug(f"æ’é™¤è‡ªå¢ä¸»é”®åˆ—: {col_name}")

                    return insertable
        except Exception as e:
            logger.error(f"è·å–å¯æ’å…¥åˆ—å¤±è´¥: {e}")
            # ä¿åº•æ–¹æ¡ˆï¼šæ’é™¤å·²çŸ¥çš„è‡ªå¢ä¸»é”®
            return [col for col in self.table_columns if col.lower() != 'id']

    def _prepare_records(self, df: pd.DataFrame, columns: List[str]) -> List[tuple]:
        """å‡†å¤‡æ’å…¥è®°å½•"""
        records = []

        for _, row in df.iterrows():
            record = []
            for col in columns:
                val = row[col]

                # å¤„ç†ç‰¹æ®Šå€¼
                if pd.isna(val):
                    record.append(None)
                elif isinstance(val, (np.integer, np.int64)):
                    record.append(int(val))
                elif isinstance(val, (np.floating, np.float64)):
                    # é™åˆ¶å°æ•°ä½æ•°
                    if col in ['pct_change', 'amplitude', 'turnover_rate', 'turnover_rate_f']:
                        record.append(round(float(val), 4))
                    elif col in ['open', 'close', 'high', 'low', 'pre_close',
                                 'ma5', 'ma10', 'ma20', 'ma30', 'ma60', 'ma120', 'ma250']:
                        record.append(round(float(val), 3))
                    elif col == 'amount':
                        record.append(round(float(val), 3))
                    elif col == 'volume_ratio':
                        record.append(round(float(val), 3))
                    else:
                        record.append(float(val))
                elif isinstance(val, datetime):
                    record.append(val.strftime('%Y-%m-%d %H:%M:%S'))
                elif isinstance(val, pd.Timestamp):
                    record.append(val.strftime('%Y-%m-%d %H:%M:%S'))
                else:
                    record.append(val)

            records.append(tuple(record))

        return records

    def save_daily_data(self, df: pd.DataFrame) -> bool:
        """
        å…¼å®¹æ€§æ–¹æ³•ï¼šä¿å­˜æ—¥çº¿æ•°æ®

        Args:
            df: è¦å­˜å‚¨çš„æ•°æ®

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        logger.warning("ä½¿ç”¨å…¼å®¹æ–¹æ³•save_daily_data")
        try:
            affected_rows, report = self.store_daily_data(df)
            return affected_rows > 0
        except Exception as e:
            logger.error(f"save_daily_dataå¤±è´¥: {e}")
            return False

    def batch_store_daily_data(self, data_dict: Dict[str, pd.DataFrame]) -> Dict:
        """
        æ‰¹é‡å­˜å‚¨æ—¥çº¿æ•°æ®

        Args:
            data_dict: {è‚¡ç¥¨ä»£ç : DataFrame} å­—å…¸

        Returns:
            æ‰¹é‡å¤„ç†æŠ¥å‘Š
        """
        total_symbols = len(data_dict)
        logger.info(f"å¼€å§‹æ‰¹é‡å­˜å‚¨: {total_symbols}åªè‚¡ç¥¨")

        results = {
            'total_symbols': total_symbols,
            'success_count': 0,
            'error_count': 0,
            'total_records': 0,
            'total_affected': 0,
            'details': []
        }

        for symbol, df in data_dict.items():
            try:
                affected_rows, report = self.store_daily_data(df)

                results['total_records'] += len(df)
                results['total_affected'] += affected_rows

                if report['status'] == 'success':
                    results['success_count'] += 1
                    logger.debug(f"âœ… æˆåŠŸ: {symbol}, {affected_rows}è¡Œ")
                else:
                    results['error_count'] += 1
                    logger.warning(f"âš ï¸ å¤±è´¥: {symbol}, åŸå› : {report.get('reason', 'unknown')}")

                results['details'].append({
                    'symbol': symbol,
                    'records': len(df),
                    'affected': affected_rows,
                    'status': report['status']
                })

            except Exception as e:
                results['error_count'] += 1
                logger.error(f"âŒ å¼‚å¸¸: {symbol}, é”™è¯¯: {e}")
                results['details'].append({
                    'symbol': symbol,
                    'status': 'error',
                    'error': str(e)
                })

        results['success_rate'] = (results['success_count'] / total_symbols * 100) if total_symbols > 0 else 0

        logger.info(
            f"æ‰¹é‡å­˜å‚¨å®Œæˆ: "
            f"æˆåŠŸ{results['success_count']}/{total_symbols}, "
            f"æˆåŠŸç‡{results['success_rate']:.1f}%, "
            f"æ€»è®°å½•{results['total_records']}, "
            f"æ€»å½±å“{results['total_affected']}"
        )

        return results

    def get_last_update_date(self, symbol: str) -> Optional[str]:
        """
        è·å–è‚¡ç¥¨æœ€åæ›´æ–°æ—¥æœŸ

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            æœ€åæ›´æ–°æ—¥æœŸ (YYYYMMDD) æˆ– None
        """
        try:
            # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
            normalized_symbol = normalize_stock_code(symbol)

            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    query = f"""
                        SELECT MAX(trade_date) as last_date 
                        FROM {self.table_name} 
                        WHERE symbol = %s
                    """
                    cursor.execute(query, (normalized_symbol,))
                    result = cursor.fetchone()

                    if result and result[0]:
                        # è½¬æ¢ä¸ºYYYYMMDDæ ¼å¼
                        if isinstance(result[0], str):
                            return result[0].replace('-', '')
                        else:
                            return result[0].strftime('%Y%m%d')
                    else:
                        return None

        except Exception as e:
            logger.error(f"è·å–æœ€åæ›´æ–°æ—¥æœŸå¤±è´¥ {symbol}: {e}")
            return None

    def get_stock_count(self, symbol: str) -> int:
        """
        è·å–è‚¡ç¥¨æ•°æ®è®°å½•æ•°

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            è®°å½•æ•°
        """
        try:
            normalized_symbol = normalize_stock_code(symbol)

            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    query = f"SELECT COUNT(*) FROM {self.table_name} WHERE symbol = %s"
                    cursor.execute(query, (normalized_symbol,))
                    result = cursor.fetchone()
                    return result[0] if result else 0

        except Exception as e:
            logger.error(f"è·å–è®°å½•æ•°å¤±è´¥ {symbol}: {e}")
            return 0

    def log_data_update(self, data_type: str, symbol: str, *args, **kwargs) -> Dict[str, Any]:
        """
        è®°å½•æ•°æ®æ›´æ–°æ—¥å¿— - v0.6.0 è¶…çº§å…¼å®¹ç‰ˆ
        æ”¯æŒæ‰€æœ‰å†å²è°ƒç”¨æ ¼å¼ï¼Œæ°¸ä¸æŠ›å‡ºå¼‚å¸¸
        """
        try:
            # ===== æ™ºèƒ½å‚æ•°è§£æå™¨ï¼ˆv0.6.0 å¢å¼ºï¼‰=====
            start_date = end_date = error_message = None
            execution_time = 0.0
            rows_affected = 0
            status = 'unknown'

            # å‚æ•°è®¡æ•°è°ƒè¯•
            self.logger.debug(f"[v0.6.0] log_data_update called with {len(args)} args, {len(kwargs)} kwargs")

            # æƒ…å†µ1ï¼šä½ç½®å‚æ•°æ¨¡å¼ï¼ˆdata_scheduler.py ä¸“ç”¨ï¼‰
            if len(args) == 2 and isinstance(args[0], (int, tuple, dict)):
                # æ ¼å¼: ('daily', symbol, rows_affected, status)
                rows_affected = args[0]
                status = args[1] if len(args) > 1 else 'unknown'
                self.logger.debug(f"[v0.6.0] è§£æä¸º4å‚æ•°æ¨¡å¼: rows={rows_affected}, status={status}")

            # æƒ…å†µ2ï¼šæ‰©å±•ä½ç½®å‚æ•°æ¨¡å¼
            elif len(args) >= 4:
                start_date = args[0] if len(args) > 0 else None
                end_date = args[1] if len(args) > 1 else None
                rows_affected = args[2] if len(args) > 2 else 0
                status = args[3] if len(args) > 3 else 'unknown'
                error_message = args[4] if len(args) > 4 else None
                execution_time = args[5] if len(args) > 5 else 0
                self.logger.debug(
                    f"[v0.6.0] è§£æä¸ºæ‰©å±•æ¨¡å¼: start={start_date}, end={end_date}, rows={rows_affected}")

            # æƒ…å†µ3ï¼šæ··åˆæ¨¡å¼ï¼ˆä½ç½®+å…³é”®å­—ï¼‰
            else:
                # è§£æ kwargs
                rows_affected = kwargs.get('rows_affected', 0)
                status = kwargs.get('status', 'unknown')
                start_date = kwargs.get('start_date')
                end_date = kwargs.get('end_date')
                error_message = kwargs.get('error_message')
                execution_time = kwargs.get('execution_time', 0)
                self.logger.debug(f"[v0.6.0] è§£æä¸ºå…³é”®å­—æ¨¡å¼: rows={rows_affected}, status={status}")

            # ===== å®‰å…¨æå– rows_affectedï¼ˆv0.6.0 é˜²å¼¹ç‰ˆï¼‰=====
            rows_int = self._safe_extract_rows(rows_affected)

            # ===== è®°å½•åˆ°æ•°æ®åº“æ—¥å¿—è¡¨ =====
            self._log_to_database(
                data_type=data_type,
                symbol=symbol,
                start_date=start_date,
                end_date=end_date,
                rows_affected=rows_int,
                status=status,
                error_message=error_message,
                execution_time=execution_time
            )

            self.logger.info(f"âœ… æ—¥å¿—è®°å½•æˆåŠŸ: {data_type} {symbol} rows={rows_int} status={status}")
            return {'success': True, 'rows_logged': rows_int}

        except Exception as e:
            self.logger.error(f"âŒ log_data_update è§£æå¤±è´¥ï¼ˆä½†æµç¨‹ç»§ç»­ï¼‰: {e}", exc_info=True)
            return {'success': False, 'error': str(e)}

    def _safe_extract_rows(self, rows_value: Any) -> int:
        """å®‰å…¨æå–è¡Œæ•°ï¼ˆv0.6.0 ç»ˆæç‰ˆï¼‰"""
        try:
            # æƒ…å†µ1ï¼šå·²ç»æ˜¯æ•´æ•°
            if isinstance(rows_value, int):
                return rows_value

            # æƒ…å†µ2ï¼šå…ƒç»„ (5, {...})
            if isinstance(rows_value, tuple) and len(rows_value) > 0:
                first = rows_value[0]
                return int(first) if first is not None else 0

            # æƒ…å†µ3ï¼šå­—å…¸ {'rows_affected': 5}
            if isinstance(rows_value, dict):
                return rows_value.get('rows_affected', 0)

            # æƒ…å†µ4ï¼šå­—ç¬¦ä¸²
            if isinstance(rows_value, str):
                return int(rows_value) if rows_value.isdigit() else 0

            # æƒ…å†µ5ï¼šå…¶ä»–ï¼ˆå°è¯•è½¬æ¢ï¼‰
            return int(rows_value) if rows_value is not None else 0

        except (ValueError, TypeError) as e:
            self.logger.warning(f"è¡Œæ•°æå–å¤±è´¥ï¼Œè¿”å›0: {e}, è¾“å…¥å€¼: {rows_value}")
            return 0

    def _log_to_database(self, **log_data):
        """å°†æ—¥å¿—å†™å…¥æ•°æ®åº“ï¼ˆç¡®ä¿å³ä½¿å¤±è´¥ä¹Ÿä¸å½±å“ä¸»æµç¨‹ï¼‰"""
        try:
            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    sql = """
                        INSERT INTO data_update_logs 
                        (data_type, symbol, start_date, end_date, rows_affected, status, error_message, execution_time)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    """
                    cursor.execute(sql, (
                        log_data.get('data_type'),
                        log_data.get('symbol'),
                        log_data.get('start_date'),
                        log_data.get('end_date'),
                        log_data.get('rows_affected', 0),
                        log_data.get('status', 'unknown'),
                        log_data.get('error_message'),
                        log_data.get('execution_time', 0)
                    ))
                    conn.commit()
        except Exception as e:
            self.logger.warning(f"æ•°æ®åº“æ—¥å¿—å†™å…¥å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")


# ------------------------------
# æµ‹è¯•å‡½æ•°
def test_adaptive_storage():
    """æµ‹è¯•è‡ªé€‚åº”å­˜å‚¨å™¨"""
    print("ğŸ§ª æµ‹è¯•è‡ªé€‚åº”æ•°æ®å­˜å‚¨å™¨")
    print("=" * 50)

    try:
        # 1. åˆå§‹åŒ–
        storage = AdaptiveDataStorage()
        print(f"âœ… åˆå§‹åŒ–æˆåŠŸï¼Œè¡¨: {storage.table_name}")
        print(f"   è¡¨åˆ—æ•°: {len(storage.table_columns)}")
        print(f"   æœ‰æ•ˆæ˜ å°„: {len(storage.column_mapping)}")

        # 2. åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆæ¨¡æ‹Ÿå¢å¼ºå¤„ç†å™¨è¾“å‡ºï¼‰
        import pandas as pd
        test_data = pd.DataFrame({
            'symbol': ['sh600519'] * 3,
            'trade_date': ['20241201', '20241202', '20241203'],
            'open_price': [100.123, 101.456, 102.789],
            'high_price': [105.123, 106.456, 107.789],
            'low_price': [98.123, 99.456, 100.789],
            'close_price': [103.123, 104.456, 105.789],
            'pre_close_price': [102.0, 103.0, 104.0],
            'change_percent': [1.1, 1.41, 1.72],
            'change': [1.123, 1.456, 1.789],
            'volume': [1000000, 2000000, 3000000],
            'amount': [103123000.0, 208912000.0, 317367000.0],
            'amplitude': [7.12, 7.01, 6.95],
            'ma5': [101.5, 102.2, 102.9],
            'ma10': [100.8, 101.3, 101.8],
            'ma20': [99.5, 99.8, 100.1],
            'data_source': ['baostock'] * 3,  # è¿™ä¸ªåˆ—ä¼šè¢«å¿½ç•¥
            'processed_time': [datetime.now()] * 3,  # è¿™ä¸ªåˆ—ä¼šè¢«å¿½ç•¥
            'quality_grade': ['excellent'] * 3  # è¿™ä¸ªåˆ—ä¼šè¢«å¿½ç•¥
        })

        print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {len(test_data)}æ¡, {len(test_data.columns)}åˆ—")
        print(f"   æ•°æ®åˆ—: {list(test_data.columns)}")

        # 3. æµ‹è¯•å­˜å‚¨
        print("ğŸ”§ æµ‹è¯•æ•°æ®å­˜å‚¨...")
        affected_rows, report = storage.store_daily_data(test_data)

        print(f"âœ… å­˜å‚¨ç»“æœ:")
        print(f"   çŠ¶æ€: {report['status']}")
        print(f"   å½±å“è¡Œæ•°: {affected_rows}")
        print(f"   è¡¨å: {report.get('table', 'N/A')}")
        print(f"   è®°å½•æ•°: {report.get('records', 0)}")
        print(f"   è‚¡ç¥¨: {report.get('symbol', 'unknown')}")

        # 4. æµ‹è¯•å…¼å®¹æ–¹æ³•
        print("ğŸ”§ æµ‹è¯•å…¼å®¹æ–¹æ³•...")
        saved = storage.save_daily_data(test_data)
        print(f"   å…¼å®¹æ–¹æ³•ç»“æœ: {saved}")

        # 5. æµ‹è¯•æ‰¹é‡å­˜å‚¨
        print("ğŸ”§ æµ‹è¯•æ‰¹é‡å­˜å‚¨...")
        batch_data = {
            'sz000001': test_data.copy().assign(symbol='sz000001'),
            'sz000858': test_data.copy().assign(symbol='sz000858')
        }

        batch_result = storage.batch_store_daily_data(batch_data)
        print(f"âœ… æ‰¹é‡ç»“æœ:")
        print(f"   æ€»è‚¡ç¥¨: {batch_result['total_symbols']}")
        print(f"   æˆåŠŸ: {batch_result['success_count']}")
        print(f"   å¤±è´¥: {batch_result['error_count']}")
        print(f"   æˆåŠŸç‡: {batch_result['success_rate']:.1f}%")

        print("âœ… è‡ªé€‚åº”æ•°æ®å­˜å‚¨å™¨æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(traceback.format_exc())
        return False


if __name__ == "__main__":
    success = test_adaptive_storage()
    exit(0 if success else 1)