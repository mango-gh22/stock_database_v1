# _*_ coding: utf-8 _*_
# File Path: E:/MyFile/stock_database_v1/src/data\integrated_pipeline_fixed.py
# --ä¿®æ”¹ä¸ºï¼šFile Path: E:/MyFile/stock_database_v1/src/data\integrated_pipeline.py
# File Name: integrated_pipeline
# @ Author: mango-gh22
# @ Dateï¼š2025/12/10 21:13
"""
desc ä¿®å¤ç‰ˆæ•°æ®ç®¡é“ - ç›´æ¥è§£å†³å­˜å‚¨é—®é¢˜
integrated_pipeline_fixed.py-->integrated_pipeline.py
"""
# å®Œæ•´æ›¿æ¢ä¸ºæ— æ±¡æŸ“ç‰ˆæœ¬

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import logging
import time
import sys
import importlib
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.utils.logger import get_logger
from src.data.baostock_collector import BaostockCollector
from src.data.adaptive_storage import AdaptiveDataStorage
from src.data.enhanced_processor import EnhancedDataProcessor
from src.data.storage_tracer import StorageTracer  # v0.6.0 æ–°å¢è¿½è¸ªå™¨

logger = get_logger(__name__)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


class IntegratedDataPipeline:
    """v0.6.0 çº¯å‡€ç‰ˆæ•°æ®ç®¡é“"""

    def __init__(self, config_path: Optional[str] = None):
        logger.info("ğŸš€ åˆå§‹åŒ– v0.6.0 æ•°æ®ç®¡é“")

        # v0.6.0 å¢å¼ºï¼šæ˜ç¡®é»˜è®¤è·¯å¾„
        if config_path is None:
            from pathlib import Path
            config_path = str(Path(__file__).parent.parent.parent / "config" / "database.yaml")
            logger.info(f"ä½¿ç”¨é»˜è®¤é…ç½®è·¯å¾„: {config_path}")

        # æ˜ç¡®ç±»å‹å®ä¾‹åŒ–
        self.collector = BaostockCollector()
        self.processor = EnhancedDataProcessor(config_path)
        self.storage = AdaptiveDataStorage(config_path)
        self.tracer = StorageTracer()  # v0.6.0 å­˜å‚¨è¿½è¸ªå™¨

        # éªŒè¯ç»„ä»¶
        self._validate_components()

        logger.info("âœ… æ•°æ®ç®¡é“åˆå§‹åŒ–å®Œæˆ")

    def _validate_components(self):
        """éªŒè¯ç»„ä»¶å®Œæ•´æ€§"""
        checks = {
            'collector': hasattr(self.collector, 'fetch_daily_data'),
            'processor': hasattr(self.processor, 'process_stock_data'),
            'storage': hasattr(self.storage, 'store_daily_data'),
            'tracer': hasattr(self.tracer, 'trace_store_daily_data')
        }

        logger.info(f"ç»„ä»¶éªŒè¯: {checks}")

        if not all(checks.values()):
            missing = [k for k, v in checks.items() if not v]
            raise RuntimeError(f"ç»„ä»¶ç¼ºå¤±æ–¹æ³•: {missing}")

    def process_single_stock(self, symbol: str, start_date: str, end_date: str, adjust: str = 'qfq') -> Dict[str, Any]:
        """å¤„ç†å•åªè‚¡ç¥¨ - v0.6.0 è¿½è¸ªç‰ˆ"""
        start_time = time.time()
        trace_id = f"{symbol}_{datetime.now().strftime('%H%M%S')}"

        try:
            logger.info(f"[{trace_id}] ğŸ“Š å¼€å§‹å¤„ç†: {symbol}")

            # 1. é‡‡é›†
            raw_data = self.collector.fetch_daily_data(
                # symbol=symbol, start_date=start_date, end_date=end_date, adjust=adjust
                symbol=symbol, start_date=start_date, end_date=end_date
            )

            if raw_data.empty:
                logger.warning(f"[{trace_id}] âš ï¸ æ— æ•°æ®: {symbol}")
                return {'symbol': symbol, 'status': 'no_data', 'trace_id': trace_id}

            logger.info(f"[{trace_id}] ğŸ“¥ é‡‡é›†å®Œæˆ: {len(raw_data)}æ¡")

            # 2. å¤„ç†
            processed_data, quality_report = self.processor.process_stock_data(
                raw_data, symbol, 'baostock'
            )

            if processed_data.empty:
                logger.warning(f"[{trace_id}] âš ï¸ å¤„ç†ä¸ºç©º: {symbol}")
                return {'symbol': symbol, 'status': 'processed_empty', 'trace_id': trace_id}

            logger.info(f"[{trace_id}] ğŸ”§ å¤„ç†å®Œæˆ: {len(processed_data)}æ¡")

            # 3. å­˜å‚¨ï¼ˆv0.6.0 è¿½è¸ªï¼‰
            affected_rows, storage_report = self.tracer.trace_store_daily_data(
                self.storage, processed_data
            )

            # 4. éªŒè¯ä¸€è‡´æ€§
            validation = storage_report.get('validation', {})
            if not validation.get('consistent', False):
                raise RuntimeError(f"å­˜å‚¨éªŒè¯å¤±è´¥: {validation}")

            execution_time = time.time() - start_time

            # 5. è®°å½•æ—¥å¿—
            self.storage.log_data_update(
                data_type='daily',
                symbol=symbol,
                start_date=start_date,
                end_date=end_date,
                rows_affected=affected_rows,
                status='success',
                execution_time=execution_time
            )

            logger.info(f"[{trace_id}] âœ… å®Œæˆ: {symbol}, {affected_rows}è¡Œ, {execution_time:.2f}s")

            return {
                'symbol': symbol,
                'status': 'success',
                'records': len(processed_data),
                'affected': affected_rows,
                'quality_score': quality_report.get('total_score', 0),
                'execution_time': execution_time,
                'trace_id': trace_id
            }

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"[{trace_id}] âŒ å¤±è´¥: {symbol}, {e}", exc_info=True)

            # é”™è¯¯æ—¥å¿—
            self.storage.log_data_update(
                data_type='daily',
                symbol=symbol,
                start_date=start_date,
                end_date=end_date,
                rows_affected=0,
                status='error',
                error_message=str(e),
                execution_time=execution_time
            )

            return {
                'symbol': symbol,
                'status': 'error',
                'reason': str(e),
                'trace_id': trace_id,
                'execution_time': execution_time
            }

    def batch_process(self, symbols: List[str], start_date: str, end_date: str, max_concurrent: int = 3) -> Dict[
        str, Any]:
        """æ‰¹é‡å¤„ç† - v0.6.0"""
        batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        logger.info(f"[{batch_id}] æ‰¹é‡å¤„ç† {len(symbols)} åªè‚¡ç¥¨ï¼Œå¹¶å‘æ•°: {max_concurrent}")

        results = {
            'batch_id': batch_id,
            'total': len(symbols),
            'success': 0,
            'failed': 0,
            'total_rows': 0,
            'details': [],
            'start_time': datetime.now().isoformat()
        }

        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            futures = {
                executor.submit(self.process_single_stock, sym, start_date, end_date): sym
                for sym in symbols
            }

            for future in as_completed(futures):
                symbol = futures[future]
                try:
                    result = future.result()
                    results['details'].append(result)

                    if result['status'] == 'success':
                        results['success'] += 1
                        results['total_rows'] += result['affected']
                    else:
                        results['failed'] += 1

                except Exception as e:
                    results['failed'] += 1
                    results['details'].append({
                        'symbol': symbol,
                        'status': 'error',
                        'reason': str(e)
                    })

                processed = results['success'] + results['failed']
                if processed % 5 == 0:
                    logger.info(f"[{batch_id}] è¿›åº¦: {processed}/{results['total']}")

        results['end_time'] = datetime.now().isoformat()
        results['duration'] = (datetime.fromisoformat(results['end_time']) -
                               datetime.fromisoformat(results['start_time'])).total_seconds()

        logger.info(f"[{batch_id}] âœ… æ‰¹é‡å®Œæˆ: æˆåŠŸ{results['success']}/{results['total']}, "
                    f"å¤±è´¥{results['failed']}, æ€»è¡Œæ•°{results['total_rows']}, "
                    f"è€—æ—¶{results['duration']:.2f}s")

        return results


# æµ‹è¯•å‡½æ•°
def test_pipeline():
    """æµ‹è¯•ç®¡é“"""
    print("\nğŸ§ª æµ‹è¯• IntegratedDataPipeline (v0.6.0)")
    print("=" * 50)

    pipeline = IntegratedDataPipeline()
    result = pipeline.process_single_stock('sh600519', '20240101', '20240105')

    print(f"æµ‹è¯•ç»“æœ:")
    for key, value in result.items():
        print(f"  {key}: {value}")

    success = result['status'] == 'success'
    print(f"\n{'âœ… æµ‹è¯•é€šè¿‡' if success else 'âŒ æµ‹è¯•å¤±è´¥'}")

    return success


if __name__ == "__main__":
    success = test_pipeline()
    import sys

    sys.exit(0 if success else 1)