# _*_ coding: utf-8 _*_
# File Path: E:/MyFile/stock_database_v1/src/data\factor_storage_manager.py
# @ Author: mango-gh22
# @ Dateï¼š2026/1/3 12:41
"""
desc å› å­æ•°æ®å­˜å‚¨ç®¡ç†å™¨ - ä¸“é—¨å¤„ç†PBç­‰å› å­æ•°æ®çš„å­˜å‚¨å’Œå¢é‡æ›´æ–°
é›†æˆDataStorageæ¶æ„ï¼Œé’ˆå¯¹å› å­æ•°æ®è¿›è¡Œä¼˜åŒ–
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import logging
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from src.data.data_storage import DataStorage
from src.config.logging_config import setup_logging
from src.utils.code_converter import normalize_stock_code  # âœ… å¼ºåˆ¶æ·»åŠ æ­¤è¡Œ

logger = setup_logging()


class FactorStorageManager(DataStorage):
    """
    å› å­æ•°æ®å­˜å‚¨ç®¡ç†å™¨ - ä¸“é—¨å¤„ç†ä¼°å€¼å› å­æ•°æ®
    ç»§æ‰¿DataStorageï¼Œæ·»åŠ å› å­ä¸“ç”¨åŠŸèƒ½å’Œå¢é‡æ›´æ–°é€»è¾‘
    """

    def __init__(self, config_path: str = 'config/database.yaml'):
        """
        åˆå§‹åŒ–å› å­å­˜å‚¨ç®¡ç†å™¨

        Args:
            config_path: æ•°æ®åº“é…ç½®æ–‡ä»¶è·¯å¾„
        """
        super().__init__(config_path)

        # å› å­ç›¸å…³é…ç½®
        self.factor_fields = [
            'pe', 'pe_ttm', 'pb', 'ps', 'ps_ttm',
            'dv_ratio', 'dv_ttm'
        ]

        # è´¢åŠ¡æŒ‡æ ‡å­—æ®µ
        self.financial_fields = [
            'total_share', 'float_share', 'free_share',
            'total_mv', 'circ_mv'
        ]

        # ç¼“å­˜ç®¡ç†
        self._last_date_cache = {}

        logger.info("âœ… å› å­å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def _preprocess_factor_data(self, df: pd.DataFrame, table_name: str = 'stock_daily_data') -> pd.DataFrame:
        """
        é¢„å¤„ç†å› å­æ•°æ® - ä¸“é—¨å¤„ç†PBç­‰å› å­æ•°æ®

        Args:
            df: åŸå§‹å› å­æ•°æ®
            table_name: ç›®æ ‡è¡¨å

        Returns:
            é¢„å¤„ç†åçš„DataFrame
        """
        if df.empty:
            logger.warning("å› å­æ•°æ®ä¸ºç©º")
            return df

        df_processed = df.copy()

        # 1. å­—æ®µæ˜ å°„ï¼šBaostockå­—æ®µ -> æ•°æ®åº“å­—æ®µ
        field_mapping = {
            # ä¼°å€¼æŒ‡æ ‡
            'peTTM': 'pe_ttm',
            'pbMRQ': 'pb',
            'psTTM': 'ps_ttm',
            'pcfNcfTTM': 'pcf_ttm',

            # åŸºç¡€å­—æ®µ
            'code': 'symbol',
            'tradeDate': 'trade_date',
            'turnoverRate': 'turnover_rate',
            'turnoverRate_f': 'turnover_rate_f',
            'volumeRatio': 'volume_ratio',

            # å¸‚å€¼ç›¸å…³
            'totalShare': 'total_share',
            'floatShare': 'float_share',
            'freeShare': 'free_share',
            'totalMv': 'total_mv',
            'circMv': 'circ_mv',

            # å…¶ä»–
            'pe': 'pe',
            'ps': 'ps',
            'dvRatio': 'dv_ratio',
            'dvTtm': 'dv_ttm'
        }

        # åº”ç”¨å­—æ®µæ˜ å°„
        rename_map = {}
        for src_field, target_field in field_mapping.items():
            if src_field in df_processed.columns and target_field not in df_processed.columns:
                rename_map[src_field] = target_field

        if rename_map:
            df_processed = df_processed.rename(columns=rename_map)
            logger.debug(f"å­—æ®µæ˜ å°„: {rename_map}")

    # åœ¨ _preprocess_factor_data æ–¹æ³•ä¸­ï¼Œç¡®ä¿ symbol æ ‡å‡†åŒ–ï¼š

        # 2. âœ… ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨ä¸”æ ‡å‡†åŒ–
        if 'bs_code' in df_processed.columns:
            # ä» bs_code (sh.600519) è½¬æ¢ä¸º sh600519
            df_processed['symbol'] = df_processed['bs_code'].apply(
                lambda x: str(x).replace('.', '') if pd.notna(x) else None
            )
            logger.debug("ä»bs_codeç”Ÿæˆæ ‡å‡†åŒ– symbol")
        elif 'code' in df_processed.columns:
            df_processed['symbol'] = df_processed['code'].apply(
                lambda x: str(x).replace('.', '') if pd.notna(x) else None
            )
            logger.debug("ä»codeç”Ÿæˆæ ‡å‡†åŒ– symbol")
        elif 'symbol' in df_processed.columns:
            # å¼ºåˆ¶æ ‡å‡†åŒ–å·²æœ‰çš„ symbol å­—æ®µ
            df_processed['symbol'] = df_processed['symbol'].apply(
                lambda x: normalize_stock_code(str(x)) if pd.notna(x) else None
            )
            logger.debug("æ ‡å‡†åŒ–ç°æœ‰ symbol å­—æ®µ")

        # 3. æ—¥æœŸæ ¼å¼åŒ–
        if 'trade_date' in df_processed.columns:
            # ç¡®ä¿æ—¥æœŸæ ¼å¼
            df_processed['trade_date'] = pd.to_datetime(
                df_processed['trade_date'], errors='coerce'
            ).dt.strftime('%Y-%m-%d')

        # 4. æ•°å€¼å­—æ®µè½¬æ¢
        numeric_fields = self.factor_fields + self.financial_fields
        for field in numeric_fields:
            if field in df_processed.columns:
                try:
                    df_processed[field] = pd.to_numeric(df_processed[field], errors='coerce')
                except Exception as e:
                    logger.warning(f"æ•°å€¼è½¬æ¢å¤±è´¥ {field}: {e}")

        # 5. é€‰æ‹©éœ€è¦æ’å…¥çš„å­—æ®µï¼ˆåªåŒ…å«è¡¨ä¸­æœ‰çš„å­—æ®µï¼‰
        table_columns = self._get_table_columns(table_name)
        available_columns = [col for col in df_processed.columns if col in table_columns]

        # ç¡®ä¿å¿…éœ€å­—æ®µ
        required_columns = ['symbol', 'trade_date']
        for req_col in required_columns:
            if req_col in df_processed.columns and req_col not in available_columns:
                available_columns.append(req_col)

        df_processed = df_processed[available_columns] if available_columns else pd.DataFrame()

        logger.info(f"é¢„å¤„ç†å®Œæˆ: {len(df_processed)} æ¡è®°å½•ï¼Œ{len(available_columns)} ä¸ªå­—æ®µ")

        return df_processed

    # åœ¨ FactorStorageManager ç±»ä¸­æ·»åŠ ï¼ˆæ”¾åœ¨ __init__ ä¹‹åæˆ–å…¶ä»–æ–¹æ³•é™„è¿‘ï¼‰
    def prepare_factor_data_for_storage(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å…¬æœ‰æ¥å£ï¼šå‡†å¤‡å› å­æ•°æ®ç”¨äºå­˜å‚¨
        """
        return self._preprocess_factor_data(df, table_name='stock_daily_data')

    def store_factor_data(self, data: Any, table_name: str = 'stock_daily_data') -> Tuple[int, Dict]:
        """
        å­˜å‚¨å› å­æ•°æ® - ä¸“é—¨å¤„ç†PBç­‰å› å­

        Args:
            data: å› å­æ•°æ®ï¼Œå¯ä»¥æ˜¯DataFrameæˆ–å­—å…¸åˆ—è¡¨
            table_name: ç›®æ ‡è¡¨åï¼Œé»˜è®¤stock_daily_data

        Returns:
            (å½±å“è¡Œæ•°, è¯¦ç»†ä¿¡æ¯å­—å…¸)
        """
        try:
            # è½¬æ¢ä¸ºDataFrame
            if isinstance(data, pd.DataFrame):
                df = data.copy()
            elif isinstance(data, list):
                df = pd.DataFrame(data)
            elif isinstance(data, dict):
                df = pd.DataFrame([data])
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(data)}")

            if df.empty:
                logger.warning("å› å­æ•°æ®ä¸ºç©º")
                return 0, {'status': 'skipped', 'reason': 'empty_data'}

            logger.info(f"å¼€å§‹å­˜å‚¨å› å­æ•°æ®: {len(df)} æ¡è®°å½•")
            logger.debug(f"åŸå§‹å­—æ®µ: {list(df.columns)}")

            # é¢„å¤„ç†å› å­æ•°æ®
            df_processed = self._preprocess_factor_data(df, table_name)

            if df_processed.empty:
                logger.error("é¢„å¤„ç†åæ— æœ‰æ•ˆæ•°æ®")
                return 0, {'status': 'skipped', 'reason': 'preprocess_failed'}

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if 'symbol' not in df_processed.columns or 'trade_date' not in df_processed.columns:
                logger.error(f"ç¼ºå°‘å¿…éœ€å­—æ®µï¼Œç°æœ‰å­—æ®µ: {list(df_processed.columns)}")
                return 0, {'status': 'error', 'reason': 'missing_required_fields'}

            # è·å–è‚¡ç¥¨ä»£ç å’Œæ—¥æœŸèŒƒå›´ä¿¡æ¯
            symbol = df_processed['symbol'].iloc[0]
            dates = df_processed['trade_date'].tolist()
            date_range = f"{min(dates)} è‡³ {max(dates)}" if dates else "N/A"

            logger.info(f"æ•°æ®ä¿¡æ¯: {symbol}, æ—¥æœŸèŒƒå›´: {date_range}")
            logger.debug(f"é¢„å¤„ç†åå­—æ®µ: {list(df_processed.columns)}")
            logger.debug(f"å‰2è¡Œç¤ºä¾‹:\n{df_processed.head(2).to_string()}")

            # è°ƒç”¨çˆ¶ç±»çš„store_daily_dataæ–¹æ³•
            affected_rows, result = super().store_daily_data(df_processed, table_name)

            if affected_rows > 0:
                logger.info(f"âœ… å› å­æ•°æ®å­˜å‚¨æˆåŠŸ: {symbol}, {affected_rows} æ¡è®°å½•")
            else:
                # åˆ†æä¸ºä»€ä¹ˆæ²¡æœ‰å½±å“è¡Œæ•°
                existing_count = self._check_existing_data(symbol, dates[0] if dates else None)
                logger.info(f"âš ï¸  å­˜å‚¨0æ¡è®°å½•: {symbol} (æ•°æ®åº“ä¸­å·²æœ‰ {existing_count} æ¡è®°å½•)")

            return affected_rows, {
                'status': 'success' if affected_rows > 0 else 'skipped',
                'symbol': symbol,
                'records_processed': len(df_processed),
                'records_affected': affected_rows,
                'date_range': date_range,
                'factor_fields': [f for f in self.factor_fields if f in df_processed.columns]
            }

        except Exception as e:
            logger.error(f"å­˜å‚¨å› å­æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return 0, {
                'status': 'error',
                'error': str(e),
                'error_type': type(e).__name__
            }

    def _check_existing_data(self, symbol: str, date: str) -> int:
        """
        æ£€æŸ¥æ•°æ®åº“ä¸­å·²æœ‰çš„æ•°æ®æ•°é‡
        """
        try:
            clean_symbol = str(symbol).replace('.', '')

            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    # æŸ¥è¯¢æŒ‡å®šæ—¥æœŸåŠä¹‹å‰çš„æ•°æ®
                    if date:
                        cursor.execute(
                            "SELECT COUNT(*) FROM stock_daily_data WHERE symbol = %s AND trade_date <= %s",
                            (clean_symbol, date)
                        )
                    else:
                        cursor.execute(
                            "SELECT COUNT(*) FROM stock_daily_data WHERE symbol = %s",
                            (clean_symbol,)
                        )
                    count = cursor.fetchone()[0]
                    return count
        except Exception as e:
            logger.warning(f"æ£€æŸ¥ç°æœ‰æ•°æ®å¤±è´¥: {e}")
            return 0

    # åœ¨ src/data/factor_storage_manager.py ä¸­ä¿®å¤ get_last_factor_date æ–¹æ³•
    def get_last_factor_date(self, symbol: str, table_name: str = 'stock_daily_data') -> Optional[str]:
        """
        è·å–æŒ‡å®šè‚¡ç¥¨çš„æœ€åå› å­æ•°æ®æ—¥æœŸ

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            table_name: è¡¨å

        Returns:
            æœ€åæ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD) æˆ– None
        """
        cache_key = f"{symbol}_{table_name}"

        if cache_key in self._last_date_cache:
            return self._last_date_cache[cache_key]

        try:
            clean_symbol = str(symbol).replace('.', '')

            # å°è¯•è·å–æœ€åæœ‰å› å­æ•°æ®çš„æ—¥æœŸï¼ˆä»»ä½•å› å­å­—æ®µä¸ä¸ºç©ºï¼‰
            with self.db_connector.get_connection() as conn:
                with conn.cursor() as cursor:
                    # æŸ¥è¯¢æœ‰pbã€pe_ttmæˆ–ps_ttmæ•°æ®çš„æœ€åæ—¥æœŸ
                    cursor.execute(f"""
                        SELECT MAX(trade_date) 
                        FROM {table_name} 
                        WHERE symbol = %s 
                        AND (pb IS NOT NULL OR pe_ttm IS NOT NULL OR ps_ttm IS NOT NULL)
                    """, (clean_symbol,))

                    result = cursor.fetchone()
                    if result and result[0]:
                        last_date = result[0]
                        if isinstance(last_date, str):
                            # å·²ç»æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                            formatted_date = last_date
                        else:
                            # datetimeæ ¼å¼è½¬æ¢
                            formatted_date = last_date.strftime('%Y-%m-%d')

                        self._last_date_cache[cache_key] = formatted_date
                        logger.debug(f"æœ€åå› å­æ—¥æœŸ: {symbol} -> {formatted_date}")
                        return formatted_date

                    # å¦‚æœæ²¡æœ‰å› å­æ•°æ®ï¼Œè¿”å›æœ€åäº¤æ˜“æ—¥æœŸ
                    cursor.execute(f"""
                        SELECT MAX(trade_date) 
                        FROM {table_name} 
                        WHERE symbol = %s
                    """, (clean_symbol,))

                    result = cursor.fetchone()
                    if result and result[0]:
                        last_date = result[0]
                        if isinstance(last_date, str):
                            formatted_date = last_date
                        else:
                            formatted_date = last_date.strftime('%Y-%m-%d')
                        return formatted_date

                    return None

        except Exception as e:
            logger.warning(f"è·å–æœ€åå› å­æ—¥æœŸå¤±è´¥ {symbol}: {e}")
            return None

    # def calculate_incremental_range(self, symbol: str, factor_type: str = 'pb') -> Tuple[Optional[str], Optional[str]]:
    #     """
    #     è®¡ç®—å¢é‡ä¸‹è½½èŒƒå›´
    #
    #     Args:
    #         symbol: è‚¡ç¥¨ä»£ç 
    #         factor_type: å› å­ç±»å‹ï¼Œç”¨äºæ—¥å¿—è®°å½•
    #
    #     Returns:
    #         (å¼€å§‹æ—¥æœŸ, ç»“æŸæ—¥æœŸ)ï¼Œå¦‚æœæ— éœ€æ›´æ–°åˆ™è¿”å› (None, None)
    #     """
    #     try:
    #         # è·å–æœ€åæ›´æ–°æ—¥æœŸ
    #         last_date = self.get_last_factor_date(symbol)
    #
    #         if not last_date:
    #             logger.info(f"{symbol}: æ— å†å²æ•°æ®ï¼Œéœ€è¦å…¨é‡ä¸‹è½½")
    #             return '20050101', datetime.now().strftime('%Y%m%d')  # ä»2005å¹´å¼€å§‹
    #
    #         # è½¬æ¢ä¸ºdatetime
    #         try:
    #             last_dt = datetime.strptime(str(last_date), '%Y-%m-%d')
    #         except:
    #             last_dt = datetime.strptime(str(last_date), '%Y%m%d')
    #
    #         # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæœ€åæ—¥æœŸæ˜¯å¦åœ¨ä»Šå¤©ä¹‹å‰ï¼‰
    #         today = datetime.now().date()
    #
    #         if last_dt.date() >= today:
    #             logger.info(f"{symbol}: æ•°æ®å·²æœ€æ–°ï¼ˆæœ€åæ›´æ–°: {last_date}ï¼‰")
    #             return None, None
    #
    #         # è®¡ç®—å¼€å§‹æ—¥æœŸï¼ˆæœ€åæ—¥æœŸçš„ä¸‹ä¸€å¤©ï¼‰
    #         start_date = (last_dt + timedelta(days=1)).strftime('%Y%m%d')
    #         end_date = today.strftime('%Y%m%d')
    #
    #         logger.info(f"{symbol}: å¢é‡èŒƒå›´ {start_date} - {end_date}ï¼ˆåŸºäºæœ€åæ›´æ–°: {last_date}ï¼‰")
    #         return start_date, end_date
    #
    #     except Exception as e:
    #         logger.error(f"è®¡ç®—å¢é‡èŒƒå›´å¤±è´¥ {symbol}: {e}")
    #         return None, None

    # åœ¨_factor_storage_manager.pyä¸­ä¿®æ”¹ä»¥ä¸‹æ–¹æ³•

    def calculate_improved_incremental_range(self, symbol: str, factor_type: str = 'pb') -> Tuple[
        Optional[str], Optional[str]]:
        """
        æ”¹è¿›çš„å¢é‡èŒƒå›´è®¡ç®— - ä¿®å¤æ—¥æœŸæ¯”è¾ƒé€»è¾‘

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            factor_type: å› å­ç±»å‹ï¼Œç”¨äºæ—¥å¿—è®°å½•

        Returns:
            (å¼€å§‹æ—¥æœŸ, ç»“æŸæ—¥æœŸ)ï¼Œå¦‚æœæ— éœ€æ›´æ–°åˆ™è¿”å› (None, None)
        """
        try:
            # è·å–æœ€åæ›´æ–°æ—¥æœŸ
            last_date = self.get_last_factor_date(symbol)

            if not last_date:
                logger.info(f"{symbol}: æ— å†å²æ•°æ®ï¼Œéœ€è¦å…¨é‡ä¸‹è½½")
                return '20050101', datetime.now().strftime('%Y%m%d')  # ä»2005å¹´å¼€å§‹

            # è½¬æ¢ä¸ºdatetime
            try:
                last_dt = datetime.strptime(str(last_date), '%Y-%m-%d')
            except:
                try:
                    last_dt = datetime.strptime(str(last_date), '%Y%m%d')
                except:
                    logger.error(f"æ— æ³•è§£ææœ€åæ—¥æœŸæ ¼å¼: {last_date}")
                    return '20050101', datetime.now().strftime('%Y%m%d')

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæœ€åæ—¥æœŸæ˜¯å¦åœ¨æ˜¨å¤©ä¹‹å‰ï¼‰
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)

            # ä¿®æ­£ï¼šåªæœ‰å½“æœ€åæ—¥æœŸ >= ä»Šå¤©æ‰è·³è¿‡
            if last_dt.date() >= today:
                logger.info(f"{symbol}: æ•°æ®å·²æœ€æ–°ï¼ˆæœ€åæ›´æ–°: {last_date} >= ä»Šå¤©ï¼‰")
                return None, None

            # è®¡ç®—å¼€å§‹æ—¥æœŸï¼ˆæœ€åæ—¥æœŸçš„ä¸‹ä¸€å¤©ï¼‰
            start_date = (last_dt + timedelta(days=1)).strftime('%Y%m%d')
            end_date = yesterday.strftime('%Y%m%d')  # ç»“æŸåˆ°æ˜¨å¤©ï¼Œé¿å…è¯·æ±‚æœªæ¥æ•°æ®

            # ç¡®ä¿å¼€å§‹æ—¥æœŸä¸æ™šäºç»“æŸæ—¥æœŸ
            if start_date > end_date:
                logger.warning(f"{symbol}: å¼€å§‹æ—¥æœŸ {start_date} æ™šäºç»“æŸæ—¥æœŸ {end_date}")
                # å¦‚æœå¼€å§‹æ—¥æœŸæ™šäºç»“æŸæ—¥æœŸï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€å¤©
                if start_date[:8] == end_date[:8]:
                    # åŒä¸€å¤©ï¼Œè¯´æ˜æ•°æ®å·²æœ€æ–°
                    return None, None
                else:
                    # ä¸åŒå¤©ï¼Œéœ€è¦è°ƒæ•´
                    start_date, end_date = end_date, start_date

            logger.info(f"{symbol}: å¢é‡èŒƒå›´ {start_date} - {end_date}ï¼ˆåŸºäºæœ€åæ›´æ–°: {last_date}ï¼‰")
            return start_date, end_date

        except Exception as e:
            logger.error(f"è®¡ç®—å¢é‡èŒƒå›´å¤±è´¥ {symbol}: {e}")
            # å‡ºé”™æ—¶è¿”å›å…¨é‡èŒƒå›´ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
            return '20050101', datetime.now().strftime('%Y%m%d')

    # åœ¨FactorStorageManagerç±»ä¸­æ›¿æ¢åŸæ¥çš„calculate_incremental_rangeæ–¹æ³•
    def calculate_incremental_range(self, symbol: str, factor_type: str = 'pb') -> Tuple[Optional[str], Optional[str]]:
        """å‘åå…¼å®¹çš„å¢é‡èŒƒå›´è®¡ç®—ï¼ˆè°ƒç”¨æ”¹è¿›ç‰ˆæœ¬ï¼‰"""
        return self.calculate_improved_incremental_range(symbol, factor_type)



    def update_factors_for_symbol(
            self,
            symbol: str,
            downloader,
            incremental: bool = True,
            table_name: str = 'stock_daily_data'
    ) -> Tuple[bool, Dict]:
        """
        æ›´æ–°å•åªè‚¡ç¥¨çš„å› å­æ•°æ®ï¼ˆç«¯åˆ°ç«¯ï¼‰

        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '600519'ï¼‰
            downloader: å› å­ä¸‹è½½å™¨å®ä¾‹ï¼ˆéœ€æœ‰ fetch_factor_data æ–¹æ³•ï¼‰
            incremental: æ˜¯å¦å¢é‡æ›´æ–°
            table_name: ç›®æ ‡è¡¨å

        Returns:
            (success: bool, result: dict)
        """
        try:
            if incremental:
                start_date, end_date = self.calculate_incremental_range(symbol)
                if not start_date or not end_date:
                    return True, {'status': 'up_to_date'}
            else:
                # å…¨é‡ä¸‹è½½ï¼ˆä¾‹å¦‚ä»2005å¹´è‡³ä»Šï¼‰
                start_date = '20050101'
                end_date = datetime.now().strftime('%Y%m%d')

            if not start_date or not end_date:
                return True, {'status': 'no_range'}

            logger.info(f"ğŸ“¥ è¯·æ±‚å› å­æ•°æ®: {symbol} [{start_date} - {end_date}]")

            # ä¸‹è½½åŸå§‹æ•°æ®
            raw_df = downloader.fetch_factor_data(symbol, start_date, end_date)

            if raw_df.empty:
                logger.warning(f"âš ï¸ æ— å› å­æ•°æ®: {symbol} [{start_date} - {end_date}]")
                return True, {
                    'status': 'no_data',
                    'request_range': {'start': start_date, 'end': end_date}
                }

            # é¢„å¤„ç†
            processed_df = self._preprocess_factor_data(raw_df, table_name)

            if processed_df.empty:
                logger.error(f"âŒ é¢„å¤„ç†åæ— æœ‰æ•ˆæ•°æ®: {symbol}")
                return False, {
                    'status': 'preprocess_failed',
                    'error': 'é¢„å¤„ç†åæ— æœ‰æ•ˆå­—æ®µ'
                }

            # å­˜å‚¨
            affected_rows, store_report = self.store_factor_data(processed_df, table_name)

            # æ„å»ºè¿”å›ç»“æœ
            dates = processed_df['trade_date'].tolist()
            result = {
                'status': 'success',
                'records_stored': affected_rows,
                'request_range': {'start': start_date, 'end': end_date},
                'data_range': {
                    'start': min(dates) if dates else None,
                    'end': max(dates) if dates else None
                },
                'factor_fields': store_report.get('factor_fields', [])
            }

            logger.info(f"âœ… æ›´æ–°å®Œæˆ: {symbol}, å­˜å‚¨ {affected_rows} æ¡è®°å½•")
            return True, result

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å› å­å¤±è´¥ {symbol}: {e}", exc_info=True)
            return False, {
                'status': 'error',
                'error': str(e),
                'error_type': type(e).__name__
            }



    def clear_cache(self, symbol: str = None):
        """
        æ¸…ç†ç¼“å­˜æ•°æ®

        Args:
            symbol: æŒ‡å®šè‚¡ç¥¨ä»£ç ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…ç†æ‰€æœ‰ç¼“å­˜
        """
        if symbol:
            keys_to_remove = [k for k in self._last_date_cache.keys() if k.startswith(symbol)]
            for key in keys_to_remove:
                del self._last_date_cache[key]
            logger.debug(f"æ¸…ç†ç¼“å­˜: {symbol}")
        else:
            self._last_date_cache.clear()
            logger.debug("æ¸…ç†æ‰€æœ‰ç¼“å­˜")


# æµ‹è¯•å‡½æ•°
def test_factor_storage_manager():
    """æµ‹è¯•å› å­å­˜å‚¨ç®¡ç†å™¨"""
    print("\nğŸ§ª æµ‹è¯•å› å­å­˜å‚¨ç®¡ç†å™¨")
    print("=" * 50)

    try:
        # 1. åˆå§‹åŒ–
        print("åˆå§‹åŒ–FactorStorageManager...")
        storage = FactorStorageManager()
        print("âœ… åˆå§‹åŒ–æˆåŠŸ")

        # 2. åˆ›å»ºæµ‹è¯•æ•°æ®
        print("åˆ›å»ºæµ‹è¯•å› å­æ•°æ®...")
        import pandas as pd

        test_data = pd.DataFrame({
            'bs_code': ['sh600519'],
            'trade_date': ['2026-01-03'],
            'pe_ttm': [20.5],
            'pb': [5.2],
            'ps_ttm': [8.3],
            'total_share': [1250000000.0],
            'total_mv': [250000000000.0]
        })

        print(f"æµ‹è¯•æ•°æ®: {len(test_data)} æ¡")
        print(f"å­—æ®µ: {list(test_data.columns)}")

        # 3. å­˜å‚¨æ•°æ®
        print("å­˜å‚¨å› å­æ•°æ®...")
        affected_rows, report = storage.store_factor_data(test_data)

        print(f"å­˜å‚¨ç»“æœ:")
        print(f"  å½±å“è¡Œæ•°: {affected_rows}")
        print(f"  çŠ¶æ€: {report['status']}")
        print(f"  è‚¡ç¥¨: {report['symbol']}")
        print(f"  å¤„ç†è®°å½•: {report['records_processed']}")

        # 4. æµ‹è¯•å¢é‡èŒƒå›´è®¡ç®—
        print("\næµ‹è¯•å¢é‡èŒƒå›´è®¡ç®—...")
        if report['symbol']:
            start_date, end_date = storage.calculate_incremental_range(report['symbol'])
            print(f"  å¢é‡èŒƒå›´: {start_date} - {end_date}")

        # 5. æµ‹è¯•æœ€åæ—¥æœŸæŸ¥è¯¢
        if report['symbol']:
            last_date = storage.get_last_factor_date(report['symbol'])
            print(f"  æœ€åæ›´æ–°æ—¥æœŸ: {last_date}")

        # 6. æ¸…ç†æµ‹è¯•ç¼“å­˜
        storage.clear_cache()

        return affected_rows >= 0  # è¿”å›Trueè¡¨ç¤ºæµ‹è¯•æ­£å¸¸å®Œæˆï¼ˆä¸è¦æ±‚ä¸€å®šæ’å…¥æ•°æ®ï¼‰

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_factor_storage_manager()

    if success:
        print("\nâœ… å› å­å­˜å‚¨ç®¡ç†å™¨æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ å› å­å­˜å‚¨ç®¡ç†å™¨æµ‹è¯•å¤±è´¥")

    exit(0 if success else 1)